{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet50 - PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "455cb31c-2b11-4f7d-f4eb-b20da21bc07d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "49f8bfd4-f1fb-45dd-90ff-0cbc6c9b7db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "import collections\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, Reshape, GlobalAveragePooling2D\n",
        "from keras import regularizers\n",
        "from keras.regularizers import l2\n",
        "from keras.applications.resnet_v2 import ResNet50V2\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint, LearningRateScheduler"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "597253fb-7d36-42ce-adc6-2e3d964afa9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xWOu2-Hkw-8",
        "colab_type": "code",
        "outputId": "6639b6b6-6982-40d0-9ffe-33b4ae606b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "class_weights = {}\n",
        "cl_names = [\"gender_output\", \"image_quality_output\", \"age_output\", \"weight_output\", \"bag_output\", \"footwear_output\", \"emotion_output\", \"pose_output\"]\n",
        "cols = list(df.columns)\n",
        "cols.pop()\n",
        "i = 0\n",
        "for col in cols:\n",
        "    a = df[col].value_counts(normalize=True)\n",
        "    od = collections.OrderedDict(sorted(dict(a).items()))\n",
        "    temp = {}\n",
        "    ix = 0\n",
        "    for d in od:\n",
        "        temp[ix] = od[d]\n",
        "        ix +=1\n",
        "    class_weights[cl_names[i]] = temp\n",
        "    i +=1\n",
        "\n",
        "print(class_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'gender_output': {0: 0.43741251013040594, 1: 0.5625874898695941}, 'image_quality_output': {0: 0.5532306785530097, 1: 0.16503352243424446, 2: 0.28173579901274587}, 'age_output': {0: 0.18374714506741324, 1: 0.39865910263022175, 2: 0.25307595962572754, 3: 0.10977676269063583, 4: 0.054741029986001624}, 'weight_output': {0: 0.6356737640904737, 1: 0.06564503057540706, 2: 0.23546747218743094, 3: 0.06321373314668828}, 'bag_output': {0: 0.33912915346644074, 1: 0.09732557282840934, 2: 0.56354527370515}, 'footwear_output': {0: 0.37044131732115226, 1: 0.18470492890296913, 2: 0.4448537537758786}, 'emotion_output': {0: 0.11051351948721727, 1: 0.11854416856995506, 2: 0.7117070654976793, 3: 0.05923524644514846}, 'pose_output': {0: 0.16260222500552568, 1: 0.6176232225742282, 2: 0.21977455242024607}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "63456f2f-ecf1-44c1-dfe8-d4c16a58c311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None, im_size=224):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "        self.im_size = im_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        ims = [cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()]\n",
        "        image = np.stack([cv2.resize(im, (self.im_size, self.im_size)) for im in ims])\n",
        "        image = image.astype('float32') / 255\n",
        "        images_mean = np.mean(image, axis=0)\n",
        "        image -= images_mean\n",
        "\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        if self.augmentation is not None:\n",
        "          image = self.augmentation.flow(image, shuffle=False, batch_size=self.batch_size).next()\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "27c28e79-2e51-426e-fa20-28ed2e17cf6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.2,random_state=31)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10858, 28), (2715, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augmentor(images):\n",
        "\t\t'Apply data augmentation'\n",
        "\t\tsometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "\t\tseq = iaa.Sequential(\n",
        "\t\t\t\t[\n",
        "\t\t\t\t# apply the following augmenters to most images\n",
        "\t\t\t\tiaa.Fliplr(0.3),  # horizontally flip 50% of all images\n",
        "\t\t\t\tsometimes(iaa.Affine(\n",
        "\t\t\t\t\tscale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
        "\t\t\t\t\t# scale images to 80-120% of their size, individually per axis\n",
        "\t\t\t\t\ttranslate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
        "\t\t\t\t\t# translate by -20 to +20 percent (per axis)\n",
        "\t\t\t\t\trotate=(-10, 10),  # rotate by -45 to +45 degrees\n",
        "\t\t\t\t\tshear=(-5, 5),  # shear by -16 to +16 degrees\n",
        "\t\t\t\t\torder=[0, 1],\n",
        "\t\t\t\t\t# use nearest neighbour or bilinear interpolation (fast)\n",
        "\t\t\t\t\tcval=(0, 255),  # if mode is constant, use a cval between 0 and 255\n",
        "\t\t\t\t\tmode=ia.ALL\n",
        "\t\t\t\t\t# use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
        "\t\t\t\t)),\n",
        "\t\t\t\t# execute 0 to 5 of the following (less important) augmenters per image\n",
        "\t\t\t\t# don't execute all of them, as that would often be way too strong\n",
        "\t\t\t\tiaa.SomeOf((0, 5),\n",
        "\t\t\t\t           [sometimes(iaa.Superpixels(p_replace=(0, 1.0),\n",
        "\t\t\t\t\t\t                                     n_segments=(20, 200))),\n",
        "\t\t\t\t\t           # convert images into their superpixel representation\n",
        "\t\t\t\t\t           iaa.OneOf([\n",
        "\t\t\t\t\t\t\t           iaa.GaussianBlur((0, 1.0)),\n",
        "\t\t\t\t\t\t\t           # blur images with a sigma between 0 and 3.0\n",
        "\t\t\t\t\t\t\t           iaa.AverageBlur(k=(3, 5)),\n",
        "\t\t\t\t\t\t\t           # blur image using local means with kernel sizes between 2 and 7\n",
        "\t\t\t\t\t\t\t           iaa.MedianBlur(k=(3, 5)),\n",
        "\t\t\t\t\t\t\t           # blur image using local medians with kernel sizes between 2 and 7\n",
        "\t\t\t\t\t           ]),\n",
        "\t\t\t\t\t           iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)),\n",
        "\t\t\t\t\t           # sharpen images\n",
        "\t\t\t\t\t           iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n",
        "\t\t\t\t\t           # emboss images\n",
        "\t\t\t\t\t           # search either for all edges or for directed edges,\n",
        "\t\t\t\t\t           # blend the result with the original image using a blobby mask\n",
        "\t\t\t\t\t           iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
        "\t\t\t\t\t\t\t           iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
        "\t\t\t\t\t\t\t           iaa.DirectedEdgeDetect(alpha=(0.5, 1.0),\n",
        "\t\t\t\t\t\t\t                                  direction=(0.0, 1.0)),\n",
        "\t\t\t\t\t           ])),\n",
        "\t\t\t\t\t           iaa.AdditiveGaussianNoise(loc=0,\n",
        "\t\t\t\t\t                                     scale=(0.0, 0.01 * 255),\n",
        "\t\t\t\t\t                                     per_channel=0.5),\n",
        "\t\t\t\t\t           # add gaussian noise to images\n",
        "\t\t\t\t\t           \n",
        "\t\t\t\t\t           \n",
        "\t\t\t\t\t           iaa.Add((-2, 2), per_channel=0.5),\n",
        "\t\t\t\t\t           # change brightness of images (by -10 to 10 of original value)\n",
        "\t\t\t\t\t           iaa.AddToHueAndSaturation((-1, 1)),\n",
        "\t\t\t\t\t           # change hue and saturation\n",
        "\t\t\t\t\t           # either change the brightness of the whole image (sometimes\n",
        "\t\t\t\t\t           # per channel) or change the brightness of subareas\n",
        "\t\t\t\t\t           iaa.OneOf([\n",
        "\t\t\t\t\t\t\t           iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
        "\t\t\t\t\t\t\t           iaa.FrequencyNoiseAlpha(\n",
        "\t\t\t\t\t\t\t\t\t           exponent=(-1, 0),\n",
        "\t\t\t\t\t\t\t\t\t           first=iaa.Multiply((0.9, 1.1),\n",
        "\t\t\t\t\t\t\t\t\t                              per_channel=True),\n",
        "\t\t\t\t\t\t\t\t\t           second=iaa.ContrastNormalization(\n",
        "\t\t\t\t\t\t\t\t\t\t\t           (0.9, 1.1))\n",
        "\t\t\t\t\t\t\t           )\n",
        "\t\t\t\t\t           ]),\n",
        "\t\t\t\t\t           sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5),\n",
        "\t\t\t\t\t                                               sigma=0.25)),\n",
        "\t\t\t\t\t           # move pixels locally around (with random strengths)\n",
        "\t\t\t\t\t           sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))),\n",
        "\t\t\t\t\t           # sometimes move parts of the image around\n",
        "\t\t\t\t\t           sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
        "\t\t\t\t           ],\n",
        "\t\t\t\t           random_order=True\n",
        "\t\t\t\t           )\n",
        "\t\t\t\t],\n",
        "\t\t\t\trandom_order=True\n",
        "\t\t)\n",
        "\t\treturn seq.augment_images(images)\n",
        "  \n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "\n",
        "  \"\"\"\n",
        "  p : the probability that random erasing is performed\n",
        "  s_l, s_h : minimum / maximum proportion of erased area against input image\n",
        "  r_1, r_2 : minimum / maximum aspect ratio of erased area\n",
        "  v_l, v_h : minimum / maximum value for erased area\n",
        "  pixel_level : pixel-level randomization for erased area\n",
        "  \"\"\"\n",
        "\n",
        "  def eraser(input_img):\n",
        "    img_h, img_w, img_c = input_img.shape\n",
        "    p_1 = np.random.rand()\n",
        "\n",
        "    if p_1 > p:\n",
        "        return input_img\n",
        "\n",
        "    while True:\n",
        "        s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "        r = np.random.uniform(r_1, r_2)\n",
        "        w = int(np.sqrt(s / r))\n",
        "        h = int(np.sqrt(s * r))\n",
        "        left = np.random.randint(0, img_w)\n",
        "        top = np.random.randint(0, img_h)\n",
        "\n",
        "        if left + w <= img_w and top + h <= img_h:\n",
        "            break\n",
        "\n",
        "    if pixel_level:\n",
        "        c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "    else:\n",
        "        c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "    input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "    return input_img\n",
        "\n",
        "  return eraser\n",
        "\n",
        "\n",
        "aug = ImageDataGenerator(\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=2,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.08,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.05,\n",
        "        # set range for random shear\n",
        "        shear_range=0.07,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.05,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.07,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.1,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        preprocessing_function=get_random_eraser(p=0.08,v_l=0, v_h=0.3, s_l=0.008, s_h=0.05, pixel_level=False)\n",
        "\t\t\t\t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32,augmentation=aug)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=32, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "91ed7d49-4d78-4a9d-df67-498d6ee77516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgK-BmUD5d_c",
        "colab_type": "code",
        "outputId": "6c54605b-fa64-4ca5-ef08-d74419017e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "model = ResNet50V2(weights= None, include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "neck = model.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128,activation='relu')(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "model = Model(inputs=[model.input], outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQIrjiSHJdbT",
        "colab_type": "code",
        "outputId": "c7ceea8b-1e89-4a63-aeaf-c884cd5ab11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_bn (BatchNo (None, 56, 56, 64)   256         pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_relu (Activ (None, 56, 56, 64)   0           conv2_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Add)          (None, 56, 56, 256)  0           conv2_block1_0_conv[0][0]        \n",
            "                                                                 conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 28, 28, 64)   36864       conv2_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 28, 28, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 28, 28, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 256)  0           conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 28, 28, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Add)          (None, 28, 28, 256)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_bn (BatchNo (None, 28, 28, 256)  1024        conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_relu (Activ (None, 28, 28, 256)  0           conv3_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Add)          (None, 28, 28, 512)  0           conv3_block1_0_conv[0][0]        \n",
            "                                                                 conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 14, 14, 128)  147456      conv3_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 14, 14, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 14, 14, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 512)  0           conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 14, 14, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Add)          (None, 14, 14, 512)  0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_bn (BatchNo (None, 14, 14, 512)  2048        conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_relu (Activ (None, 14, 14, 512)  0           conv4_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_conv[0][0]        \n",
            "                                                                 conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block5_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    589824      conv4_block6_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Add)          (None, 7, 7, 1024)   0           max_pooling2d_3[0][0]            \n",
            "                                                                 conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_bn (BatchNo (None, 7, 7, 1024)   4096        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_relu (Activ (None, 7, 7, 1024)   0           conv5_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524288      conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_conv[0][0]        \n",
            "                                                                 conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "post_bn (BatchNormalization)    (None, 7, 7, 2048)   8192        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "post_relu (Activation)          (None, 7, 7, 2048)   0           post_bn[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 100352)       0           post_relu[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          51380736    flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          65664       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          65664       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          65664       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          65664       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          65664       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          65664       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          65664       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          65664       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 75,474,331\n",
            "Trainable params: 75,428,891\n",
            "Non-trainable params: 45,440\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrzs6dYLS4S6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CyclicLR(keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self,base_lr, max_lr, step_size, base_m, max_m, cyclical_momentum):\n",
        " \n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.base_m = base_m\n",
        "        self.max_m = max_m\n",
        "        self.cyclical_momentum = cyclical_momentum\n",
        "        self.step_size = step_size\n",
        "        \n",
        "        self.clr_iterations = 0.\n",
        "        self.cm_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "        \n",
        "    def clr(self):\n",
        "        \n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        \n",
        "        if cycle == 2:\n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)          \n",
        "            return self.base_lr-(self.base_lr-self.base_lr/100)*np.maximum(0,(1-x))\n",
        "        \n",
        "        else:\n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0,(1-x))\n",
        "    \n",
        "    def cm(self):\n",
        "        \n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        \n",
        "        if cycle == 2:\n",
        "            \n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1) \n",
        "            return self.max_m\n",
        "        \n",
        "        else:\n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "            return self.max_m - (self.max_m-self.base_m)*np.maximum(0,(1-x))\n",
        "        \n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "            \n",
        "        if self.cyclical_momentum == True:\n",
        "            if self.clr_iterations == 0:\n",
        "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
        "            else:\n",
        "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
        "            \n",
        "            \n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "        \n",
        "        if self.cyclical_momentum == True:\n",
        "            self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "        \n",
        "        if self.cyclical_momentum == True:\n",
        "            K.set_value(self.model.optimizer.momentum, self.cm())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNLL-xNnD3ng",
        "colab_type": "code",
        "outputId": "b771eaa1-885e-4a37-dd53-3bc6346ac1ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "losses = {\n",
        "\t\"gender_output\": \"binary_crossentropy\",\n",
        "\t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\t\"age_output\": \"categorical_crossentropy\",\n",
        "\t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0, \"weight_output\": 1.0, \"bag_output\": 1.0, \"footwear_output\": 1.0, \"pose_output\": 1.0, \"emotion_output\": 1.0}\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    #loss=losses, \n",
        "    #loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AR_V4h4kpZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 100\n",
        "max_lr = 0.0001\n",
        "base_lr = max_lr/100\n",
        "max_m = 0.98\n",
        "base_m = 0.85\n",
        "\n",
        "augment = True\n",
        "cycles = 8\n",
        "\n",
        "iterations = round(len(train_df)/32*epochs)\n",
        "iterations = list(range(0,iterations+1))\n",
        "step_size = len(iterations)/(cycles)\n",
        "\n",
        "\n",
        "clr =  CyclicLR(base_lr=base_lr,\n",
        "                max_lr=max_lr,\n",
        "                step_size=step_size,\n",
        "                max_m=max_m,\n",
        "                base_m=base_m,\n",
        "                cyclical_momentum=True)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists('gdrive/My Drive/saved_models'):\n",
        "    os.mkdir('gdrive/My Drive/saved_models')\n",
        "LOGS_DIR = 'gdrive/My Drive/saved_models'\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
        "csv_logger = CSVLogger(LOGS_DIR +'/'+'log.csv')\n",
        "filepath=os.path.join(LOGS_DIR+os.sep+'model-{epoch:03d}-{val_loss:.4f}.hdf5')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVGh9H-ExDOu",
        "colab_type": "code",
        "outputId": "fd4f656d-7c45-479b-d744-61c2a92d4825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    #class_weight= class_weights,\n",
        "    #callbacks=[lr_reducer, early_stopper, csv_logger, checkpoint, LearningRateScheduler(scheduler, verbose=1)],\n",
        "    callbacks=[checkpoint,clr]\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "339/339 [==============================] - 207s 612ms/step - loss: 8.5421 - gender_output_loss: 0.7273 - image_quality_output_loss: 1.0463 - age_output_loss: 1.5673 - weight_output_loss: 1.0768 - bag_output_loss: 1.0121 - footwear_output_loss: 1.0857 - pose_output_loss: 0.9867 - emotion_output_loss: 1.0399 - gender_output_acc: 0.5260 - image_quality_output_acc: 0.5084 - age_output_acc: 0.3273 - weight_output_acc: 0.5982 - bag_output_acc: 0.5086 - footwear_output_acc: 0.4575 - pose_output_acc: 0.5871 - emotion_output_acc: 0.6650 - val_loss: 8.3247 - val_gender_output_loss: 0.7489 - val_image_quality_output_loss: 1.0361 - val_age_output_loss: 1.4986 - val_weight_output_loss: 1.0491 - val_bag_output_loss: 0.9682 - val_footwear_output_loss: 1.0765 - val_pose_output_loss: 0.9762 - val_emotion_output_loss: 0.9711 - val_gender_output_acc: 0.5454 - val_image_quality_output_acc: 0.5193 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6272 - val_bag_output_acc: 0.5379 - val_footwear_output_acc: 0.4877 - val_pose_output_acc: 0.5956 - val_emotion_output_acc: 0.7169\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 8.32474, saving model to gdrive/My Drive/saved_models/model-001-8.3247.hdf5\n",
            "Epoch 2/100\n",
            "339/339 [==============================] - 193s 570ms/step - loss: 8.0379 - gender_output_loss: 0.6990 - image_quality_output_loss: 1.0094 - age_output_loss: 1.4699 - weight_output_loss: 1.0123 - bag_output_loss: 0.9414 - footwear_output_loss: 1.0174 - pose_output_loss: 0.9462 - emotion_output_loss: 0.9423 - gender_output_acc: 0.5375 - image_quality_output_acc: 0.5366 - age_output_acc: 0.3793 - weight_output_acc: 0.6300 - bag_output_acc: 0.5394 - footwear_output_acc: 0.4980 - pose_output_acc: 0.6134 - emotion_output_acc: 0.7081 - val_loss: 7.9050 - val_gender_output_loss: 0.6907 - val_image_quality_output_loss: 0.9892 - val_age_output_loss: 1.4558 - val_weight_output_loss: 0.9969 - val_bag_output_loss: 0.9288 - val_footwear_output_loss: 1.0046 - val_pose_output_loss: 0.9393 - val_emotion_output_loss: 0.8997 - val_gender_output_acc: 0.5435 - val_image_quality_output_acc: 0.5376 - val_age_output_acc: 0.3910 - val_weight_output_acc: 0.6354 - val_bag_output_acc: 0.5339 - val_footwear_output_acc: 0.5093 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00002: val_loss improved from 8.32474 to 7.90498, saving model to gdrive/My Drive/saved_models/model-002-7.9050.hdf5\n",
            "Epoch 3/100\n",
            "339/339 [==============================] - 194s 572ms/step - loss: 7.9815 - gender_output_loss: 0.6938 - image_quality_output_loss: 1.0013 - age_output_loss: 1.4560 - weight_output_loss: 1.0060 - bag_output_loss: 0.9364 - footwear_output_loss: 1.0059 - pose_output_loss: 0.9459 - emotion_output_loss: 0.9362 - gender_output_acc: 0.5496 - image_quality_output_acc: 0.5435 - age_output_acc: 0.3764 - weight_output_acc: 0.6329 - bag_output_acc: 0.5489 - footwear_output_acc: 0.5041 - pose_output_acc: 0.6164 - emotion_output_acc: 0.7094 - val_loss: 7.8889 - val_gender_output_loss: 0.6938 - val_image_quality_output_loss: 0.9895 - val_age_output_loss: 1.4575 - val_weight_output_loss: 0.9909 - val_bag_output_loss: 0.9280 - val_footwear_output_loss: 0.9967 - val_pose_output_loss: 0.9308 - val_emotion_output_loss: 0.9016 - val_gender_output_acc: 0.5487 - val_image_quality_output_acc: 0.5517 - val_age_output_acc: 0.3806 - val_weight_output_acc: 0.6347 - val_bag_output_acc: 0.5487 - val_footwear_output_acc: 0.5141 - val_pose_output_acc: 0.6157 - val_emotion_output_acc: 0.7176\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.90498 to 7.88894, saving model to gdrive/My Drive/saved_models/model-003-7.8889.hdf5\n",
            "Epoch 4/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.9383 - gender_output_loss: 0.6913 - image_quality_output_loss: 0.9950 - age_output_loss: 1.4555 - weight_output_loss: 0.9966 - bag_output_loss: 0.9326 - footwear_output_loss: 0.9973 - pose_output_loss: 0.9384 - emotion_output_loss: 0.9317 - gender_output_acc: 0.5515 - image_quality_output_acc: 0.5472 - age_output_acc: 0.3802 - weight_output_acc: 0.6324 - bag_output_acc: 0.5531 - footwear_output_acc: 0.5072 - pose_output_acc: 0.6175 - emotion_output_acc: 0.7097\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "339/339 [==============================] - 193s 571ms/step - loss: 7.9377 - gender_output_loss: 0.6912 - image_quality_output_loss: 0.9946 - age_output_loss: 1.4554 - weight_output_loss: 0.9963 - bag_output_loss: 0.9321 - footwear_output_loss: 0.9976 - pose_output_loss: 0.9387 - emotion_output_loss: 0.9316 - gender_output_acc: 0.5516 - image_quality_output_acc: 0.5477 - age_output_acc: 0.3801 - weight_output_acc: 0.6327 - bag_output_acc: 0.5532 - footwear_output_acc: 0.5072 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7096 - val_loss: 7.8753 - val_gender_output_loss: 0.6850 - val_image_quality_output_loss: 0.9904 - val_age_output_loss: 1.4456 - val_weight_output_loss: 0.9926 - val_bag_output_loss: 0.9288 - val_footwear_output_loss: 0.9958 - val_pose_output_loss: 0.9414 - val_emotion_output_loss: 0.8958 - val_gender_output_acc: 0.5569 - val_image_quality_output_acc: 0.5525 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6332 - val_bag_output_acc: 0.5357 - val_footwear_output_acc: 0.5126 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7176\n",
            "\n",
            "Epoch 00004: val_loss improved from 7.88894 to 7.87528, saving model to gdrive/My Drive/saved_models/model-004-7.8753.hdf5\n",
            "Epoch 5/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.9182 - gender_output_loss: 0.6871 - image_quality_output_loss: 0.9954 - age_output_loss: 1.4452 - weight_output_loss: 0.9964 - bag_output_loss: 0.9272 - footwear_output_loss: 0.9953 - pose_output_loss: 0.9374 - emotion_output_loss: 0.9341 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5473 - age_output_acc: 0.3865 - weight_output_acc: 0.6339 - bag_output_acc: 0.5582 - footwear_output_acc: 0.5197 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7100\n",
            "339/339 [==============================] - 193s 570ms/step - loss: 7.9179 - gender_output_loss: 0.6872 - image_quality_output_loss: 0.9952 - age_output_loss: 1.4448 - weight_output_loss: 0.9959 - bag_output_loss: 0.9277 - footwear_output_loss: 0.9955 - pose_output_loss: 0.9373 - emotion_output_loss: 0.9343 - gender_output_acc: 0.5629 - image_quality_output_acc: 0.5478 - age_output_acc: 0.3866 - weight_output_acc: 0.6342 - bag_output_acc: 0.5578 - footwear_output_acc: 0.5198 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7099 - val_loss: 7.8376 - val_gender_output_loss: 0.6866 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4396 - val_weight_output_loss: 0.9955 - val_bag_output_loss: 0.9245 - val_footwear_output_loss: 0.9771 - val_pose_output_loss: 0.9288 - val_emotion_output_loss: 0.9031 - val_gender_output_acc: 0.5465 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.3966 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5528 - val_footwear_output_acc: 0.5253 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00005: val_loss improved from 7.87528 to 7.83761, saving model to gdrive/My Drive/saved_models/model-005-7.8376.hdf5\n",
            "Epoch 6/100\n",
            "339/339 [==============================] - 192s 567ms/step - loss: 7.8998 - gender_output_loss: 0.6836 - image_quality_output_loss: 0.9941 - age_output_loss: 1.4426 - weight_output_loss: 0.9985 - bag_output_loss: 0.9261 - footwear_output_loss: 0.9856 - pose_output_loss: 0.9380 - emotion_output_loss: 0.9313 - gender_output_acc: 0.5670 - image_quality_output_acc: 0.5467 - age_output_acc: 0.3849 - weight_output_acc: 0.6336 - bag_output_acc: 0.5600 - footwear_output_acc: 0.5235 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7094 - val_loss: 7.8426 - val_gender_output_loss: 0.6841 - val_image_quality_output_loss: 0.9887 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9901 - val_bag_output_loss: 0.9241 - val_footwear_output_loss: 0.9853 - val_pose_output_loss: 0.9370 - val_emotion_output_loss: 0.8970 - val_gender_output_acc: 0.5525 - val_image_quality_output_acc: 0.5528 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5506 - val_footwear_output_acc: 0.5242 - val_pose_output_acc: 0.6131 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 7.83761\n",
            "Epoch 7/100\n",
            "339/339 [==============================] - 192s 567ms/step - loss: 7.8647 - gender_output_loss: 0.6834 - image_quality_output_loss: 0.9896 - age_output_loss: 1.4376 - weight_output_loss: 0.9939 - bag_output_loss: 0.9255 - footwear_output_loss: 0.9738 - pose_output_loss: 0.9318 - emotion_output_loss: 0.9289 - gender_output_acc: 0.5639 - image_quality_output_acc: 0.5497 - age_output_acc: 0.3897 - weight_output_acc: 0.6357 - bag_output_acc: 0.5579 - footwear_output_acc: 0.5405 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7098 - val_loss: 7.8072 - val_gender_output_loss: 0.6794 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.4431 - val_weight_output_loss: 0.9865 - val_bag_output_loss: 0.9235 - val_footwear_output_loss: 0.9737 - val_pose_output_loss: 0.9303 - val_emotion_output_loss: 0.8930 - val_gender_output_acc: 0.5688 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3955 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5472 - val_footwear_output_acc: 0.5257 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================]\n",
            "Epoch 00007: val_loss improved from 7.83761 to 7.80723, saving model to gdrive/My Drive/saved_models/model-007-7.8072.hdf5\n",
            "Epoch 8/100\n",
            "339/339 [==============================] - 193s 568ms/step - loss: 7.8556 - gender_output_loss: 0.6788 - image_quality_output_loss: 0.9868 - age_output_loss: 1.4337 - weight_output_loss: 0.9950 - bag_output_loss: 0.9261 - footwear_output_loss: 0.9757 - pose_output_loss: 0.9335 - emotion_output_loss: 0.9261 - gender_output_acc: 0.5725 - image_quality_output_acc: 0.5496 - age_output_acc: 0.3909 - weight_output_acc: 0.6351 - bag_output_acc: 0.5576 - footwear_output_acc: 0.5381 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7100 - val_loss: 7.8171 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.4395 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.9235 - val_footwear_output_loss: 0.9742 - val_pose_output_loss: 0.9269 - val_emotion_output_loss: 0.9045 - val_gender_output_acc: 0.5662 - val_image_quality_output_acc: 0.5584 - val_age_output_acc: 0.3925 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5521 - val_footwear_output_acc: 0.5379 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 7.80723\n",
            "Epoch 9/100\n",
            "339/339 [==============================] - 194s 571ms/step - loss: 7.8433 - gender_output_loss: 0.6784 - image_quality_output_loss: 0.9881 - age_output_loss: 1.4343 - weight_output_loss: 0.9961 - bag_output_loss: 0.9231 - footwear_output_loss: 0.9651 - pose_output_loss: 0.9317 - emotion_output_loss: 0.9264 - gender_output_acc: 0.5720 - image_quality_output_acc: 0.5510 - age_output_acc: 0.3899 - weight_output_acc: 0.6347 - bag_output_acc: 0.5614 - footwear_output_acc: 0.5453 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7098 - val_loss: 7.7619 - val_gender_output_loss: 0.6747 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 1.4332 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.9178 - val_footwear_output_loss: 0.9607 - val_pose_output_loss: 0.9215 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.5818 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5428 - val_footwear_output_acc: 0.5391 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: val_loss improved from 7.80723 to 7.76188, saving model to gdrive/My Drive/saved_models/model-009-7.7619.hdf5\n",
            "Epoch 10/100\n",
            "339/339 [==============================] - 193s 570ms/step - loss: 7.8192 - gender_output_loss: 0.6729 - image_quality_output_loss: 0.9831 - age_output_loss: 1.4328 - weight_output_loss: 0.9953 - bag_output_loss: 0.9230 - footwear_output_loss: 0.9562 - pose_output_loss: 0.9297 - emotion_output_loss: 0.9262 - gender_output_acc: 0.5825 - image_quality_output_acc: 0.5504 - age_output_acc: 0.3893 - weight_output_acc: 0.6351 - bag_output_acc: 0.5561 - footwear_output_acc: 0.5501 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7100 - val_loss: 7.7536 - val_gender_output_loss: 0.6719 - val_image_quality_output_loss: 0.9756 - val_age_output_loss: 1.4313 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.9182 - val_footwear_output_loss: 0.9584 - val_pose_output_loss: 0.9232 - val_emotion_output_loss: 0.8907 - val_gender_output_acc: 0.5897 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3925 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5528 - val_footwear_output_acc: 0.5357 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00010: val_loss improved from 7.76188 to 7.75356, saving model to gdrive/My Drive/saved_models/model-010-7.7536.hdf5\n",
            "Epoch 11/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.8035 - gender_output_loss: 0.6739 - image_quality_output_loss: 0.9871 - age_output_loss: 1.4276 - weight_output_loss: 0.9916 - bag_output_loss: 0.9178 - footwear_output_loss: 0.9555 - pose_output_loss: 0.9313 - emotion_output_loss: 0.9188 - gender_output_acc: 0.5805 - image_quality_output_acc: 0.5504 - age_output_acc: 0.3906 - weight_output_acc: 0.6348 - bag_output_acc: 0.5597 - footwear_output_acc: 0.5509 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7102 - val_loss: 7.7352 - val_gender_output_loss: 0.6708 - val_image_quality_output_loss: 0.9741 - val_age_output_loss: 1.4282 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.9169 - val_footwear_output_loss: 0.9430 - val_pose_output_loss: 0.9201 - val_emotion_output_loss: 0.8987 - val_gender_output_acc: 0.5811 - val_image_quality_output_acc: 0.5584 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5413 - val_footwear_output_acc: 0.5610 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00011: val_loss improved from 7.75356 to 7.73523, saving model to gdrive/My Drive/saved_models/model-011-7.7352.hdf5\n",
            "Epoch 12/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.7961 - gender_output_loss: 0.6697 - image_quality_output_loss: 0.9880 - age_output_loss: 1.4297 - weight_output_loss: 0.9915 - bag_output_loss: 0.9179 - footwear_output_loss: 0.9494 - pose_output_loss: 0.9276 - emotion_output_loss: 0.9224 - gender_output_acc: 0.5862 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3885 - weight_output_acc: 0.6342 - bag_output_acc: 0.5638 - footwear_output_acc: 0.5522 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7098 - val_loss: 7.7214 - val_gender_output_loss: 0.6682 - val_image_quality_output_loss: 0.9763 - val_age_output_loss: 1.4262 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.9140 - val_footwear_output_loss: 0.9415 - val_pose_output_loss: 0.9200 - val_emotion_output_loss: 0.8923 - val_gender_output_acc: 0.5844 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5521 - val_footwear_output_acc: 0.5558 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00012: val_loss improved from 7.73523 to 7.72137, saving model to gdrive/My Drive/saved_models/model-012-7.7214.hdf5\n",
            "Epoch 13/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.7774 - gender_output_loss: 0.6698 - image_quality_output_loss: 0.9835 - age_output_loss: 1.4240 - weight_output_loss: 0.9846 - bag_output_loss: 0.9177 - footwear_output_loss: 0.9448 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9253 - gender_output_acc: 0.5840 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3934 - weight_output_acc: 0.6348 - bag_output_acc: 0.5626 - footwear_output_acc: 0.5582 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7096\n",
            "Epoch 00012: val_loss improved from 7.73523 to 7.72137, saving model to gdrive/My Drive/saved_models/model-012-7.7214.hdf5\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.7771 - gender_output_loss: 0.6697 - image_quality_output_loss: 0.9837 - age_output_loss: 1.4239 - weight_output_loss: 0.9852 - bag_output_loss: 0.9173 - footwear_output_loss: 0.9449 - pose_output_loss: 0.9276 - emotion_output_loss: 0.9249 - gender_output_acc: 0.5843 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3935 - weight_output_acc: 0.6347 - bag_output_acc: 0.5628 - footwear_output_acc: 0.5582 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7098 - val_loss: 7.7413 - val_gender_output_loss: 0.6687 - val_image_quality_output_loss: 0.9737 - val_age_output_loss: 1.4347 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.9162 - val_footwear_output_loss: 0.9488 - val_pose_output_loss: 0.9254 - val_emotion_output_loss: 0.8937 - val_gender_output_acc: 0.6023 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5446 - val_footwear_output_acc: 0.5536 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 7.72137\n",
            "Epoch 14/100\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.7647 - gender_output_loss: 0.6701 - image_quality_output_loss: 0.9816 - age_output_loss: 1.4207 - weight_output_loss: 0.9891 - bag_output_loss: 0.9151 - footwear_output_loss: 0.9403 - pose_output_loss: 0.9242 - emotion_output_loss: 0.9237 - gender_output_acc: 0.5768 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3944 - weight_output_acc: 0.6357 - bag_output_acc: 0.5645 - footwear_output_acc: 0.5659 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7099 - val_loss: 7.7106 - val_gender_output_loss: 0.6671 - val_image_quality_output_loss: 0.9728 - val_age_output_loss: 1.4216 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9156 - val_footwear_output_loss: 0.9354 - val_pose_output_loss: 0.9234 - val_emotion_output_loss: 0.8959 - val_gender_output_acc: 0.5826 - val_image_quality_output_acc: 0.5517 - val_age_output_acc: 0.3966 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5417 - val_footwear_output_acc: 0.5707 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 00014: val_loss improved from 7.72137 to 7.71063, saving model to gdrive/My Drive/saved_models/model-014-7.7106.hdf5\n",
            "Epoch 15/100\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.7633 - gender_output_loss: 0.6685 - image_quality_output_loss: 0.9842 - age_output_loss: 1.4219 - weight_output_loss: 0.9861 - bag_output_loss: 0.9130 - footwear_output_loss: 0.9394 - pose_output_loss: 0.9262 - emotion_output_loss: 0.9238 - gender_output_acc: 0.5841 - image_quality_output_acc: 0.5510 - age_output_acc: 0.3907 - weight_output_acc: 0.6359 - bag_output_acc: 0.5605 - footwear_output_acc: 0.5658 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7100 - val_loss: 7.6909 - val_gender_output_loss: 0.6595 - val_image_quality_output_loss: 0.9729 - val_age_output_loss: 1.4200 - val_weight_output_loss: 0.9817 - val_bag_output_loss: 0.9129 - val_footwear_output_loss: 0.9310 - val_pose_output_loss: 0.9207 - val_emotion_output_loss: 0.8920 - val_gender_output_acc: 0.5938 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5547 - val_footwear_output_acc: 0.5618 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00015: val_loss improved from 7.71063 to 7.69089, saving model to gdrive/My Drive/saved_models/model-015-7.6909.hdf5\n",
            "Epoch 16/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.7491 - gender_output_loss: 0.6643 - image_quality_output_loss: 0.9846 - age_output_loss: 1.4179 - weight_output_loss: 0.9874 - bag_output_loss: 0.9155 - footwear_output_loss: 0.9352 - pose_output_loss: 0.9233 - emotion_output_loss: 0.9207 - gender_output_acc: 0.5878 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3929 - weight_output_acc: 0.6351 - bag_output_acc: 0.5620 - footwear_output_acc: 0.5633 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7103\n",
            "339/339 [==============================] - 195s 577ms/step - loss: 7.7500 - gender_output_loss: 0.6643 - image_quality_output_loss: 0.9846 - age_output_loss: 1.4185 - weight_output_loss: 0.9874 - bag_output_loss: 0.9154 - footwear_output_loss: 0.9351 - pose_output_loss: 0.9230 - emotion_output_loss: 0.9218 - gender_output_acc: 0.5880 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3927 - weight_output_acc: 0.6350 - bag_output_acc: 0.5620 - footwear_output_acc: 0.5635 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7099 - val_loss: 7.6810 - val_gender_output_loss: 0.6578 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.4190 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.9129 - val_footwear_output_loss: 0.9281 - val_pose_output_loss: 0.9171 - val_emotion_output_loss: 0.8879 - val_gender_output_acc: 0.5956 - val_image_quality_output_acc: 0.5532 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5543 - val_footwear_output_acc: 0.5789 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00016: val_loss improved from 7.69089 to 7.68098, saving model to gdrive/My Drive/saved_models/model-016-7.6810.hdf5\n",
            "Epoch 17/100\n",
            "339/339 [==============================] - 194s 573ms/step - loss: 7.7409 - gender_output_loss: 0.6613 - image_quality_output_loss: 0.9848 - age_output_loss: 1.4140 - weight_output_loss: 0.9855 - bag_output_loss: 0.9168 - footwear_output_loss: 0.9320 - pose_output_loss: 0.9240 - emotion_output_loss: 0.9225 - gender_output_acc: 0.5954 - image_quality_output_acc: 0.5507 - age_output_acc: 0.3959 - weight_output_acc: 0.6355 - bag_output_acc: 0.5608 - footwear_output_acc: 0.5672 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7097 - val_loss: 7.6734 - val_gender_output_loss: 0.6572 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.4232 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9126 - val_footwear_output_loss: 0.9200 - val_pose_output_loss: 0.9204 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.5949 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.5748 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00017: val_loss improved from 7.68098 to 7.67345, saving model to gdrive/My Drive/saved_models/model-017-7.6734.hdf5\n",
            "Epoch 18/100\n",
            "339/339 [==============================] - 197s 580ms/step - loss: 7.7192 - gender_output_loss: 0.6597 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4157 - weight_output_loss: 0.9860 - bag_output_loss: 0.9097 - footwear_output_loss: 0.9282 - pose_output_loss: 0.9201 - emotion_output_loss: 0.9192 - gender_output_acc: 0.5942 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3973 - weight_output_acc: 0.6360 - bag_output_acc: 0.5606 - footwear_output_acc: 0.5687 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7097 - val_loss: 7.6640 - val_gender_output_loss: 0.6564 - val_image_quality_output_loss: 0.9744 - val_age_output_loss: 1.4154 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.9124 - val_footwear_output_loss: 0.9265 - val_pose_output_loss: 0.9160 - val_emotion_output_loss: 0.8856 - val_gender_output_acc: 0.6019 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3966 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5528 - val_footwear_output_acc: 0.5666 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00018: val_loss improved from 7.67345 to 7.66404, saving model to gdrive/My Drive/saved_models/model-018-7.6640.hdf5\n",
            "Epoch 19/100\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.7152 - gender_output_loss: 0.6555 - image_quality_output_loss: 0.9793 - age_output_loss: 1.4172 - weight_output_loss: 0.9846 - bag_output_loss: 0.9106 - footwear_output_loss: 0.9308 - pose_output_loss: 0.9206 - emotion_output_loss: 0.9165 - gender_output_acc: 0.6000 - image_quality_output_acc: 0.5514 - age_output_acc: 0.3950 - weight_output_acc: 0.6344 - bag_output_acc: 0.5623 - footwear_output_acc: 0.5702 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7098 - val_loss: 7.6584 - val_gender_output_loss: 0.6562 - val_image_quality_output_loss: 0.9775 - val_age_output_loss: 1.4170 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.9100 - val_footwear_output_loss: 0.9181 - val_pose_output_loss: 0.9116 - val_emotion_output_loss: 0.8888 - val_gender_output_acc: 0.5878 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5484 - val_footwear_output_acc: 0.5815 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00019: val_loss improved from 7.66404 to 7.65842, saving model to gdrive/My Drive/saved_models/model-019-7.6584.hdf5\n",
            "Epoch 20/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.7062 - gender_output_loss: 0.6565 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4122 - weight_output_loss: 0.9872 - bag_output_loss: 0.9088 - footwear_output_loss: 0.9236 - pose_output_loss: 0.9187 - emotion_output_loss: 0.9178 - gender_output_acc: 0.5983 - image_quality_output_acc: 0.5515 - age_output_acc: 0.3945 - weight_output_acc: 0.6356 - bag_output_acc: 0.5649 - footwear_output_acc: 0.5772 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7098 - val_loss: 7.6514 - val_gender_output_loss: 0.6522 - val_image_quality_output_loss: 0.9714 - val_age_output_loss: 1.4173 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.9102 - val_footwear_output_loss: 0.9185 - val_pose_output_loss: 0.9136 - val_emotion_output_loss: 0.8917 - val_gender_output_acc: 0.5897 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.5830 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00020: val_loss improved from 7.65842 to 7.65144, saving model to gdrive/My Drive/saved_models/model-020-7.6514.hdf5\n",
            "Epoch 21/100\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.6969 - gender_output_loss: 0.6526 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4107 - weight_output_loss: 0.9839 - bag_output_loss: 0.9094 - footwear_output_loss: 0.9275 - pose_output_loss: 0.9160 - emotion_output_loss: 0.9160 - gender_output_acc: 0.6047 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3992 - weight_output_acc: 0.6349 - bag_output_acc: 0.5653 - footwear_output_acc: 0.5716 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7103 - val_loss: 7.6502 - val_gender_output_loss: 0.6557 - val_image_quality_output_loss: 0.9740 - val_age_output_loss: 1.4150 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.9087 - val_footwear_output_loss: 0.9163 - val_pose_output_loss: 0.9095 - val_emotion_output_loss: 0.8886 - val_gender_output_acc: 0.5960 - val_image_quality_output_acc: 0.5513 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5759 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.65144 to 7.65019, saving model to gdrive/My Drive/saved_models/model-021-7.6502.hdf5\n",
            "Epoch 22/100\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.6969 - gender_output_loss: 0.6526 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4107 - weight_output_loss: 0.9839 - bag_output_loss: 0.9094 - footwear_output_loss: 0.9275 - pose_output_loss: 0.9160 - emotion_output_loss: 0.9160 - gender_output_acc: 0.6047 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3992 - weight_output_acc: 0.6349 - bag_output_acc: 0.5653 - footwear_output_acc: 0.5716 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7103 - val_loss: 7.6502 - val_gender_output_loss: 0.6557 - val_image_quality_output_loss: 0.9740 - val_age_output_loss: 1.4150 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.9087 - val_footwear_output_loss: 0.9163 - val_pose_output_loss: 0.9095 - val_emotion_output_loss: 0.8886 - val_gender_output_acc: 0.5960 - val_image_quality_output_acc: 0.5513 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5759 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6939 - gender_output_loss: 0.6515 - image_quality_output_loss: 0.9796 - age_output_loss: 1.4139 - weight_output_loss: 0.9815 - bag_output_loss: 0.9068 - footwear_output_loss: 0.9262 - pose_output_loss: 0.9196 - emotion_output_loss: 0.9148 - gender_output_acc: 0.6077 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3952 - weight_output_acc: 0.6355 - bag_output_acc: 0.5643 - footwear_output_acc: 0.5736 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7100 - val_loss: 7.6447 - val_gender_output_loss: 0.6513 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.4116 - val_weight_output_loss: 0.9764 - val_bag_output_loss: 0.9088 - val_footwear_output_loss: 0.9188 - val_pose_output_loss: 0.9151 - val_emotion_output_loss: 0.8892 - val_gender_output_acc: 0.5990 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5558 - val_footwear_output_acc: 0.5740 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00022: val_loss improved from 7.65019 to 7.64473, saving model to gdrive/My Drive/saved_models/model-022-7.6447.hdf5\n",
            "Epoch 23/100\n",
            "339/339 [==============================] - 197s 582ms/step - loss: 7.6756 - gender_output_loss: 0.6504 - image_quality_output_loss: 0.9796 - age_output_loss: 1.4055 - weight_output_loss: 0.9817 - bag_output_loss: 0.9089 - footwear_output_loss: 0.9199 - pose_output_loss: 0.9147 - emotion_output_loss: 0.9149 - gender_output_acc: 0.6017 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4008 - weight_output_acc: 0.6359 - bag_output_acc: 0.5634 - footwear_output_acc: 0.5742 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7100 - val_loss: 7.6209 - val_gender_output_loss: 0.6480 - val_image_quality_output_loss: 0.9728 - val_age_output_loss: 1.4076 - val_weight_output_loss: 0.9775 - val_bag_output_loss: 0.9050 - val_footwear_output_loss: 0.9082 - val_pose_output_loss: 0.9140 - val_emotion_output_loss: 0.8879 - val_gender_output_acc: 0.6071 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3969 - val_weight_output_acc: 0.6354 - val_bag_output_acc: 0.5603 - val_footwear_output_acc: 0.5833 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================]\n",
            "Epoch 00023: val_loss improved from 7.64473 to 7.62087, saving model to gdrive/My Drive/saved_models/model-023-7.6209.hdf5\n",
            "Epoch 24/100\n",
            "339/339 [==============================] - 196s 578ms/step - loss: 7.6696 - gender_output_loss: 0.6499 - image_quality_output_loss: 0.9778 - age_output_loss: 1.4066 - weight_output_loss: 0.9820 - bag_output_loss: 0.9072 - footwear_output_loss: 0.9152 - pose_output_loss: 0.9149 - emotion_output_loss: 0.9162 - gender_output_acc: 0.6121 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3961 - weight_output_acc: 0.6350 - bag_output_acc: 0.5624 - footwear_output_acc: 0.5805 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7103 - val_loss: 7.6384 - val_gender_output_loss: 0.6479 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.4089 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.9062 - val_footwear_output_loss: 0.9147 - val_pose_output_loss: 0.9151 - val_emotion_output_loss: 0.8901 - val_gender_output_acc: 0.6086 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5569 - val_footwear_output_acc: 0.5737 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 7.62087\n",
            "Epoch 25/100\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6646 - gender_output_loss: 0.6480 - image_quality_output_loss: 0.9772 - age_output_loss: 1.4076 - weight_output_loss: 0.9805 - bag_output_loss: 0.9057 - footwear_output_loss: 0.9157 - pose_output_loss: 0.9153 - emotion_output_loss: 0.9146 - gender_output_acc: 0.6077 - image_quality_output_acc: 0.5518 - age_output_acc: 0.3937 - weight_output_acc: 0.6351 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5789 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7100 - val_loss: 7.6180 - val_gender_output_loss: 0.6483 - val_image_quality_output_loss: 0.9721 - val_age_output_loss: 1.4093 - val_weight_output_loss: 0.9776 - val_bag_output_loss: 0.9053 - val_footwear_output_loss: 0.9096 - val_pose_output_loss: 0.9094 - val_emotion_output_loss: 0.8864 - val_gender_output_acc: 0.6034 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5577 - val_footwear_output_acc: 0.5774 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 7.62087\n",
            "Epoch 00025: val_loss improved from 7.62087 to 7.61804, saving model to gdrive/My Drive/saved_models/model-025-7.6180.hdf5\n",
            "Epoch 26/100\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6453 - gender_output_loss: 0.6479 - image_quality_output_loss: 0.9754 - age_output_loss: 1.4057 - weight_output_loss: 0.9818 - bag_output_loss: 0.9034 - footwear_output_loss: 0.9064 - pose_output_loss: 0.9123 - emotion_output_loss: 0.9123 - gender_output_acc: 0.6118 - image_quality_output_acc: 0.5525 - age_output_acc: 0.4010 - weight_output_acc: 0.6353 - bag_output_acc: 0.5678 - footwear_output_acc: 0.5817 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7099 - val_loss: 7.6146 - val_gender_output_loss: 0.6459 - val_image_quality_output_loss: 0.9729 - val_age_output_loss: 1.4105 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9047 - val_footwear_output_loss: 0.9072 - val_pose_output_loss: 0.9088 - val_emotion_output_loss: 0.8875 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3999 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5536 - val_footwear_output_acc: 0.5807 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00025: val_loss improved from 7.62087 to 7.61804, saving model to gdrive/My Drive/saved_models/model-025-7.6180.hdf5\n",
            "\n",
            "Epoch 00026: val_loss improved from 7.61804 to 7.61462, saving model to gdrive/My Drive/saved_models/model-026-7.6146.hdf5\n",
            "Epoch 27/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6505 - gender_output_loss: 0.6466 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4070 - weight_output_loss: 0.9804 - bag_output_loss: 0.9053 - footwear_output_loss: 0.9090 - pose_output_loss: 0.9123 - emotion_output_loss: 0.9137 - gender_output_acc: 0.6072 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3980 - weight_output_acc: 0.6361 - bag_output_acc: 0.5680 - footwear_output_acc: 0.5823 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7095\n",
            "Epoch 00026: val_loss improved from 7.61804 to 7.61462, saving model to gdrive/My Drive/saved_models/model-026-7.6146.hdf5\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.6509 - gender_output_loss: 0.6463 - image_quality_output_loss: 0.9771 - age_output_loss: 1.4071 - weight_output_loss: 0.9807 - bag_output_loss: 0.9054 - footwear_output_loss: 0.9086 - pose_output_loss: 0.9128 - emotion_output_loss: 0.9129 - gender_output_acc: 0.6077 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3979 - weight_output_acc: 0.6360 - bag_output_acc: 0.5679 - footwear_output_acc: 0.5826 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7099 - val_loss: 7.6115 - val_gender_output_loss: 0.6465 - val_image_quality_output_loss: 0.9728 - val_age_output_loss: 1.4091 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9036 - val_footwear_output_loss: 0.9066 - val_pose_output_loss: 0.9087 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.6124 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5543 - val_footwear_output_acc: 0.5811 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00027: val_loss improved from 7.61462 to 7.61148, saving model to gdrive/My Drive/saved_models/model-027-7.6115.hdf5\n",
            "Epoch 28/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6474 - gender_output_loss: 0.6429 - image_quality_output_loss: 0.9769 - age_output_loss: 1.4082 - weight_output_loss: 0.9800 - bag_output_loss: 0.9042 - footwear_output_loss: 0.9095 - pose_output_loss: 0.9103 - emotion_output_loss: 0.9154 - gender_output_acc: 0.6128 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3972 - weight_output_acc: 0.6351 - bag_output_acc: 0.5680 - footwear_output_acc: 0.5784 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7095\n",
            "Epoch 00027: val_loss improved from 7.61462 to 7.61148, saving model to gdrive/My Drive/saved_models/model-027-7.6115.hdf5\n",
            "339/339 [==============================] - 195s 577ms/step - loss: 7.6456 - gender_output_loss: 0.6428 - image_quality_output_loss: 0.9770 - age_output_loss: 1.4077 - weight_output_loss: 0.9796 - bag_output_loss: 0.9041 - footwear_output_loss: 0.9092 - pose_output_loss: 0.9106 - emotion_output_loss: 0.9147 - gender_output_acc: 0.6130 - image_quality_output_acc: 0.5512 - age_output_acc: 0.3975 - weight_output_acc: 0.6351 - bag_output_acc: 0.5682 - footwear_output_acc: 0.5785 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7099 - val_loss: 7.6097 - val_gender_output_loss: 0.6462 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4092 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9025 - val_footwear_output_loss: 0.9067 - val_pose_output_loss: 0.9086 - val_emotion_output_loss: 0.8867 - val_gender_output_acc: 0.6116 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.5815 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00028: val_loss improved from 7.61148 to 7.60966, saving model to gdrive/My Drive/saved_models/model-028-7.6097.hdf5\n",
            "Epoch 29/100\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.6493 - gender_output_loss: 0.6458 - image_quality_output_loss: 0.9781 - age_output_loss: 1.4087 - weight_output_loss: 0.9790 - bag_output_loss: 0.9023 - footwear_output_loss: 0.9121 - pose_output_loss: 0.9107 - emotion_output_loss: 0.9127 - gender_output_acc: 0.6113 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3928 - weight_output_acc: 0.6358 - bag_output_acc: 0.5679 - footwear_output_acc: 0.5807 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7097 - val_loss: 7.6091 - val_gender_output_loss: 0.6463 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4086 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9023 - val_footwear_output_loss: 0.9065 - val_pose_output_loss: 0.9088 - val_emotion_output_loss: 0.8873 - val_gender_output_acc: 0.6094 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5818 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00029: val_loss improved from 7.60966 to 7.60914, saving model to gdrive/My Drive/saved_models/model-029-7.6091.hdf5\n",
            "Epoch 30/100\n",
            "339/339 [==============================] - 200s 589ms/step - loss: 7.6507 - gender_output_loss: 0.6472 - image_quality_output_loss: 0.9772 - age_output_loss: 1.4075 - weight_output_loss: 0.9799 - bag_output_loss: 0.9026 - footwear_output_loss: 0.9096 - pose_output_loss: 0.9135 - emotion_output_loss: 0.9131 - gender_output_acc: 0.6072 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3980 - weight_output_acc: 0.6354 - bag_output_acc: 0.5695 - footwear_output_acc: 0.5831 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7099 - val_loss: 7.6081 - val_gender_output_loss: 0.6462 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4091 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9016 - val_footwear_output_loss: 0.9074 - val_pose_output_loss: 0.9078 - val_emotion_output_loss: 0.8869 - val_gender_output_acc: 0.6083 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5807 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00029: val_loss improved from 7.60966 to 7.60914, saving model to gdrive/My Drive/saved_models/model-029-7.6091.hdf5\n",
            "Epoch 00030: val_loss improved from 7.60914 to 7.60805, saving model to gdrive/My Drive/saved_models/model-030-7.6081.hdf5\n",
            "Epoch 31/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6432 - gender_output_loss: 0.6446 - image_quality_output_loss: 0.9775 - age_output_loss: 1.4067 - weight_output_loss: 0.9799 - bag_output_loss: 0.9008 - footwear_output_loss: 0.9095 - pose_output_loss: 0.9097 - emotion_output_loss: 0.9145 - gender_output_acc: 0.6120 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3925 - weight_output_acc: 0.6354 - bag_output_acc: 0.5681 - footwear_output_acc: 0.5827 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7098\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6446 - gender_output_loss: 0.6445 - image_quality_output_loss: 0.9775 - age_output_loss: 1.4072 - weight_output_loss: 0.9803 - bag_output_loss: 0.9011 - footwear_output_loss: 0.9100 - pose_output_loss: 0.9096 - emotion_output_loss: 0.9144 - gender_output_acc: 0.6124 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3923 - weight_output_acc: 0.6352 - bag_output_acc: 0.5682 - footwear_output_acc: 0.5823 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7097 - val_loss: 7.6091 - val_gender_output_loss: 0.6462 - val_image_quality_output_loss: 0.9730 - val_age_output_loss: 1.4084 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9023 - val_footwear_output_loss: 0.9062 - val_pose_output_loss: 0.9081 - val_emotion_output_loss: 0.8876 - val_gender_output_acc: 0.6105 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5822 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 7.60805\n",
            "Epoch 32/100\n",
            "339/339 [==============================] - 197s 580ms/step - loss: 7.6444 - gender_output_loss: 0.6434 - image_quality_output_loss: 0.9773 - age_output_loss: 1.4067 - weight_output_loss: 0.9804 - bag_output_loss: 0.9018 - footwear_output_loss: 0.9115 - pose_output_loss: 0.9110 - emotion_output_loss: 0.9123 - gender_output_acc: 0.6162 - image_quality_output_acc: 0.5515 - age_output_acc: 0.3987 - weight_output_acc: 0.6353 - bag_output_acc: 0.5644 - footwear_output_acc: 0.5808 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7101 - val_loss: 7.6090 - val_gender_output_loss: 0.6460 - val_image_quality_output_loss: 0.9730 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.9025 - val_footwear_output_loss: 0.9068 - val_pose_output_loss: 0.9077 - val_emotion_output_loss: 0.8873 - val_gender_output_acc: 0.6101 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5804 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 7.60805\n",
            "Epoch 33/100\n",
            "339/339 [==============================] - 199s 586ms/step - loss: 7.6401 - gender_output_loss: 0.6446 - image_quality_output_loss: 0.9772 - age_output_loss: 1.4065 - weight_output_loss: 0.9789 - bag_output_loss: 0.9030 - footwear_output_loss: 0.9102 - pose_output_loss: 0.9104 - emotion_output_loss: 0.9092 - gender_output_acc: 0.6079 - image_quality_output_acc: 0.5536 - age_output_acc: 0.3992 - weight_output_acc: 0.6355 - bag_output_acc: 0.5693 - footwear_output_acc: 0.5796 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7100 - val_loss: 7.6083 - val_gender_output_loss: 0.6458 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.4087 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9023 - val_footwear_output_loss: 0.9064 - val_pose_output_loss: 0.9079 - val_emotion_output_loss: 0.8864 - val_gender_output_acc: 0.6060 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5577 - val_footwear_output_acc: 0.5804 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 7.60805\n",
            "Epoch 34/100\n",
            "339/339 [==============================] - 200s 591ms/step - loss: 7.6323 - gender_output_loss: 0.6423 - image_quality_output_loss: 0.9750 - age_output_loss: 1.4053 - weight_output_loss: 0.9775 - bag_output_loss: 0.9010 - footwear_output_loss: 0.9083 - pose_output_loss: 0.9114 - emotion_output_loss: 0.9116 - gender_output_acc: 0.6188 - image_quality_output_acc: 0.5517 - age_output_acc: 0.3985 - weight_output_acc: 0.6353 - bag_output_acc: 0.5711 - footwear_output_acc: 0.5782 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7101 - val_loss: 7.6092 - val_gender_output_loss: 0.6466 - val_image_quality_output_loss: 0.9729 - val_age_output_loss: 1.4073 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9027 - val_footwear_output_loss: 0.9062 - val_pose_output_loss: 0.9088 - val_emotion_output_loss: 0.8873 - val_gender_output_acc: 0.6053 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5577 - val_footwear_output_acc: 0.5774 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 7.60805\n",
            "Epoch 35/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6427 - gender_output_loss: 0.6442 - image_quality_output_loss: 0.9755 - age_output_loss: 1.4049 - weight_output_loss: 0.9788 - bag_output_loss: 0.9037 - footwear_output_loss: 0.9105 - pose_output_loss: 0.9106 - emotion_output_loss: 0.9144 - gender_output_acc: 0.6161 - image_quality_output_acc: 0.5538 - age_output_acc: 0.3969 - weight_output_acc: 0.6356 - bag_output_acc: 0.5670 - footwear_output_acc: 0.5789 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7096\n",
            "339/339 [==============================] - 207s 611ms/step - loss: 7.6436 - gender_output_loss: 0.6442 - image_quality_output_loss: 0.9757 - age_output_loss: 1.4050 - weight_output_loss: 0.9792 - bag_output_loss: 0.9040 - footwear_output_loss: 0.9107 - pose_output_loss: 0.9110 - emotion_output_loss: 0.9138 - gender_output_acc: 0.6161 - image_quality_output_acc: 0.5536 - age_output_acc: 0.3968 - weight_output_acc: 0.6353 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5785 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7099 - val_loss: 7.6090 - val_gender_output_loss: 0.6461 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.4079 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9026 - val_footwear_output_loss: 0.9066 - val_pose_output_loss: 0.9082 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.6075 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5770 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 7.60805\n",
            "Epoch 36/100\n",
            "339/339 [==============================] - 202s 597ms/step - loss: 7.6424 - gender_output_loss: 0.6458 - image_quality_output_loss: 0.9758 - age_output_loss: 1.4057 - weight_output_loss: 0.9800 - bag_output_loss: 0.9009 - footwear_output_loss: 0.9090 - pose_output_loss: 0.9108 - emotion_output_loss: 0.9144 - gender_output_acc: 0.6122 - image_quality_output_acc: 0.5516 - age_output_acc: 0.4015 - weight_output_acc: 0.6356 - bag_output_acc: 0.5694 - footwear_output_acc: 0.5808 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7099 - val_loss: 7.6082 - val_gender_output_loss: 0.6456 - val_image_quality_output_loss: 0.9737 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.9021 - val_footwear_output_loss: 0.9065 - val_pose_output_loss: 0.9081 - val_emotion_output_loss: 0.8865 - val_gender_output_acc: 0.6094 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.4014 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5569 - val_footwear_output_acc: 0.5781 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 7.60805\n",
            "Epoch 37/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6404 - gender_output_loss: 0.6427 - image_quality_output_loss: 0.9749 - age_output_loss: 1.4055 - weight_output_loss: 0.9793 - bag_output_loss: 0.9019 - footwear_output_loss: 0.9101 - pose_output_loss: 0.9117 - emotion_output_loss: 0.9142 - gender_output_acc: 0.6138 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3969 - weight_output_acc: 0.6357 - bag_output_acc: 0.5674 - footwear_output_acc: 0.5774 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7100\n",
            "Epoch 00036: val_loss did not improve from 7.60805\n",
            "Epoch 37/100\n",
            "339/339 [==============================] - 197s 582ms/step - loss: 7.6408 - gender_output_loss: 0.6426 - image_quality_output_loss: 0.9755 - age_output_loss: 1.4055 - weight_output_loss: 0.9798 - bag_output_loss: 0.9016 - footwear_output_loss: 0.9101 - pose_output_loss: 0.9119 - emotion_output_loss: 0.9137 - gender_output_acc: 0.6139 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3970 - weight_output_acc: 0.6354 - bag_output_acc: 0.5677 - footwear_output_acc: 0.5774 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7103 - val_loss: 7.6090 - val_gender_output_loss: 0.6461 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.4081 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9024 - val_footwear_output_loss: 0.9065 - val_pose_output_loss: 0.9080 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5778 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 7.60805\n",
            "Epoch 38/100\n",
            " 45/339 [==>...........................] - ETA: 2:42 - loss: 7.6759 - gender_output_loss: 0.6534 - image_quality_output_loss: 0.9660 - age_output_loss: 1.4182 - weight_output_loss: 0.9981 - bag_output_loss: 0.9083 - footwear_output_loss: 0.9130 - pose_output_loss: 0.9036 - emotion_output_loss: 0.9154 - gender_output_acc: 0.6021 - image_quality_output_acc: 0.5632 - age_output_acc: 0.3854 - weight_output_acc: 0.6292 - bag_output_acc: 0.5576 - footwear_output_acc: 0.5868 - pose_output_acc: 0.6215 - emotion_output_acc: 0.7104"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-450:\n",
            "Process ForkPoolWorker-452:\n",
            "Process ForkPoolWorker-448:\n",
            "Process ForkPoolWorker-447:\n",
            "Process ForkPoolWorker-446:\n",
            "Process ForkPoolWorker-445:\n",
            "Process ForkPoolWorker-449:\n",
            "Process ForkPoolWorker-455:\n",
            "Process ForkPoolWorker-453:\n",
            "Process ForkPoolWorker-456:\n",
            "Process ForkPoolWorker-454:\n",
            "Process ForkPoolWorker-451:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"<ipython-input-6-669647fad76f>\", line 52, in __getitem__\n",
            "    image = self.augmentation.flow(image, shuffle=False, batch_size=self.batch_size).next()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"<ipython-input-6-669647fad76f>\", line 52, in __getitem__\n",
            "    image = self.augmentation.flow(image, shuffle=False, batch_size=self.batch_size).next()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-580866953faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#class_weight= class_weights,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#callbacks=[lr_reducer, early_stopper, csv_logger, checkpoint, LearningRateScheduler(scheduler, verbose=1)],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24R2ennSvbi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('gdrive/My Drive/saved_models/model-030-7.6081.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkKnxP-xvqgJ",
        "colab_type": "code",
        "outputId": "715f6930-bb4f-472c-a3c0-f7fbccc5fc7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    initial_epoch=37,\n",
        "    #class_weight= class_weights,\n",
        "    #callbacks=[lr_reducer, early_stopper, csv_logger, checkpoint, LearningRateScheduler(scheduler, verbose=1)],\n",
        "    callbacks=[checkpoint,clr]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/100\n",
            "339/339 [==============================] - 213s 628ms/step - loss: 7.6420 - gender_output_loss: 0.6483 - image_quality_output_loss: 0.9752 - age_output_loss: 1.4056 - weight_output_loss: 0.9787 - bag_output_loss: 0.9030 - footwear_output_loss: 0.9080 - pose_output_loss: 0.9109 - emotion_output_loss: 0.9123 - gender_output_acc: 0.6074 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3968 - weight_output_acc: 0.6346 - bag_output_acc: 0.5713 - footwear_output_acc: 0.5810 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7100 - val_loss: 7.6091 - val_gender_output_loss: 0.6466 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4089 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9021 - val_footwear_output_loss: 0.9072 - val_pose_output_loss: 0.9079 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.6101 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5592 - val_footwear_output_acc: 0.5815 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 7.60805\n",
            "Epoch 39/100\n",
            "339/339 [==============================] - 213s 628ms/step - loss: 7.6420 - gender_output_loss: 0.6483 - image_quality_output_loss: 0.9752 - age_output_loss: 1.4056 - weight_output_loss: 0.9787 - bag_output_loss: 0.9030 - footwear_output_loss: 0.9080 - pose_output_loss: 0.9109 - emotion_output_loss: 0.9123 - gender_output_acc: 0.6074 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3968 - weight_output_acc: 0.6346 - bag_output_acc: 0.5713 - footwear_output_acc: 0.5810 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7100 - val_loss: 7.6091 - val_gender_output_loss: 0.6466 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4089 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9021 - val_footwear_output_loss: 0.9072 - val_pose_output_loss: 0.9079 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.6101 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5592 - val_footwear_output_acc: 0.5815 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================] - 202s 595ms/step - loss: 7.6399 - gender_output_loss: 0.6444 - image_quality_output_loss: 0.9742 - age_output_loss: 1.4066 - weight_output_loss: 0.9763 - bag_output_loss: 0.9038 - footwear_output_loss: 0.9093 - pose_output_loss: 0.9125 - emotion_output_loss: 0.9128 - gender_output_acc: 0.6150 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3975 - weight_output_acc: 0.6356 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5806 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7099 - val_loss: 7.6089 - val_gender_output_loss: 0.6463 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.4087 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9018 - val_footwear_output_loss: 0.9071 - val_pose_output_loss: 0.9080 - val_emotion_output_loss: 0.8871 - val_gender_output_acc: 0.6105 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5826 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 7.60805\n",
            "Epoch 40/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6447 - gender_output_loss: 0.6439 - image_quality_output_loss: 0.9765 - age_output_loss: 1.4050 - weight_output_loss: 0.9808 - bag_output_loss: 0.9029 - footwear_output_loss: 0.9123 - pose_output_loss: 0.9119 - emotion_output_loss: 0.9114 - gender_output_acc: 0.6153 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3964 - weight_output_acc: 0.6355 - bag_output_acc: 0.5655 - footwear_output_acc: 0.5797 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7098\n",
            "Epoch 00039: val_loss did not improve from 7.60805\n",
            "Epoch 40/100\n",
            "339/339 [==============================] - 198s 584ms/step - loss: 7.6448 - gender_output_loss: 0.6437 - image_quality_output_loss: 0.9764 - age_output_loss: 1.4047 - weight_output_loss: 0.9808 - bag_output_loss: 0.9027 - footwear_output_loss: 0.9126 - pose_output_loss: 0.9125 - emotion_output_loss: 0.9114 - gender_output_acc: 0.6156 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3963 - weight_output_acc: 0.6355 - bag_output_acc: 0.5655 - footwear_output_acc: 0.5794 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7098 - val_loss: 7.6086 - val_gender_output_loss: 0.6462 - val_image_quality_output_loss: 0.9722 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9019 - val_footwear_output_loss: 0.9068 - val_pose_output_loss: 0.9085 - val_emotion_output_loss: 0.8875 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3999 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5796 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 7.60805\n",
            "Epoch 41/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6430 - gender_output_loss: 0.6451 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4052 - weight_output_loss: 0.9772 - bag_output_loss: 0.9047 - footwear_output_loss: 0.9106 - pose_output_loss: 0.9127 - emotion_output_loss: 0.9113 - gender_output_acc: 0.6102 - image_quality_output_acc: 0.5538 - age_output_acc: 0.3978 - weight_output_acc: 0.6350 - bag_output_acc: 0.5683 - footwear_output_acc: 0.5814 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7104\n",
            "Epoch 00040: val_loss did not improve from 7.60805\n",
            "Epoch 41/100\n",
            "339/339 [==============================] - 197s 582ms/step - loss: 7.6437 - gender_output_loss: 0.6450 - image_quality_output_loss: 0.9765 - age_output_loss: 1.4053 - weight_output_loss: 0.9770 - bag_output_loss: 0.9047 - footwear_output_loss: 0.9104 - pose_output_loss: 0.9126 - emotion_output_loss: 0.9121 - gender_output_acc: 0.6103 - image_quality_output_acc: 0.5535 - age_output_acc: 0.3977 - weight_output_acc: 0.6351 - bag_output_acc: 0.5690 - footwear_output_acc: 0.5816 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7100 - val_loss: 7.6080 - val_gender_output_loss: 0.6464 - val_image_quality_output_loss: 0.9718 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.9017 - val_footwear_output_loss: 0.9069 - val_pose_output_loss: 0.9085 - val_emotion_output_loss: 0.8874 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5528 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5569 - val_footwear_output_acc: 0.5785 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00041: val_loss improved from 7.60805 to 7.60800, saving model to gdrive/My Drive/saved_models/model-041-7.6080.hdf5\n",
            "Epoch 42/100\n",
            "281/339 [=======================>......] - ETA: 31s - loss: 7.6403 - gender_output_loss: 0.6448 - image_quality_output_loss: 0.9752 - age_output_loss: 1.4028 - weight_output_loss: 0.9755 - bag_output_loss: 0.9107 - footwear_output_loss: 0.9106 - pose_output_loss: 0.9107 - emotion_output_loss: 0.9101 - gender_output_acc: 0.6079 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3999 - weight_output_acc: 0.6380 - bag_output_acc: 0.5665 - footwear_output_acc: 0.5846 - pose_output_acc: 0.6173 - emotion_output_acc: 0.7121"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}