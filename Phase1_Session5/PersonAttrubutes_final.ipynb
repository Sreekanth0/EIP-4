{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet50 - PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "79d38b08-246c-4a72-c00c-ad004fa168fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "import collections\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, Reshape, GlobalAveragePooling2D\n",
        "from keras import regularizers\n",
        "from keras.regularizers import l2\n",
        "from keras.applications.resnet_v2 import ResNet50V2\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint, LearningRateScheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "ea9ec951-81d0-4f5e-eb24-fc91c76416a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xWOu2-Hkw-8",
        "colab_type": "code",
        "outputId": "657dbfbb-982d-4cc6-dcee-5b65a18b05fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "class_weights = {}\n",
        "cl_names = [\"gender_output\", \"image_quality_output\", \"age_output\", \"weight_output\", \"bag_output\", \"footwear_output\", \"emotion_output\", \"pose_output\"]\n",
        "cols = list(df.columns)\n",
        "cols.pop()\n",
        "i = 0\n",
        "for col in cols:\n",
        "    a = df[col].value_counts(normalize=True)\n",
        "    od = collections.OrderedDict(sorted(dict(a).items()))\n",
        "    temp = {}\n",
        "    ix = 0\n",
        "    for d in od:\n",
        "        temp[ix] = od[d]\n",
        "        ix +=1\n",
        "    class_weights[cl_names[i]] = temp\n",
        "    i +=1\n",
        "\n",
        "print(class_weights)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'gender_output': {0: 0.43741251013040594, 1: 0.5625874898695941}, 'image_quality_output': {0: 0.5532306785530097, 1: 0.16503352243424446, 2: 0.28173579901274587}, 'age_output': {0: 0.18374714506741324, 1: 0.39865910263022175, 2: 0.25307595962572754, 3: 0.10977676269063583, 4: 0.054741029986001624}, 'weight_output': {0: 0.6356737640904737, 1: 0.06564503057540706, 2: 0.23546747218743094, 3: 0.06321373314668828}, 'bag_output': {0: 0.33912915346644074, 1: 0.09732557282840934, 2: 0.56354527370515}, 'footwear_output': {0: 0.37044131732115226, 1: 0.18470492890296913, 2: 0.4448537537758786}, 'emotion_output': {0: 0.11051351948721727, 1: 0.11854416856995506, 2: 0.7117070654976793, 3: 0.05923524644514846}, 'pose_output': {0: 0.16260222500552568, 1: 0.6176232225742282, 2: 0.21977455242024607}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "00ba477c-b7c7-40e4-d7c3-297818503f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None, im_size=224):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "        self.im_size = im_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        ims = [cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()]\n",
        "        image = np.stack([cv2.resize(im, (self.im_size, self.im_size)) for im in ims])\n",
        "        image = image.astype('float32') / 255\n",
        "        images_mean = np.mean(image, axis=0)\n",
        "        image -= images_mean\n",
        "\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        if self.augmentation is not None:\n",
        "          image = self.augmentation.flow(image, shuffle=False, batch_size=self.batch_size).next()\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "3b462fc0-4ce8-4007-ad5b-335ada6e9eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.2,random_state=31)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10858, 28), (2715, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augmentor(images):\n",
        "\t\t'Apply data augmentation'\n",
        "\t\tsometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "\t\tseq = iaa.Sequential(\n",
        "\t\t\t\t[\n",
        "\t\t\t\t# apply the following augmenters to most images\n",
        "\t\t\t\tiaa.Fliplr(0.3),  # horizontally flip 50% of all images\n",
        "\t\t\t\tsometimes(iaa.Affine(\n",
        "\t\t\t\t\tscale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
        "\t\t\t\t\t# scale images to 80-120% of their size, individually per axis\n",
        "\t\t\t\t\ttranslate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
        "\t\t\t\t\t# translate by -20 to +20 percent (per axis)\n",
        "\t\t\t\t\trotate=(-10, 10),  # rotate by -45 to +45 degrees\n",
        "\t\t\t\t\tshear=(-5, 5),  # shear by -16 to +16 degrees\n",
        "\t\t\t\t\torder=[0, 1],\n",
        "\t\t\t\t\t# use nearest neighbour or bilinear interpolation (fast)\n",
        "\t\t\t\t\tcval=(0, 255),  # if mode is constant, use a cval between 0 and 255\n",
        "\t\t\t\t\tmode=ia.ALL\n",
        "\t\t\t\t\t# use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
        "\t\t\t\t)),\n",
        "\t\t\t\t# execute 0 to 5 of the following (less important) augmenters per image\n",
        "\t\t\t\t# don't execute all of them, as that would often be way too strong\n",
        "\t\t\t\tiaa.SomeOf((0, 5),\n",
        "\t\t\t\t           [sometimes(iaa.Superpixels(p_replace=(0, 1.0),\n",
        "\t\t\t\t\t\t                                     n_segments=(20, 200))),\n",
        "\t\t\t\t\t           # convert images into their superpixel representation\n",
        "\t\t\t\t\t           iaa.OneOf([\n",
        "\t\t\t\t\t\t\t           iaa.GaussianBlur((0, 1.0)),\n",
        "\t\t\t\t\t\t\t           # blur images with a sigma between 0 and 3.0\n",
        "\t\t\t\t\t\t\t           iaa.AverageBlur(k=(3, 5)),\n",
        "\t\t\t\t\t\t\t           # blur image using local means with kernel sizes between 2 and 7\n",
        "\t\t\t\t\t\t\t           iaa.MedianBlur(k=(3, 5)),\n",
        "\t\t\t\t\t\t\t           # blur image using local medians with kernel sizes between 2 and 7\n",
        "\t\t\t\t\t           ]),\n",
        "\t\t\t\t\t           iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)),\n",
        "\t\t\t\t\t           # sharpen images\n",
        "\t\t\t\t\t           iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n",
        "\t\t\t\t\t           # emboss images\n",
        "\t\t\t\t\t           # search either for all edges or for directed edges,\n",
        "\t\t\t\t\t           # blend the result with the original image using a blobby mask\n",
        "\t\t\t\t\t           iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
        "\t\t\t\t\t\t\t           iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
        "\t\t\t\t\t\t\t           iaa.DirectedEdgeDetect(alpha=(0.5, 1.0),\n",
        "\t\t\t\t\t\t\t                                  direction=(0.0, 1.0)),\n",
        "\t\t\t\t\t           ])),\n",
        "\t\t\t\t\t           iaa.AdditiveGaussianNoise(loc=0,\n",
        "\t\t\t\t\t                                     scale=(0.0, 0.01 * 255),\n",
        "\t\t\t\t\t                                     per_channel=0.5),\n",
        "\t\t\t\t\t           # add gaussian noise to images\n",
        "\t\t\t\t\t           \n",
        "\t\t\t\t\t           \n",
        "\t\t\t\t\t           iaa.Add((-2, 2), per_channel=0.5),\n",
        "\t\t\t\t\t           # change brightness of images (by -10 to 10 of original value)\n",
        "\t\t\t\t\t           iaa.AddToHueAndSaturation((-1, 1)),\n",
        "\t\t\t\t\t           # change hue and saturation\n",
        "\t\t\t\t\t           # either change the brightness of the whole image (sometimes\n",
        "\t\t\t\t\t           # per channel) or change the brightness of subareas\n",
        "\t\t\t\t\t           iaa.OneOf([\n",
        "\t\t\t\t\t\t\t           iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
        "\t\t\t\t\t\t\t           iaa.FrequencyNoiseAlpha(\n",
        "\t\t\t\t\t\t\t\t\t           exponent=(-1, 0),\n",
        "\t\t\t\t\t\t\t\t\t           first=iaa.Multiply((0.9, 1.1),\n",
        "\t\t\t\t\t\t\t\t\t                              per_channel=True),\n",
        "\t\t\t\t\t\t\t\t\t           second=iaa.ContrastNormalization(\n",
        "\t\t\t\t\t\t\t\t\t\t\t           (0.9, 1.1))\n",
        "\t\t\t\t\t\t\t           )\n",
        "\t\t\t\t\t           ]),\n",
        "\t\t\t\t\t           sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5),\n",
        "\t\t\t\t\t                                               sigma=0.25)),\n",
        "\t\t\t\t\t           # move pixels locally around (with random strengths)\n",
        "\t\t\t\t\t           sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))),\n",
        "\t\t\t\t\t           # sometimes move parts of the image around\n",
        "\t\t\t\t\t           sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
        "\t\t\t\t           ],\n",
        "\t\t\t\t           random_order=True\n",
        "\t\t\t\t           )\n",
        "\t\t\t\t],\n",
        "\t\t\t\trandom_order=True\n",
        "\t\t)\n",
        "\t\treturn seq.augment_images(images)\n",
        "  \n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "\n",
        "  \"\"\"\n",
        "  p : the probability that random erasing is performed\n",
        "  s_l, s_h : minimum / maximum proportion of erased area against input image\n",
        "  r_1, r_2 : minimum / maximum aspect ratio of erased area\n",
        "  v_l, v_h : minimum / maximum value for erased area\n",
        "  pixel_level : pixel-level randomization for erased area\n",
        "  \"\"\"\n",
        "\n",
        "  def eraser(input_img):\n",
        "    img_h, img_w, img_c = input_img.shape\n",
        "    p_1 = np.random.rand()\n",
        "\n",
        "    if p_1 > p:\n",
        "        return input_img\n",
        "\n",
        "    while True:\n",
        "        s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "        r = np.random.uniform(r_1, r_2)\n",
        "        w = int(np.sqrt(s / r))\n",
        "        h = int(np.sqrt(s * r))\n",
        "        left = np.random.randint(0, img_w)\n",
        "        top = np.random.randint(0, img_h)\n",
        "\n",
        "        if left + w <= img_w and top + h <= img_h:\n",
        "            break\n",
        "\n",
        "    if pixel_level:\n",
        "        c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "    else:\n",
        "        c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "    input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "    return input_img\n",
        "\n",
        "  return eraser\n",
        "\n",
        "\n",
        "aug = ImageDataGenerator(\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=2,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.08,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.05,\n",
        "        # set range for random shear\n",
        "        shear_range=0.07,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.05,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.07,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.1,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        preprocessing_function=get_random_eraser(p=0.08,v_l=0, v_h=0.3, s_l=0.008, s_h=0.05, pixel_level=False)\n",
        "\t\t\t\t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32,augmentation=aug)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=32, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "2e4c10c3-5cdc-4f92-806d-5c83bbbbab68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgK-BmUD5d_c",
        "colab_type": "code",
        "outputId": "6c54605b-fa64-4ca5-ef08-d74419017e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "model = ResNet50V2(weights= None, include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "neck = model.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128,activation='relu')(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "model = Model(inputs=[model.input], outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQIrjiSHJdbT",
        "colab_type": "code",
        "outputId": "c7ceea8b-1e89-4a63-aeaf-c884cd5ab11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_bn (BatchNo (None, 56, 56, 64)   256         pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_relu (Activ (None, 56, 56, 64)   0           conv2_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Add)          (None, 56, 56, 256)  0           conv2_block1_0_conv[0][0]        \n",
            "                                                                 conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36864       conv2_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_bn (BatchNo (None, 56, 56, 256)  1024        conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_relu (Activ (None, 56, 56, 256)  0           conv2_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_pad (ZeroPadding (None, 58, 58, 64)   0           conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 28, 28, 64)   36864       conv2_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 28, 28, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 28, 28, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 256)  0           conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 28, 28, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Add)          (None, 28, 28, 256)  0           max_pooling2d_1[0][0]            \n",
            "                                                                 conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_bn (BatchNo (None, 28, 28, 256)  1024        conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_relu (Activ (None, 28, 28, 256)  0           conv3_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Add)          (None, 28, 28, 512)  0           conv3_block1_0_conv[0][0]        \n",
            "                                                                 conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147456      conv3_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_bn (BatchNo (None, 28, 28, 512)  2048        conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_relu (Activ (None, 28, 28, 512)  0           conv3_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_pad (ZeroPadding (None, 30, 30, 128)  0           conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 14, 14, 128)  147456      conv3_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 14, 14, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 14, 14, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 512)  0           conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 14, 14, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Add)          (None, 14, 14, 512)  0           max_pooling2d_2[0][0]            \n",
            "                                                                 conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_bn (BatchNo (None, 14, 14, 512)  2048        conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_relu (Activ (None, 14, 14, 512)  0           conv4_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131072      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_conv[0][0]        \n",
            "                                                                 conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block5_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block5_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  589824      conv4_block5_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_bn (BatchNo (None, 14, 14, 1024) 4096        conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_relu (Activ (None, 14, 14, 1024) 0           conv4_block6_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262144      conv4_block6_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_pad (ZeroPadding (None, 16, 16, 256)  0           conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 7, 7, 256)    589824      conv4_block6_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 7, 7, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 7, 7, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 1024)   0           conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 7, 7, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Add)          (None, 7, 7, 1024)   0           max_pooling2d_3[0][0]            \n",
            "                                                                 conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_bn (BatchNo (None, 7, 7, 1024)   4096        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_relu (Activ (None, 7, 7, 1024)   0           conv5_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524288      conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_conv[0][0]        \n",
            "                                                                 conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_bn (BatchNo (None, 7, 7, 2048)   8192        conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_relu (Activ (None, 7, 7, 2048)   0           conv5_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1048576     conv5_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_pad (ZeroPadding (None, 9, 9, 512)    0           conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359296     conv5_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "post_bn (BatchNormalization)    (None, 7, 7, 2048)   8192        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "post_relu (Activation)          (None, 7, 7, 2048)   0           post_bn[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 100352)       0           post_relu[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          51380736    flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          65664       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          65664       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          65664       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 128)          65664       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          65664       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          65664       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          65664       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          65664       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            258         dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            387         dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            645         dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            516         dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            387         dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            387         dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            387         dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            516         dense_8[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 75,474,331\n",
            "Trainable params: 75,428,891\n",
            "Non-trainable params: 45,440\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrzs6dYLS4S6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CyclicLR(keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self,base_lr, max_lr, step_size, base_m, max_m, cyclical_momentum):\n",
        " \n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.base_m = base_m\n",
        "        self.max_m = max_m\n",
        "        self.cyclical_momentum = cyclical_momentum\n",
        "        self.step_size = step_size\n",
        "        \n",
        "        self.clr_iterations = 0.\n",
        "        self.cm_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "        \n",
        "    def clr(self):\n",
        "        \n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        \n",
        "        if cycle == 2:\n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)          \n",
        "            return self.base_lr-(self.base_lr-self.base_lr/100)*np.maximum(0,(1-x))\n",
        "        \n",
        "        else:\n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0,(1-x))\n",
        "    \n",
        "    def cm(self):\n",
        "        \n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        \n",
        "        if cycle == 2:\n",
        "            \n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1) \n",
        "            return self.max_m\n",
        "        \n",
        "        else:\n",
        "            x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "            return self.max_m - (self.max_m-self.base_m)*np.maximum(0,(1-x))\n",
        "        \n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "            \n",
        "        if self.cyclical_momentum == True:\n",
        "            if self.clr_iterations == 0:\n",
        "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
        "            else:\n",
        "                K.set_value(self.model.optimizer.momentum, self.cm())\n",
        "            \n",
        "            \n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "        \n",
        "        if self.cyclical_momentum == True:\n",
        "            self.history.setdefault('momentum', []).append(K.get_value(self.model.optimizer.momentum))\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "        \n",
        "        if self.cyclical_momentum == True:\n",
        "            K.set_value(self.model.optimizer.momentum, self.cm())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNLL-xNnD3ng",
        "colab_type": "code",
        "outputId": "b771eaa1-885e-4a37-dd53-3bc6346ac1ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "losses = {\n",
        "\t\"gender_output\": \"binary_crossentropy\",\n",
        "\t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\t\"age_output\": \"categorical_crossentropy\",\n",
        "\t\"weight_output\": \"categorical_crossentropy\",\n",
        "  \"bag_output\": \"categorical_crossentropy\",\n",
        "  \"footwear_output\": \"categorical_crossentropy\",\n",
        "  \"pose_output\": \"categorical_crossentropy\",\n",
        "  \"emotion_output\": \"categorical_crossentropy\"\n",
        "}\n",
        "\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0, \"weight_output\": 1.0, \"bag_output\": 1.0, \"footwear_output\": 1.0, \"pose_output\": 1.0, \"emotion_output\": 1.0}\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    #loss=losses, \n",
        "    #loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AR_V4h4kpZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 100\n",
        "max_lr = 0.0001\n",
        "base_lr = max_lr/100\n",
        "max_m = 0.98\n",
        "base_m = 0.85\n",
        "\n",
        "augment = True\n",
        "cycles = 8\n",
        "\n",
        "iterations = round(len(train_df)/32*epochs)\n",
        "iterations = list(range(0,iterations+1))\n",
        "step_size = len(iterations)/(cycles)\n",
        "\n",
        "\n",
        "clr =  CyclicLR(base_lr=base_lr,\n",
        "                max_lr=max_lr,\n",
        "                step_size=step_size,\n",
        "                max_m=max_m,\n",
        "                base_m=base_m,\n",
        "                cyclical_momentum=True)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists('gdrive/My Drive/saved_models'):\n",
        "    os.mkdir('gdrive/My Drive/saved_models')\n",
        "LOGS_DIR = 'gdrive/My Drive/saved_models'\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
        "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
        "csv_logger = CSVLogger(LOGS_DIR +'/'+'log.csv')\n",
        "filepath=os.path.join(LOGS_DIR+os.sep+'model-{epoch:03d}-{val_loss:.4f}.hdf5')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVGh9H-ExDOu",
        "colab_type": "code",
        "outputId": "fd4f656d-7c45-479b-d744-61c2a92d4825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    #class_weight= class_weights,\n",
        "    #callbacks=[lr_reducer, early_stopper, csv_logger, checkpoint, LearningRateScheduler(scheduler, verbose=1)],\n",
        "    callbacks=[checkpoint,clr]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "339/339 [==============================] - 207s 612ms/step - loss: 8.5421 - gender_output_loss: 0.7273 - image_quality_output_loss: 1.0463 - age_output_loss: 1.5673 - weight_output_loss: 1.0768 - bag_output_loss: 1.0121 - footwear_output_loss: 1.0857 - pose_output_loss: 0.9867 - emotion_output_loss: 1.0399 - gender_output_acc: 0.5260 - image_quality_output_acc: 0.5084 - age_output_acc: 0.3273 - weight_output_acc: 0.5982 - bag_output_acc: 0.5086 - footwear_output_acc: 0.4575 - pose_output_acc: 0.5871 - emotion_output_acc: 0.6650 - val_loss: 8.3247 - val_gender_output_loss: 0.7489 - val_image_quality_output_loss: 1.0361 - val_age_output_loss: 1.4986 - val_weight_output_loss: 1.0491 - val_bag_output_loss: 0.9682 - val_footwear_output_loss: 1.0765 - val_pose_output_loss: 0.9762 - val_emotion_output_loss: 0.9711 - val_gender_output_acc: 0.5454 - val_image_quality_output_acc: 0.5193 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6272 - val_bag_output_acc: 0.5379 - val_footwear_output_acc: 0.4877 - val_pose_output_acc: 0.5956 - val_emotion_output_acc: 0.7169\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 8.32474, saving model to gdrive/My Drive/saved_models/model-001-8.3247.hdf5\n",
            "Epoch 2/100\n",
            "339/339 [==============================] - 193s 570ms/step - loss: 8.0379 - gender_output_loss: 0.6990 - image_quality_output_loss: 1.0094 - age_output_loss: 1.4699 - weight_output_loss: 1.0123 - bag_output_loss: 0.9414 - footwear_output_loss: 1.0174 - pose_output_loss: 0.9462 - emotion_output_loss: 0.9423 - gender_output_acc: 0.5375 - image_quality_output_acc: 0.5366 - age_output_acc: 0.3793 - weight_output_acc: 0.6300 - bag_output_acc: 0.5394 - footwear_output_acc: 0.4980 - pose_output_acc: 0.6134 - emotion_output_acc: 0.7081 - val_loss: 7.9050 - val_gender_output_loss: 0.6907 - val_image_quality_output_loss: 0.9892 - val_age_output_loss: 1.4558 - val_weight_output_loss: 0.9969 - val_bag_output_loss: 0.9288 - val_footwear_output_loss: 1.0046 - val_pose_output_loss: 0.9393 - val_emotion_output_loss: 0.8997 - val_gender_output_acc: 0.5435 - val_image_quality_output_acc: 0.5376 - val_age_output_acc: 0.3910 - val_weight_output_acc: 0.6354 - val_bag_output_acc: 0.5339 - val_footwear_output_acc: 0.5093 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00002: val_loss improved from 8.32474 to 7.90498, saving model to gdrive/My Drive/saved_models/model-002-7.9050.hdf5\n",
            "Epoch 3/100\n",
            "339/339 [==============================] - 194s 572ms/step - loss: 7.9815 - gender_output_loss: 0.6938 - image_quality_output_loss: 1.0013 - age_output_loss: 1.4560 - weight_output_loss: 1.0060 - bag_output_loss: 0.9364 - footwear_output_loss: 1.0059 - pose_output_loss: 0.9459 - emotion_output_loss: 0.9362 - gender_output_acc: 0.5496 - image_quality_output_acc: 0.5435 - age_output_acc: 0.3764 - weight_output_acc: 0.6329 - bag_output_acc: 0.5489 - footwear_output_acc: 0.5041 - pose_output_acc: 0.6164 - emotion_output_acc: 0.7094 - val_loss: 7.8889 - val_gender_output_loss: 0.6938 - val_image_quality_output_loss: 0.9895 - val_age_output_loss: 1.4575 - val_weight_output_loss: 0.9909 - val_bag_output_loss: 0.9280 - val_footwear_output_loss: 0.9967 - val_pose_output_loss: 0.9308 - val_emotion_output_loss: 0.9016 - val_gender_output_acc: 0.5487 - val_image_quality_output_acc: 0.5517 - val_age_output_acc: 0.3806 - val_weight_output_acc: 0.6347 - val_bag_output_acc: 0.5487 - val_footwear_output_acc: 0.5141 - val_pose_output_acc: 0.6157 - val_emotion_output_acc: 0.7176\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.90498 to 7.88894, saving model to gdrive/My Drive/saved_models/model-003-7.8889.hdf5\n",
            "Epoch 4/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.9383 - gender_output_loss: 0.6913 - image_quality_output_loss: 0.9950 - age_output_loss: 1.4555 - weight_output_loss: 0.9966 - bag_output_loss: 0.9326 - footwear_output_loss: 0.9973 - pose_output_loss: 0.9384 - emotion_output_loss: 0.9317 - gender_output_acc: 0.5515 - image_quality_output_acc: 0.5472 - age_output_acc: 0.3802 - weight_output_acc: 0.6324 - bag_output_acc: 0.5531 - footwear_output_acc: 0.5072 - pose_output_acc: 0.6175 - emotion_output_acc: 0.7097\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "339/339 [==============================] - 193s 571ms/step - loss: 7.9377 - gender_output_loss: 0.6912 - image_quality_output_loss: 0.9946 - age_output_loss: 1.4554 - weight_output_loss: 0.9963 - bag_output_loss: 0.9321 - footwear_output_loss: 0.9976 - pose_output_loss: 0.9387 - emotion_output_loss: 0.9316 - gender_output_acc: 0.5516 - image_quality_output_acc: 0.5477 - age_output_acc: 0.3801 - weight_output_acc: 0.6327 - bag_output_acc: 0.5532 - footwear_output_acc: 0.5072 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7096 - val_loss: 7.8753 - val_gender_output_loss: 0.6850 - val_image_quality_output_loss: 0.9904 - val_age_output_loss: 1.4456 - val_weight_output_loss: 0.9926 - val_bag_output_loss: 0.9288 - val_footwear_output_loss: 0.9958 - val_pose_output_loss: 0.9414 - val_emotion_output_loss: 0.8958 - val_gender_output_acc: 0.5569 - val_image_quality_output_acc: 0.5525 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6332 - val_bag_output_acc: 0.5357 - val_footwear_output_acc: 0.5126 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7176\n",
            "\n",
            "Epoch 00004: val_loss improved from 7.88894 to 7.87528, saving model to gdrive/My Drive/saved_models/model-004-7.8753.hdf5\n",
            "Epoch 5/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.9182 - gender_output_loss: 0.6871 - image_quality_output_loss: 0.9954 - age_output_loss: 1.4452 - weight_output_loss: 0.9964 - bag_output_loss: 0.9272 - footwear_output_loss: 0.9953 - pose_output_loss: 0.9374 - emotion_output_loss: 0.9341 - gender_output_acc: 0.5630 - image_quality_output_acc: 0.5473 - age_output_acc: 0.3865 - weight_output_acc: 0.6339 - bag_output_acc: 0.5582 - footwear_output_acc: 0.5197 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7100\n",
            "339/339 [==============================] - 193s 570ms/step - loss: 7.9179 - gender_output_loss: 0.6872 - image_quality_output_loss: 0.9952 - age_output_loss: 1.4448 - weight_output_loss: 0.9959 - bag_output_loss: 0.9277 - footwear_output_loss: 0.9955 - pose_output_loss: 0.9373 - emotion_output_loss: 0.9343 - gender_output_acc: 0.5629 - image_quality_output_acc: 0.5478 - age_output_acc: 0.3866 - weight_output_acc: 0.6342 - bag_output_acc: 0.5578 - footwear_output_acc: 0.5198 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7099 - val_loss: 7.8376 - val_gender_output_loss: 0.6866 - val_image_quality_output_loss: 0.9824 - val_age_output_loss: 1.4396 - val_weight_output_loss: 0.9955 - val_bag_output_loss: 0.9245 - val_footwear_output_loss: 0.9771 - val_pose_output_loss: 0.9288 - val_emotion_output_loss: 0.9031 - val_gender_output_acc: 0.5465 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.3966 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5528 - val_footwear_output_acc: 0.5253 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00005: val_loss improved from 7.87528 to 7.83761, saving model to gdrive/My Drive/saved_models/model-005-7.8376.hdf5\n",
            "Epoch 6/100\n",
            "339/339 [==============================] - 192s 567ms/step - loss: 7.8998 - gender_output_loss: 0.6836 - image_quality_output_loss: 0.9941 - age_output_loss: 1.4426 - weight_output_loss: 0.9985 - bag_output_loss: 0.9261 - footwear_output_loss: 0.9856 - pose_output_loss: 0.9380 - emotion_output_loss: 0.9313 - gender_output_acc: 0.5670 - image_quality_output_acc: 0.5467 - age_output_acc: 0.3849 - weight_output_acc: 0.6336 - bag_output_acc: 0.5600 - footwear_output_acc: 0.5235 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7094 - val_loss: 7.8426 - val_gender_output_loss: 0.6841 - val_image_quality_output_loss: 0.9887 - val_age_output_loss: 1.4363 - val_weight_output_loss: 0.9901 - val_bag_output_loss: 0.9241 - val_footwear_output_loss: 0.9853 - val_pose_output_loss: 0.9370 - val_emotion_output_loss: 0.8970 - val_gender_output_acc: 0.5525 - val_image_quality_output_acc: 0.5528 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5506 - val_footwear_output_acc: 0.5242 - val_pose_output_acc: 0.6131 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 7.83761\n",
            "Epoch 7/100\n",
            "339/339 [==============================] - 192s 567ms/step - loss: 7.8647 - gender_output_loss: 0.6834 - image_quality_output_loss: 0.9896 - age_output_loss: 1.4376 - weight_output_loss: 0.9939 - bag_output_loss: 0.9255 - footwear_output_loss: 0.9738 - pose_output_loss: 0.9318 - emotion_output_loss: 0.9289 - gender_output_acc: 0.5639 - image_quality_output_acc: 0.5497 - age_output_acc: 0.3897 - weight_output_acc: 0.6357 - bag_output_acc: 0.5579 - footwear_output_acc: 0.5405 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7098 - val_loss: 7.8072 - val_gender_output_loss: 0.6794 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.4431 - val_weight_output_loss: 0.9865 - val_bag_output_loss: 0.9235 - val_footwear_output_loss: 0.9737 - val_pose_output_loss: 0.9303 - val_emotion_output_loss: 0.8930 - val_gender_output_acc: 0.5688 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3955 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5472 - val_footwear_output_acc: 0.5257 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================]\n",
            "Epoch 00007: val_loss improved from 7.83761 to 7.80723, saving model to gdrive/My Drive/saved_models/model-007-7.8072.hdf5\n",
            "Epoch 8/100\n",
            "339/339 [==============================] - 193s 568ms/step - loss: 7.8556 - gender_output_loss: 0.6788 - image_quality_output_loss: 0.9868 - age_output_loss: 1.4337 - weight_output_loss: 0.9950 - bag_output_loss: 0.9261 - footwear_output_loss: 0.9757 - pose_output_loss: 0.9335 - emotion_output_loss: 0.9261 - gender_output_acc: 0.5725 - image_quality_output_acc: 0.5496 - age_output_acc: 0.3909 - weight_output_acc: 0.6351 - bag_output_acc: 0.5576 - footwear_output_acc: 0.5381 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7100 - val_loss: 7.8171 - val_gender_output_loss: 0.6826 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.4395 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.9235 - val_footwear_output_loss: 0.9742 - val_pose_output_loss: 0.9269 - val_emotion_output_loss: 0.9045 - val_gender_output_acc: 0.5662 - val_image_quality_output_acc: 0.5584 - val_age_output_acc: 0.3925 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5521 - val_footwear_output_acc: 0.5379 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 7.80723\n",
            "Epoch 9/100\n",
            "339/339 [==============================] - 194s 571ms/step - loss: 7.8433 - gender_output_loss: 0.6784 - image_quality_output_loss: 0.9881 - age_output_loss: 1.4343 - weight_output_loss: 0.9961 - bag_output_loss: 0.9231 - footwear_output_loss: 0.9651 - pose_output_loss: 0.9317 - emotion_output_loss: 0.9264 - gender_output_acc: 0.5720 - image_quality_output_acc: 0.5510 - age_output_acc: 0.3899 - weight_output_acc: 0.6347 - bag_output_acc: 0.5614 - footwear_output_acc: 0.5453 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7098 - val_loss: 7.7619 - val_gender_output_loss: 0.6747 - val_image_quality_output_loss: 0.9816 - val_age_output_loss: 1.4332 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.9178 - val_footwear_output_loss: 0.9607 - val_pose_output_loss: 0.9215 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.5818 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5428 - val_footwear_output_acc: 0.5391 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: val_loss improved from 7.80723 to 7.76188, saving model to gdrive/My Drive/saved_models/model-009-7.7619.hdf5\n",
            "Epoch 10/100\n",
            "339/339 [==============================] - 193s 570ms/step - loss: 7.8192 - gender_output_loss: 0.6729 - image_quality_output_loss: 0.9831 - age_output_loss: 1.4328 - weight_output_loss: 0.9953 - bag_output_loss: 0.9230 - footwear_output_loss: 0.9562 - pose_output_loss: 0.9297 - emotion_output_loss: 0.9262 - gender_output_acc: 0.5825 - image_quality_output_acc: 0.5504 - age_output_acc: 0.3893 - weight_output_acc: 0.6351 - bag_output_acc: 0.5561 - footwear_output_acc: 0.5501 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7100 - val_loss: 7.7536 - val_gender_output_loss: 0.6719 - val_image_quality_output_loss: 0.9756 - val_age_output_loss: 1.4313 - val_weight_output_loss: 0.9843 - val_bag_output_loss: 0.9182 - val_footwear_output_loss: 0.9584 - val_pose_output_loss: 0.9232 - val_emotion_output_loss: 0.8907 - val_gender_output_acc: 0.5897 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3925 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5528 - val_footwear_output_acc: 0.5357 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00010: val_loss improved from 7.76188 to 7.75356, saving model to gdrive/My Drive/saved_models/model-010-7.7536.hdf5\n",
            "Epoch 11/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.8035 - gender_output_loss: 0.6739 - image_quality_output_loss: 0.9871 - age_output_loss: 1.4276 - weight_output_loss: 0.9916 - bag_output_loss: 0.9178 - footwear_output_loss: 0.9555 - pose_output_loss: 0.9313 - emotion_output_loss: 0.9188 - gender_output_acc: 0.5805 - image_quality_output_acc: 0.5504 - age_output_acc: 0.3906 - weight_output_acc: 0.6348 - bag_output_acc: 0.5597 - footwear_output_acc: 0.5509 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7102 - val_loss: 7.7352 - val_gender_output_loss: 0.6708 - val_image_quality_output_loss: 0.9741 - val_age_output_loss: 1.4282 - val_weight_output_loss: 0.9835 - val_bag_output_loss: 0.9169 - val_footwear_output_loss: 0.9430 - val_pose_output_loss: 0.9201 - val_emotion_output_loss: 0.8987 - val_gender_output_acc: 0.5811 - val_image_quality_output_acc: 0.5584 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5413 - val_footwear_output_acc: 0.5610 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00011: val_loss improved from 7.75356 to 7.73523, saving model to gdrive/My Drive/saved_models/model-011-7.7352.hdf5\n",
            "Epoch 12/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.7961 - gender_output_loss: 0.6697 - image_quality_output_loss: 0.9880 - age_output_loss: 1.4297 - weight_output_loss: 0.9915 - bag_output_loss: 0.9179 - footwear_output_loss: 0.9494 - pose_output_loss: 0.9276 - emotion_output_loss: 0.9224 - gender_output_acc: 0.5862 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3885 - weight_output_acc: 0.6342 - bag_output_acc: 0.5638 - footwear_output_acc: 0.5522 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7098 - val_loss: 7.7214 - val_gender_output_loss: 0.6682 - val_image_quality_output_loss: 0.9763 - val_age_output_loss: 1.4262 - val_weight_output_loss: 0.9829 - val_bag_output_loss: 0.9140 - val_footwear_output_loss: 0.9415 - val_pose_output_loss: 0.9200 - val_emotion_output_loss: 0.8923 - val_gender_output_acc: 0.5844 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5521 - val_footwear_output_acc: 0.5558 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00012: val_loss improved from 7.73523 to 7.72137, saving model to gdrive/My Drive/saved_models/model-012-7.7214.hdf5\n",
            "Epoch 13/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.7774 - gender_output_loss: 0.6698 - image_quality_output_loss: 0.9835 - age_output_loss: 1.4240 - weight_output_loss: 0.9846 - bag_output_loss: 0.9177 - footwear_output_loss: 0.9448 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9253 - gender_output_acc: 0.5840 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3934 - weight_output_acc: 0.6348 - bag_output_acc: 0.5626 - footwear_output_acc: 0.5582 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7096\n",
            "Epoch 00012: val_loss improved from 7.73523 to 7.72137, saving model to gdrive/My Drive/saved_models/model-012-7.7214.hdf5\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.7771 - gender_output_loss: 0.6697 - image_quality_output_loss: 0.9837 - age_output_loss: 1.4239 - weight_output_loss: 0.9852 - bag_output_loss: 0.9173 - footwear_output_loss: 0.9449 - pose_output_loss: 0.9276 - emotion_output_loss: 0.9249 - gender_output_acc: 0.5843 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3935 - weight_output_acc: 0.6347 - bag_output_acc: 0.5628 - footwear_output_acc: 0.5582 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7098 - val_loss: 7.7413 - val_gender_output_loss: 0.6687 - val_image_quality_output_loss: 0.9737 - val_age_output_loss: 1.4347 - val_weight_output_loss: 0.9801 - val_bag_output_loss: 0.9162 - val_footwear_output_loss: 0.9488 - val_pose_output_loss: 0.9254 - val_emotion_output_loss: 0.8937 - val_gender_output_acc: 0.6023 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5446 - val_footwear_output_acc: 0.5536 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 7.72137\n",
            "Epoch 14/100\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.7647 - gender_output_loss: 0.6701 - image_quality_output_loss: 0.9816 - age_output_loss: 1.4207 - weight_output_loss: 0.9891 - bag_output_loss: 0.9151 - footwear_output_loss: 0.9403 - pose_output_loss: 0.9242 - emotion_output_loss: 0.9237 - gender_output_acc: 0.5768 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3944 - weight_output_acc: 0.6357 - bag_output_acc: 0.5645 - footwear_output_acc: 0.5659 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7099 - val_loss: 7.7106 - val_gender_output_loss: 0.6671 - val_image_quality_output_loss: 0.9728 - val_age_output_loss: 1.4216 - val_weight_output_loss: 0.9788 - val_bag_output_loss: 0.9156 - val_footwear_output_loss: 0.9354 - val_pose_output_loss: 0.9234 - val_emotion_output_loss: 0.8959 - val_gender_output_acc: 0.5826 - val_image_quality_output_acc: 0.5517 - val_age_output_acc: 0.3966 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5417 - val_footwear_output_acc: 0.5707 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 00014: val_loss improved from 7.72137 to 7.71063, saving model to gdrive/My Drive/saved_models/model-014-7.7106.hdf5\n",
            "Epoch 15/100\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.7633 - gender_output_loss: 0.6685 - image_quality_output_loss: 0.9842 - age_output_loss: 1.4219 - weight_output_loss: 0.9861 - bag_output_loss: 0.9130 - footwear_output_loss: 0.9394 - pose_output_loss: 0.9262 - emotion_output_loss: 0.9238 - gender_output_acc: 0.5841 - image_quality_output_acc: 0.5510 - age_output_acc: 0.3907 - weight_output_acc: 0.6359 - bag_output_acc: 0.5605 - footwear_output_acc: 0.5658 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7100 - val_loss: 7.6909 - val_gender_output_loss: 0.6595 - val_image_quality_output_loss: 0.9729 - val_age_output_loss: 1.4200 - val_weight_output_loss: 0.9817 - val_bag_output_loss: 0.9129 - val_footwear_output_loss: 0.9310 - val_pose_output_loss: 0.9207 - val_emotion_output_loss: 0.8920 - val_gender_output_acc: 0.5938 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5547 - val_footwear_output_acc: 0.5618 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00015: val_loss improved from 7.71063 to 7.69089, saving model to gdrive/My Drive/saved_models/model-015-7.6909.hdf5\n",
            "Epoch 16/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.7491 - gender_output_loss: 0.6643 - image_quality_output_loss: 0.9846 - age_output_loss: 1.4179 - weight_output_loss: 0.9874 - bag_output_loss: 0.9155 - footwear_output_loss: 0.9352 - pose_output_loss: 0.9233 - emotion_output_loss: 0.9207 - gender_output_acc: 0.5878 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3929 - weight_output_acc: 0.6351 - bag_output_acc: 0.5620 - footwear_output_acc: 0.5633 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7103\n",
            "339/339 [==============================] - 195s 577ms/step - loss: 7.7500 - gender_output_loss: 0.6643 - image_quality_output_loss: 0.9846 - age_output_loss: 1.4185 - weight_output_loss: 0.9874 - bag_output_loss: 0.9154 - footwear_output_loss: 0.9351 - pose_output_loss: 0.9230 - emotion_output_loss: 0.9218 - gender_output_acc: 0.5880 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3927 - weight_output_acc: 0.6350 - bag_output_acc: 0.5620 - footwear_output_acc: 0.5635 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7099 - val_loss: 7.6810 - val_gender_output_loss: 0.6578 - val_image_quality_output_loss: 0.9783 - val_age_output_loss: 1.4190 - val_weight_output_loss: 0.9797 - val_bag_output_loss: 0.9129 - val_footwear_output_loss: 0.9281 - val_pose_output_loss: 0.9171 - val_emotion_output_loss: 0.8879 - val_gender_output_acc: 0.5956 - val_image_quality_output_acc: 0.5532 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5543 - val_footwear_output_acc: 0.5789 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00016: val_loss improved from 7.69089 to 7.68098, saving model to gdrive/My Drive/saved_models/model-016-7.6810.hdf5\n",
            "Epoch 17/100\n",
            "339/339 [==============================] - 194s 573ms/step - loss: 7.7409 - gender_output_loss: 0.6613 - image_quality_output_loss: 0.9848 - age_output_loss: 1.4140 - weight_output_loss: 0.9855 - bag_output_loss: 0.9168 - footwear_output_loss: 0.9320 - pose_output_loss: 0.9240 - emotion_output_loss: 0.9225 - gender_output_acc: 0.5954 - image_quality_output_acc: 0.5507 - age_output_acc: 0.3959 - weight_output_acc: 0.6355 - bag_output_acc: 0.5608 - footwear_output_acc: 0.5672 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7097 - val_loss: 7.6734 - val_gender_output_loss: 0.6572 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.4232 - val_weight_output_loss: 0.9791 - val_bag_output_loss: 0.9126 - val_footwear_output_loss: 0.9200 - val_pose_output_loss: 0.9204 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.5949 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5539 - val_footwear_output_acc: 0.5748 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00017: val_loss improved from 7.68098 to 7.67345, saving model to gdrive/My Drive/saved_models/model-017-7.6734.hdf5\n",
            "Epoch 18/100\n",
            "339/339 [==============================] - 197s 580ms/step - loss: 7.7192 - gender_output_loss: 0.6597 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4157 - weight_output_loss: 0.9860 - bag_output_loss: 0.9097 - footwear_output_loss: 0.9282 - pose_output_loss: 0.9201 - emotion_output_loss: 0.9192 - gender_output_acc: 0.5942 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3973 - weight_output_acc: 0.6360 - bag_output_acc: 0.5606 - footwear_output_acc: 0.5687 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7097 - val_loss: 7.6640 - val_gender_output_loss: 0.6564 - val_image_quality_output_loss: 0.9744 - val_age_output_loss: 1.4154 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.9124 - val_footwear_output_loss: 0.9265 - val_pose_output_loss: 0.9160 - val_emotion_output_loss: 0.8856 - val_gender_output_acc: 0.6019 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3966 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5528 - val_footwear_output_acc: 0.5666 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00018: val_loss improved from 7.67345 to 7.66404, saving model to gdrive/My Drive/saved_models/model-018-7.6640.hdf5\n",
            "Epoch 19/100\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.7152 - gender_output_loss: 0.6555 - image_quality_output_loss: 0.9793 - age_output_loss: 1.4172 - weight_output_loss: 0.9846 - bag_output_loss: 0.9106 - footwear_output_loss: 0.9308 - pose_output_loss: 0.9206 - emotion_output_loss: 0.9165 - gender_output_acc: 0.6000 - image_quality_output_acc: 0.5514 - age_output_acc: 0.3950 - weight_output_acc: 0.6344 - bag_output_acc: 0.5623 - footwear_output_acc: 0.5702 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7098 - val_loss: 7.6584 - val_gender_output_loss: 0.6562 - val_image_quality_output_loss: 0.9775 - val_age_output_loss: 1.4170 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.9100 - val_footwear_output_loss: 0.9181 - val_pose_output_loss: 0.9116 - val_emotion_output_loss: 0.8888 - val_gender_output_acc: 0.5878 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5484 - val_footwear_output_acc: 0.5815 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00019: val_loss improved from 7.66404 to 7.65842, saving model to gdrive/My Drive/saved_models/model-019-7.6584.hdf5\n",
            "Epoch 20/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.7062 - gender_output_loss: 0.6565 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4122 - weight_output_loss: 0.9872 - bag_output_loss: 0.9088 - footwear_output_loss: 0.9236 - pose_output_loss: 0.9187 - emotion_output_loss: 0.9178 - gender_output_acc: 0.5983 - image_quality_output_acc: 0.5515 - age_output_acc: 0.3945 - weight_output_acc: 0.6356 - bag_output_acc: 0.5649 - footwear_output_acc: 0.5772 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7098 - val_loss: 7.6514 - val_gender_output_loss: 0.6522 - val_image_quality_output_loss: 0.9714 - val_age_output_loss: 1.4173 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.9102 - val_footwear_output_loss: 0.9185 - val_pose_output_loss: 0.9136 - val_emotion_output_loss: 0.8917 - val_gender_output_acc: 0.5897 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.5830 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00020: val_loss improved from 7.65842 to 7.65144, saving model to gdrive/My Drive/saved_models/model-020-7.6514.hdf5\n",
            "Epoch 21/100\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.6969 - gender_output_loss: 0.6526 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4107 - weight_output_loss: 0.9839 - bag_output_loss: 0.9094 - footwear_output_loss: 0.9275 - pose_output_loss: 0.9160 - emotion_output_loss: 0.9160 - gender_output_acc: 0.6047 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3992 - weight_output_acc: 0.6349 - bag_output_acc: 0.5653 - footwear_output_acc: 0.5716 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7103 - val_loss: 7.6502 - val_gender_output_loss: 0.6557 - val_image_quality_output_loss: 0.9740 - val_age_output_loss: 1.4150 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.9087 - val_footwear_output_loss: 0.9163 - val_pose_output_loss: 0.9095 - val_emotion_output_loss: 0.8886 - val_gender_output_acc: 0.5960 - val_image_quality_output_acc: 0.5513 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5759 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00021: val_loss improved from 7.65144 to 7.65019, saving model to gdrive/My Drive/saved_models/model-021-7.6502.hdf5\n",
            "Epoch 22/100\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.6969 - gender_output_loss: 0.6526 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4107 - weight_output_loss: 0.9839 - bag_output_loss: 0.9094 - footwear_output_loss: 0.9275 - pose_output_loss: 0.9160 - emotion_output_loss: 0.9160 - gender_output_acc: 0.6047 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3992 - weight_output_acc: 0.6349 - bag_output_acc: 0.5653 - footwear_output_acc: 0.5716 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7103 - val_loss: 7.6502 - val_gender_output_loss: 0.6557 - val_image_quality_output_loss: 0.9740 - val_age_output_loss: 1.4150 - val_weight_output_loss: 0.9825 - val_bag_output_loss: 0.9087 - val_footwear_output_loss: 0.9163 - val_pose_output_loss: 0.9095 - val_emotion_output_loss: 0.8886 - val_gender_output_acc: 0.5960 - val_image_quality_output_acc: 0.5513 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5759 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6939 - gender_output_loss: 0.6515 - image_quality_output_loss: 0.9796 - age_output_loss: 1.4139 - weight_output_loss: 0.9815 - bag_output_loss: 0.9068 - footwear_output_loss: 0.9262 - pose_output_loss: 0.9196 - emotion_output_loss: 0.9148 - gender_output_acc: 0.6077 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3952 - weight_output_acc: 0.6355 - bag_output_acc: 0.5643 - footwear_output_acc: 0.5736 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7100 - val_loss: 7.6447 - val_gender_output_loss: 0.6513 - val_image_quality_output_loss: 0.9734 - val_age_output_loss: 1.4116 - val_weight_output_loss: 0.9764 - val_bag_output_loss: 0.9088 - val_footwear_output_loss: 0.9188 - val_pose_output_loss: 0.9151 - val_emotion_output_loss: 0.8892 - val_gender_output_acc: 0.5990 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5558 - val_footwear_output_acc: 0.5740 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00022: val_loss improved from 7.65019 to 7.64473, saving model to gdrive/My Drive/saved_models/model-022-7.6447.hdf5\n",
            "Epoch 23/100\n",
            "339/339 [==============================] - 197s 582ms/step - loss: 7.6756 - gender_output_loss: 0.6504 - image_quality_output_loss: 0.9796 - age_output_loss: 1.4055 - weight_output_loss: 0.9817 - bag_output_loss: 0.9089 - footwear_output_loss: 0.9199 - pose_output_loss: 0.9147 - emotion_output_loss: 0.9149 - gender_output_acc: 0.6017 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4008 - weight_output_acc: 0.6359 - bag_output_acc: 0.5634 - footwear_output_acc: 0.5742 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7100 - val_loss: 7.6209 - val_gender_output_loss: 0.6480 - val_image_quality_output_loss: 0.9728 - val_age_output_loss: 1.4076 - val_weight_output_loss: 0.9775 - val_bag_output_loss: 0.9050 - val_footwear_output_loss: 0.9082 - val_pose_output_loss: 0.9140 - val_emotion_output_loss: 0.8879 - val_gender_output_acc: 0.6071 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3969 - val_weight_output_acc: 0.6354 - val_bag_output_acc: 0.5603 - val_footwear_output_acc: 0.5833 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================]\n",
            "Epoch 00023: val_loss improved from 7.64473 to 7.62087, saving model to gdrive/My Drive/saved_models/model-023-7.6209.hdf5\n",
            "Epoch 24/100\n",
            "339/339 [==============================] - 196s 578ms/step - loss: 7.6696 - gender_output_loss: 0.6499 - image_quality_output_loss: 0.9778 - age_output_loss: 1.4066 - weight_output_loss: 0.9820 - bag_output_loss: 0.9072 - footwear_output_loss: 0.9152 - pose_output_loss: 0.9149 - emotion_output_loss: 0.9162 - gender_output_acc: 0.6121 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3961 - weight_output_acc: 0.6350 - bag_output_acc: 0.5624 - footwear_output_acc: 0.5805 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7103 - val_loss: 7.6384 - val_gender_output_loss: 0.6479 - val_image_quality_output_loss: 0.9739 - val_age_output_loss: 1.4089 - val_weight_output_loss: 0.9816 - val_bag_output_loss: 0.9062 - val_footwear_output_loss: 0.9147 - val_pose_output_loss: 0.9151 - val_emotion_output_loss: 0.8901 - val_gender_output_acc: 0.6086 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5569 - val_footwear_output_acc: 0.5737 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 7.62087\n",
            "Epoch 25/100\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6646 - gender_output_loss: 0.6480 - image_quality_output_loss: 0.9772 - age_output_loss: 1.4076 - weight_output_loss: 0.9805 - bag_output_loss: 0.9057 - footwear_output_loss: 0.9157 - pose_output_loss: 0.9153 - emotion_output_loss: 0.9146 - gender_output_acc: 0.6077 - image_quality_output_acc: 0.5518 - age_output_acc: 0.3937 - weight_output_acc: 0.6351 - bag_output_acc: 0.5646 - footwear_output_acc: 0.5789 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7100 - val_loss: 7.6180 - val_gender_output_loss: 0.6483 - val_image_quality_output_loss: 0.9721 - val_age_output_loss: 1.4093 - val_weight_output_loss: 0.9776 - val_bag_output_loss: 0.9053 - val_footwear_output_loss: 0.9096 - val_pose_output_loss: 0.9094 - val_emotion_output_loss: 0.8864 - val_gender_output_acc: 0.6034 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5577 - val_footwear_output_acc: 0.5774 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 7.62087\n",
            "Epoch 00025: val_loss improved from 7.62087 to 7.61804, saving model to gdrive/My Drive/saved_models/model-025-7.6180.hdf5\n",
            "Epoch 26/100\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6453 - gender_output_loss: 0.6479 - image_quality_output_loss: 0.9754 - age_output_loss: 1.4057 - weight_output_loss: 0.9818 - bag_output_loss: 0.9034 - footwear_output_loss: 0.9064 - pose_output_loss: 0.9123 - emotion_output_loss: 0.9123 - gender_output_acc: 0.6118 - image_quality_output_acc: 0.5525 - age_output_acc: 0.4010 - weight_output_acc: 0.6353 - bag_output_acc: 0.5678 - footwear_output_acc: 0.5817 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7099 - val_loss: 7.6146 - val_gender_output_loss: 0.6459 - val_image_quality_output_loss: 0.9729 - val_age_output_loss: 1.4105 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9047 - val_footwear_output_loss: 0.9072 - val_pose_output_loss: 0.9088 - val_emotion_output_loss: 0.8875 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3999 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5536 - val_footwear_output_acc: 0.5807 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00025: val_loss improved from 7.62087 to 7.61804, saving model to gdrive/My Drive/saved_models/model-025-7.6180.hdf5\n",
            "\n",
            "Epoch 00026: val_loss improved from 7.61804 to 7.61462, saving model to gdrive/My Drive/saved_models/model-026-7.6146.hdf5\n",
            "Epoch 27/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6505 - gender_output_loss: 0.6466 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4070 - weight_output_loss: 0.9804 - bag_output_loss: 0.9053 - footwear_output_loss: 0.9090 - pose_output_loss: 0.9123 - emotion_output_loss: 0.9137 - gender_output_acc: 0.6072 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3980 - weight_output_acc: 0.6361 - bag_output_acc: 0.5680 - footwear_output_acc: 0.5823 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7095\n",
            "Epoch 00026: val_loss improved from 7.61804 to 7.61462, saving model to gdrive/My Drive/saved_models/model-026-7.6146.hdf5\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.6509 - gender_output_loss: 0.6463 - image_quality_output_loss: 0.9771 - age_output_loss: 1.4071 - weight_output_loss: 0.9807 - bag_output_loss: 0.9054 - footwear_output_loss: 0.9086 - pose_output_loss: 0.9128 - emotion_output_loss: 0.9129 - gender_output_acc: 0.6077 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3979 - weight_output_acc: 0.6360 - bag_output_acc: 0.5679 - footwear_output_acc: 0.5826 - pose_output_acc: 0.6179 - emotion_output_acc: 0.7099 - val_loss: 7.6115 - val_gender_output_loss: 0.6465 - val_image_quality_output_loss: 0.9728 - val_age_output_loss: 1.4091 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9036 - val_footwear_output_loss: 0.9066 - val_pose_output_loss: 0.9087 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.6124 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5543 - val_footwear_output_acc: 0.5811 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00027: val_loss improved from 7.61462 to 7.61148, saving model to gdrive/My Drive/saved_models/model-027-7.6115.hdf5\n",
            "Epoch 28/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6474 - gender_output_loss: 0.6429 - image_quality_output_loss: 0.9769 - age_output_loss: 1.4082 - weight_output_loss: 0.9800 - bag_output_loss: 0.9042 - footwear_output_loss: 0.9095 - pose_output_loss: 0.9103 - emotion_output_loss: 0.9154 - gender_output_acc: 0.6128 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3972 - weight_output_acc: 0.6351 - bag_output_acc: 0.5680 - footwear_output_acc: 0.5784 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7095\n",
            "Epoch 00027: val_loss improved from 7.61462 to 7.61148, saving model to gdrive/My Drive/saved_models/model-027-7.6115.hdf5\n",
            "339/339 [==============================] - 195s 577ms/step - loss: 7.6456 - gender_output_loss: 0.6428 - image_quality_output_loss: 0.9770 - age_output_loss: 1.4077 - weight_output_loss: 0.9796 - bag_output_loss: 0.9041 - footwear_output_loss: 0.9092 - pose_output_loss: 0.9106 - emotion_output_loss: 0.9147 - gender_output_acc: 0.6130 - image_quality_output_acc: 0.5512 - age_output_acc: 0.3975 - weight_output_acc: 0.6351 - bag_output_acc: 0.5682 - footwear_output_acc: 0.5785 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7099 - val_loss: 7.6097 - val_gender_output_loss: 0.6462 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4092 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9025 - val_footwear_output_loss: 0.9067 - val_pose_output_loss: 0.9086 - val_emotion_output_loss: 0.8867 - val_gender_output_acc: 0.6116 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.5815 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00028: val_loss improved from 7.61148 to 7.60966, saving model to gdrive/My Drive/saved_models/model-028-7.6097.hdf5\n",
            "Epoch 29/100\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.6493 - gender_output_loss: 0.6458 - image_quality_output_loss: 0.9781 - age_output_loss: 1.4087 - weight_output_loss: 0.9790 - bag_output_loss: 0.9023 - footwear_output_loss: 0.9121 - pose_output_loss: 0.9107 - emotion_output_loss: 0.9127 - gender_output_acc: 0.6113 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3928 - weight_output_acc: 0.6358 - bag_output_acc: 0.5679 - footwear_output_acc: 0.5807 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7097 - val_loss: 7.6091 - val_gender_output_loss: 0.6463 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4086 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9023 - val_footwear_output_loss: 0.9065 - val_pose_output_loss: 0.9088 - val_emotion_output_loss: 0.8873 - val_gender_output_acc: 0.6094 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5818 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00029: val_loss improved from 7.60966 to 7.60914, saving model to gdrive/My Drive/saved_models/model-029-7.6091.hdf5\n",
            "Epoch 30/100\n",
            "339/339 [==============================] - 200s 589ms/step - loss: 7.6507 - gender_output_loss: 0.6472 - image_quality_output_loss: 0.9772 - age_output_loss: 1.4075 - weight_output_loss: 0.9799 - bag_output_loss: 0.9026 - footwear_output_loss: 0.9096 - pose_output_loss: 0.9135 - emotion_output_loss: 0.9131 - gender_output_acc: 0.6072 - image_quality_output_acc: 0.5519 - age_output_acc: 0.3980 - weight_output_acc: 0.6354 - bag_output_acc: 0.5695 - footwear_output_acc: 0.5831 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7099 - val_loss: 7.6081 - val_gender_output_loss: 0.6462 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4091 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9016 - val_footwear_output_loss: 0.9074 - val_pose_output_loss: 0.9078 - val_emotion_output_loss: 0.8869 - val_gender_output_acc: 0.6083 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5807 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00029: val_loss improved from 7.60966 to 7.60914, saving model to gdrive/My Drive/saved_models/model-029-7.6091.hdf5\n",
            "Epoch 00030: val_loss improved from 7.60914 to 7.60805, saving model to gdrive/My Drive/saved_models/model-030-7.6081.hdf5\n",
            "Epoch 31/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6432 - gender_output_loss: 0.6446 - image_quality_output_loss: 0.9775 - age_output_loss: 1.4067 - weight_output_loss: 0.9799 - bag_output_loss: 0.9008 - footwear_output_loss: 0.9095 - pose_output_loss: 0.9097 - emotion_output_loss: 0.9145 - gender_output_acc: 0.6120 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3925 - weight_output_acc: 0.6354 - bag_output_acc: 0.5681 - footwear_output_acc: 0.5827 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7098\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6446 - gender_output_loss: 0.6445 - image_quality_output_loss: 0.9775 - age_output_loss: 1.4072 - weight_output_loss: 0.9803 - bag_output_loss: 0.9011 - footwear_output_loss: 0.9100 - pose_output_loss: 0.9096 - emotion_output_loss: 0.9144 - gender_output_acc: 0.6124 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3923 - weight_output_acc: 0.6352 - bag_output_acc: 0.5682 - footwear_output_acc: 0.5823 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7097 - val_loss: 7.6091 - val_gender_output_loss: 0.6462 - val_image_quality_output_loss: 0.9730 - val_age_output_loss: 1.4084 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9023 - val_footwear_output_loss: 0.9062 - val_pose_output_loss: 0.9081 - val_emotion_output_loss: 0.8876 - val_gender_output_acc: 0.6105 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5822 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 7.60805\n",
            "Epoch 32/100\n",
            "339/339 [==============================] - 197s 580ms/step - loss: 7.6444 - gender_output_loss: 0.6434 - image_quality_output_loss: 0.9773 - age_output_loss: 1.4067 - weight_output_loss: 0.9804 - bag_output_loss: 0.9018 - footwear_output_loss: 0.9115 - pose_output_loss: 0.9110 - emotion_output_loss: 0.9123 - gender_output_acc: 0.6162 - image_quality_output_acc: 0.5515 - age_output_acc: 0.3987 - weight_output_acc: 0.6353 - bag_output_acc: 0.5644 - footwear_output_acc: 0.5808 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7101 - val_loss: 7.6090 - val_gender_output_loss: 0.6460 - val_image_quality_output_loss: 0.9730 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.9025 - val_footwear_output_loss: 0.9068 - val_pose_output_loss: 0.9077 - val_emotion_output_loss: 0.8873 - val_gender_output_acc: 0.6101 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5804 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 7.60805\n",
            "Epoch 33/100\n",
            "339/339 [==============================] - 199s 586ms/step - loss: 7.6401 - gender_output_loss: 0.6446 - image_quality_output_loss: 0.9772 - age_output_loss: 1.4065 - weight_output_loss: 0.9789 - bag_output_loss: 0.9030 - footwear_output_loss: 0.9102 - pose_output_loss: 0.9104 - emotion_output_loss: 0.9092 - gender_output_acc: 0.6079 - image_quality_output_acc: 0.5536 - age_output_acc: 0.3992 - weight_output_acc: 0.6355 - bag_output_acc: 0.5693 - footwear_output_acc: 0.5796 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7100 - val_loss: 7.6083 - val_gender_output_loss: 0.6458 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.4087 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9023 - val_footwear_output_loss: 0.9064 - val_pose_output_loss: 0.9079 - val_emotion_output_loss: 0.8864 - val_gender_output_acc: 0.6060 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5577 - val_footwear_output_acc: 0.5804 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 7.60805\n",
            "Epoch 34/100\n",
            "339/339 [==============================] - 200s 591ms/step - loss: 7.6323 - gender_output_loss: 0.6423 - image_quality_output_loss: 0.9750 - age_output_loss: 1.4053 - weight_output_loss: 0.9775 - bag_output_loss: 0.9010 - footwear_output_loss: 0.9083 - pose_output_loss: 0.9114 - emotion_output_loss: 0.9116 - gender_output_acc: 0.6188 - image_quality_output_acc: 0.5517 - age_output_acc: 0.3985 - weight_output_acc: 0.6353 - bag_output_acc: 0.5711 - footwear_output_acc: 0.5782 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7101 - val_loss: 7.6092 - val_gender_output_loss: 0.6466 - val_image_quality_output_loss: 0.9729 - val_age_output_loss: 1.4073 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9027 - val_footwear_output_loss: 0.9062 - val_pose_output_loss: 0.9088 - val_emotion_output_loss: 0.8873 - val_gender_output_acc: 0.6053 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5577 - val_footwear_output_acc: 0.5774 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 7.60805\n",
            "Epoch 35/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6427 - gender_output_loss: 0.6442 - image_quality_output_loss: 0.9755 - age_output_loss: 1.4049 - weight_output_loss: 0.9788 - bag_output_loss: 0.9037 - footwear_output_loss: 0.9105 - pose_output_loss: 0.9106 - emotion_output_loss: 0.9144 - gender_output_acc: 0.6161 - image_quality_output_acc: 0.5538 - age_output_acc: 0.3969 - weight_output_acc: 0.6356 - bag_output_acc: 0.5670 - footwear_output_acc: 0.5789 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7096\n",
            "339/339 [==============================] - 207s 611ms/step - loss: 7.6436 - gender_output_loss: 0.6442 - image_quality_output_loss: 0.9757 - age_output_loss: 1.4050 - weight_output_loss: 0.9792 - bag_output_loss: 0.9040 - footwear_output_loss: 0.9107 - pose_output_loss: 0.9110 - emotion_output_loss: 0.9138 - gender_output_acc: 0.6161 - image_quality_output_acc: 0.5536 - age_output_acc: 0.3968 - weight_output_acc: 0.6353 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5785 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7099 - val_loss: 7.6090 - val_gender_output_loss: 0.6461 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.4079 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9026 - val_footwear_output_loss: 0.9066 - val_pose_output_loss: 0.9082 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.6075 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5770 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 7.60805\n",
            "Epoch 36/100\n",
            "339/339 [==============================] - 202s 597ms/step - loss: 7.6424 - gender_output_loss: 0.6458 - image_quality_output_loss: 0.9758 - age_output_loss: 1.4057 - weight_output_loss: 0.9800 - bag_output_loss: 0.9009 - footwear_output_loss: 0.9090 - pose_output_loss: 0.9108 - emotion_output_loss: 0.9144 - gender_output_acc: 0.6122 - image_quality_output_acc: 0.5516 - age_output_acc: 0.4015 - weight_output_acc: 0.6356 - bag_output_acc: 0.5694 - footwear_output_acc: 0.5808 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7099 - val_loss: 7.6082 - val_gender_output_loss: 0.6456 - val_image_quality_output_loss: 0.9737 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.9021 - val_footwear_output_loss: 0.9065 - val_pose_output_loss: 0.9081 - val_emotion_output_loss: 0.8865 - val_gender_output_acc: 0.6094 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.4014 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5569 - val_footwear_output_acc: 0.5781 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 7.60805\n",
            "Epoch 37/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6404 - gender_output_loss: 0.6427 - image_quality_output_loss: 0.9749 - age_output_loss: 1.4055 - weight_output_loss: 0.9793 - bag_output_loss: 0.9019 - footwear_output_loss: 0.9101 - pose_output_loss: 0.9117 - emotion_output_loss: 0.9142 - gender_output_acc: 0.6138 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3969 - weight_output_acc: 0.6357 - bag_output_acc: 0.5674 - footwear_output_acc: 0.5774 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7100\n",
            "Epoch 00036: val_loss did not improve from 7.60805\n",
            "Epoch 37/100\n",
            "339/339 [==============================] - 197s 582ms/step - loss: 7.6408 - gender_output_loss: 0.6426 - image_quality_output_loss: 0.9755 - age_output_loss: 1.4055 - weight_output_loss: 0.9798 - bag_output_loss: 0.9016 - footwear_output_loss: 0.9101 - pose_output_loss: 0.9119 - emotion_output_loss: 0.9137 - gender_output_acc: 0.6139 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3970 - weight_output_acc: 0.6354 - bag_output_acc: 0.5677 - footwear_output_acc: 0.5774 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7103 - val_loss: 7.6090 - val_gender_output_loss: 0.6461 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.4081 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9024 - val_footwear_output_loss: 0.9065 - val_pose_output_loss: 0.9080 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5778 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 7.60805\n",
            "Epoch 38/100\n",
            " 45/339 [==>...........................] - ETA: 2:42 - loss: 7.6759 - gender_output_loss: 0.6534 - image_quality_output_loss: 0.9660 - age_output_loss: 1.4182 - weight_output_loss: 0.9981 - bag_output_loss: 0.9083 - footwear_output_loss: 0.9130 - pose_output_loss: 0.9036 - emotion_output_loss: 0.9154 - gender_output_acc: 0.6021 - image_quality_output_acc: 0.5632 - age_output_acc: 0.3854 - weight_output_acc: 0.6292 - bag_output_acc: 0.5576 - footwear_output_acc: 0.5868 - pose_output_acc: 0.6215 - emotion_output_acc: 0.7104"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-450:\n",
            "Process ForkPoolWorker-452:\n",
            "Process ForkPoolWorker-448:\n",
            "Process ForkPoolWorker-447:\n",
            "Process ForkPoolWorker-446:\n",
            "Process ForkPoolWorker-445:\n",
            "Process ForkPoolWorker-449:\n",
            "Process ForkPoolWorker-455:\n",
            "Process ForkPoolWorker-453:\n",
            "Process ForkPoolWorker-456:\n",
            "Process ForkPoolWorker-454:\n",
            "Process ForkPoolWorker-451:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"<ipython-input-6-669647fad76f>\", line 52, in __getitem__\n",
            "    image = self.augmentation.flow(image, shuffle=False, batch_size=self.batch_size).next()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"<ipython-input-6-669647fad76f>\", line 52, in __getitem__\n",
            "    image = self.augmentation.flow(image, shuffle=False, batch_size=self.batch_size).next()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py\", line 153, in _get_batches_of_transformed_samples\n",
            "    x.astype(self.dtype), params)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-580866953faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#class_weight= class_weights,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#callbacks=[lr_reducer, early_stopper, csv_logger, checkpoint, LearningRateScheduler(scheduler, verbose=1)],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24R2ennSvbi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('gdrive/My Drive/saved_models/model-030-7.6081.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkKnxP-xvqgJ",
        "colab_type": "code",
        "outputId": "715f6930-bb4f-472c-a3c0-f7fbccc5fc7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    initial_epoch=37,\n",
        "    #class_weight= class_weights,\n",
        "    #callbacks=[lr_reducer, early_stopper, csv_logger, checkpoint, LearningRateScheduler(scheduler, verbose=1)],\n",
        "    callbacks=[checkpoint,clr]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 38/100\n",
            "339/339 [==============================] - 213s 628ms/step - loss: 7.6420 - gender_output_loss: 0.6483 - image_quality_output_loss: 0.9752 - age_output_loss: 1.4056 - weight_output_loss: 0.9787 - bag_output_loss: 0.9030 - footwear_output_loss: 0.9080 - pose_output_loss: 0.9109 - emotion_output_loss: 0.9123 - gender_output_acc: 0.6074 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3968 - weight_output_acc: 0.6346 - bag_output_acc: 0.5713 - footwear_output_acc: 0.5810 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7100 - val_loss: 7.6091 - val_gender_output_loss: 0.6466 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4089 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9021 - val_footwear_output_loss: 0.9072 - val_pose_output_loss: 0.9079 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.6101 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5592 - val_footwear_output_acc: 0.5815 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 7.60805\n",
            "Epoch 39/100\n",
            "339/339 [==============================] - 213s 628ms/step - loss: 7.6420 - gender_output_loss: 0.6483 - image_quality_output_loss: 0.9752 - age_output_loss: 1.4056 - weight_output_loss: 0.9787 - bag_output_loss: 0.9030 - footwear_output_loss: 0.9080 - pose_output_loss: 0.9109 - emotion_output_loss: 0.9123 - gender_output_acc: 0.6074 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3968 - weight_output_acc: 0.6346 - bag_output_acc: 0.5713 - footwear_output_acc: 0.5810 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7100 - val_loss: 7.6091 - val_gender_output_loss: 0.6466 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4089 - val_weight_output_loss: 0.9772 - val_bag_output_loss: 0.9021 - val_footwear_output_loss: 0.9072 - val_pose_output_loss: 0.9079 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.6101 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5592 - val_footwear_output_acc: 0.5815 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================] - 202s 595ms/step - loss: 7.6399 - gender_output_loss: 0.6444 - image_quality_output_loss: 0.9742 - age_output_loss: 1.4066 - weight_output_loss: 0.9763 - bag_output_loss: 0.9038 - footwear_output_loss: 0.9093 - pose_output_loss: 0.9125 - emotion_output_loss: 0.9128 - gender_output_acc: 0.6150 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3975 - weight_output_acc: 0.6356 - bag_output_acc: 0.5666 - footwear_output_acc: 0.5806 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7099 - val_loss: 7.6089 - val_gender_output_loss: 0.6463 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.4087 - val_weight_output_loss: 0.9773 - val_bag_output_loss: 0.9018 - val_footwear_output_loss: 0.9071 - val_pose_output_loss: 0.9080 - val_emotion_output_loss: 0.8871 - val_gender_output_acc: 0.6105 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.4010 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5826 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 7.60805\n",
            "Epoch 40/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6447 - gender_output_loss: 0.6439 - image_quality_output_loss: 0.9765 - age_output_loss: 1.4050 - weight_output_loss: 0.9808 - bag_output_loss: 0.9029 - footwear_output_loss: 0.9123 - pose_output_loss: 0.9119 - emotion_output_loss: 0.9114 - gender_output_acc: 0.6153 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3964 - weight_output_acc: 0.6355 - bag_output_acc: 0.5655 - footwear_output_acc: 0.5797 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7098\n",
            "Epoch 00039: val_loss did not improve from 7.60805\n",
            "Epoch 40/100\n",
            "339/339 [==============================] - 198s 584ms/step - loss: 7.6448 - gender_output_loss: 0.6437 - image_quality_output_loss: 0.9764 - age_output_loss: 1.4047 - weight_output_loss: 0.9808 - bag_output_loss: 0.9027 - footwear_output_loss: 0.9126 - pose_output_loss: 0.9125 - emotion_output_loss: 0.9114 - gender_output_acc: 0.6156 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3963 - weight_output_acc: 0.6355 - bag_output_acc: 0.5655 - footwear_output_acc: 0.5794 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7098 - val_loss: 7.6086 - val_gender_output_loss: 0.6462 - val_image_quality_output_loss: 0.9722 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9019 - val_footwear_output_loss: 0.9068 - val_pose_output_loss: 0.9085 - val_emotion_output_loss: 0.8875 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3999 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5796 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 7.60805\n",
            "Epoch 41/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6430 - gender_output_loss: 0.6451 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4052 - weight_output_loss: 0.9772 - bag_output_loss: 0.9047 - footwear_output_loss: 0.9106 - pose_output_loss: 0.9127 - emotion_output_loss: 0.9113 - gender_output_acc: 0.6102 - image_quality_output_acc: 0.5538 - age_output_acc: 0.3978 - weight_output_acc: 0.6350 - bag_output_acc: 0.5683 - footwear_output_acc: 0.5814 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7104\n",
            "Epoch 00040: val_loss did not improve from 7.60805\n",
            "Epoch 41/100\n",
            "339/339 [==============================] - 197s 582ms/step - loss: 7.6437 - gender_output_loss: 0.6450 - image_quality_output_loss: 0.9765 - age_output_loss: 1.4053 - weight_output_loss: 0.9770 - bag_output_loss: 0.9047 - footwear_output_loss: 0.9104 - pose_output_loss: 0.9126 - emotion_output_loss: 0.9121 - gender_output_acc: 0.6103 - image_quality_output_acc: 0.5535 - age_output_acc: 0.3977 - weight_output_acc: 0.6351 - bag_output_acc: 0.5690 - footwear_output_acc: 0.5816 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7100 - val_loss: 7.6080 - val_gender_output_loss: 0.6464 - val_image_quality_output_loss: 0.9718 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9768 - val_bag_output_loss: 0.9017 - val_footwear_output_loss: 0.9069 - val_pose_output_loss: 0.9085 - val_emotion_output_loss: 0.8874 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5528 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5569 - val_footwear_output_acc: 0.5785 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00041: val_loss improved from 7.60805 to 7.60800, saving model to gdrive/My Drive/saved_models/model-041-7.6080.hdf5\n",
            "Epoch 42/100\n",
            "339/339 [==============================] - 198s 583ms/step - loss: 7.6410 - gender_output_loss: 0.6435 - image_quality_output_loss: 0.9748 - age_output_loss: 1.4049 - weight_output_loss: 0.9774 - bag_output_loss: 0.9041 - footwear_output_loss: 0.9106 - pose_output_loss: 0.9116 - emotion_output_loss: 0.9140 - gender_output_acc: 0.6115 - image_quality_output_acc: 0.5514 - age_output_acc: 0.3951 - weight_output_acc: 0.6360 - bag_output_acc: 0.5690 - footwear_output_acc: 0.5835 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7101 - val_loss: 7.6080 - val_gender_output_loss: 0.6464 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9016 - val_footwear_output_loss: 0.9068 - val_pose_output_loss: 0.9084 - val_emotion_output_loss: 0.8875 - val_gender_output_acc: 0.6071 - val_image_quality_output_acc: 0.5528 - val_age_output_acc: 0.3984 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5577 - val_footwear_output_acc: 0.5796 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 7.60800\n",
            "Epoch 43/100\n",
            "339/339 [==============================] - 198s 583ms/step - loss: 7.6410 - gender_output_loss: 0.6435 - image_quality_output_loss: 0.9748 - age_output_loss: 1.4049 - weight_output_loss: 0.9774 - bag_output_loss: 0.9041 - footwear_output_loss: 0.9106 - pose_output_loss: 0.9116 - emotion_output_loss: 0.9140 - gender_output_acc: 0.6115 - image_quality_output_acc: 0.5514 - age_output_acc: 0.3951 - weight_output_acc: 0.6360 - bag_output_acc: 0.5690 - footwear_output_acc: 0.5835 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7101 - val_loss: 7.6080 - val_gender_output_loss: 0.6464 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9016 - val_footwear_output_loss: 0.9068 - val_pose_output_loss: 0.9084 - val_emotion_output_loss: 0.8875 - val_gender_output_acc: 0.6071 - val_image_quality_output_acc: 0.5528 - val_age_output_acc: 0.3984 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5577 - val_footwear_output_acc: 0.5796 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6419 - gender_output_loss: 0.6425 - image_quality_output_loss: 0.9758 - age_output_loss: 1.4054 - weight_output_loss: 0.9790 - bag_output_loss: 0.9044 - footwear_output_loss: 0.9099 - pose_output_loss: 0.9123 - emotion_output_loss: 0.9126 - gender_output_acc: 0.6159 - image_quality_output_acc: 0.5524 - age_output_acc: 0.3966 - weight_output_acc: 0.6348 - bag_output_acc: 0.5682 - footwear_output_acc: 0.5811 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7100 - val_loss: 7.6070 - val_gender_output_loss: 0.6460 - val_image_quality_output_loss: 0.9720 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9017 - val_footwear_output_loss: 0.9067 - val_pose_output_loss: 0.9089 - val_emotion_output_loss: 0.8866 - val_gender_output_acc: 0.6105 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5592 - val_footwear_output_acc: 0.5785 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00043: val_loss improved from 7.60800 to 7.60702, saving model to gdrive/My Drive/saved_models/model-043-7.6070.hdf5\n",
            "Epoch 44/100\n",
            "339/339 [==============================] - 198s 585ms/step - loss: 7.6462 - gender_output_loss: 0.6463 - image_quality_output_loss: 0.9757 - age_output_loss: 1.4069 - weight_output_loss: 0.9787 - bag_output_loss: 0.9043 - footwear_output_loss: 0.9082 - pose_output_loss: 0.9111 - emotion_output_loss: 0.9149 - gender_output_acc: 0.6093 - image_quality_output_acc: 0.5528 - age_output_acc: 0.3967 - weight_output_acc: 0.6352 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5806 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7100 - val_loss: 7.6085 - val_gender_output_loss: 0.6464 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4086 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.9018 - val_footwear_output_loss: 0.9071 - val_pose_output_loss: 0.9088 - val_emotion_output_loss: 0.8865 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5577 - val_footwear_output_acc: 0.5800 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 7.60702\n",
            "Epoch 45/100\n",
            "339/339 [==============================] - 198s 584ms/step - loss: 7.6414 - gender_output_loss: 0.6440 - image_quality_output_loss: 0.9799 - age_output_loss: 1.4041 - weight_output_loss: 0.9781 - bag_output_loss: 0.9027 - footwear_output_loss: 0.9082 - pose_output_loss: 0.9112 - emotion_output_loss: 0.9132 - gender_output_acc: 0.6183 - image_quality_output_acc: 0.5520 - age_output_acc: 0.3957 - weight_output_acc: 0.6353 - bag_output_acc: 0.5668 - footwear_output_acc: 0.5826 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7100 - val_loss: 7.6079 - val_gender_output_loss: 0.6460 - val_image_quality_output_loss: 0.9726 - val_age_output_loss: 1.4079 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.9012 - val_footwear_output_loss: 0.9074 - val_pose_output_loss: 0.9090 - val_emotion_output_loss: 0.8871 - val_gender_output_acc: 0.6124 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5577 - val_footwear_output_acc: 0.5796 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 7.60702\n",
            "Epoch 46/100\n",
            "339/339 [==============================] - 199s 586ms/step - loss: 7.6365 - gender_output_loss: 0.6457 - image_quality_output_loss: 0.9748 - age_output_loss: 1.4041 - weight_output_loss: 0.9765 - bag_output_loss: 0.9018 - footwear_output_loss: 0.9075 - pose_output_loss: 0.9132 - emotion_output_loss: 0.9128 - gender_output_acc: 0.6082 - image_quality_output_acc: 0.5516 - age_output_acc: 0.3990 - weight_output_acc: 0.6355 - bag_output_acc: 0.5695 - footwear_output_acc: 0.5829 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7100 - val_loss: 7.6070 - val_gender_output_loss: 0.6451 - val_image_quality_output_loss: 0.9727 - val_age_output_loss: 1.4085 - val_weight_output_loss: 0.9771 - val_bag_output_loss: 0.9018 - val_footwear_output_loss: 0.9066 - val_pose_output_loss: 0.9076 - val_emotion_output_loss: 0.8877 - val_gender_output_acc: 0.6164 - val_image_quality_output_acc: 0.5532 - val_age_output_acc: 0.3996 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5569 - val_footwear_output_acc: 0.5781 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00046: val_loss improved from 7.60702 to 7.60696, saving model to gdrive/My Drive/saved_models/model-046-7.6070.hdf5\n",
            "Epoch 47/100\n",
            "339/339 [==============================] - 198s 585ms/step - loss: 7.6421 - gender_output_loss: 0.6433 - image_quality_output_loss: 0.9764 - age_output_loss: 1.4069 - weight_output_loss: 0.9804 - bag_output_loss: 0.9050 - footwear_output_loss: 0.9066 - pose_output_loss: 0.9116 - emotion_output_loss: 0.9120 - gender_output_acc: 0.6103 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3977 - weight_output_acc: 0.6353 - bag_output_acc: 0.5654 - footwear_output_acc: 0.5809 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7101 - val_loss: 7.6069 - val_gender_output_loss: 0.6454 - val_image_quality_output_loss: 0.9728 - val_age_output_loss: 1.4086 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.9013 - val_footwear_output_loss: 0.9068 - val_pose_output_loss: 0.9075 - val_emotion_output_loss: 0.8878 - val_gender_output_acc: 0.6079 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3999 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5610 - val_footwear_output_acc: 0.5770 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00047: val_loss improved from 7.60696 to 7.60693, saving model to gdrive/My Drive/saved_models/model-047-7.6069.hdf5\n",
            "Epoch 48/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6389 - gender_output_loss: 0.6440 - image_quality_output_loss: 0.9768 - age_output_loss: 1.4061 - weight_output_loss: 0.9754 - bag_output_loss: 0.9049 - footwear_output_loss: 0.9088 - pose_output_loss: 0.9111 - emotion_output_loss: 0.9117 - gender_output_acc: 0.6172 - image_quality_output_acc: 0.5515 - age_output_acc: 0.3983 - weight_output_acc: 0.6352 - bag_output_acc: 0.5667 - footwear_output_acc: 0.5815 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7102\n",
            "339/339 [==============================] - 198s 584ms/step - loss: 7.6398 - gender_output_loss: 0.6440 - image_quality_output_loss: 0.9767 - age_output_loss: 1.4066 - weight_output_loss: 0.9754 - bag_output_loss: 0.9048 - footwear_output_loss: 0.9091 - pose_output_loss: 0.9113 - emotion_output_loss: 0.9119 - gender_output_acc: 0.6172 - image_quality_output_acc: 0.5516 - age_output_acc: 0.3980 - weight_output_acc: 0.6353 - bag_output_acc: 0.5669 - footwear_output_acc: 0.5815 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7099 - val_loss: 7.6046 - val_gender_output_loss: 0.6452 - val_image_quality_output_loss: 0.9721 - val_age_output_loss: 1.4081 - val_weight_output_loss: 0.9766 - val_bag_output_loss: 0.9018 - val_footwear_output_loss: 0.9049 - val_pose_output_loss: 0.9084 - val_emotion_output_loss: 0.8875 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5528 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5584 - val_footwear_output_acc: 0.5789 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00048: val_loss improved from 7.60693 to 7.60463, saving model to gdrive/My Drive/saved_models/model-048-7.6046.hdf5\n",
            "Epoch 49/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6382 - gender_output_loss: 0.6449 - image_quality_output_loss: 0.9757 - age_output_loss: 1.4049 - weight_output_loss: 0.9774 - bag_output_loss: 0.9045 - footwear_output_loss: 0.9135 - pose_output_loss: 0.9098 - emotion_output_loss: 0.9075 - gender_output_acc: 0.6091 - image_quality_output_acc: 0.5548 - age_output_acc: 0.3968 - weight_output_acc: 0.6354 - bag_output_acc: 0.5625 - footwear_output_acc: 0.5810 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7105\n",
            "339/339 [==============================] - 198s 583ms/step - loss: 7.6387 - gender_output_loss: 0.6448 - image_quality_output_loss: 0.9758 - age_output_loss: 1.4049 - weight_output_loss: 0.9775 - bag_output_loss: 0.9040 - footwear_output_loss: 0.9138 - pose_output_loss: 0.9096 - emotion_output_loss: 0.9083 - gender_output_acc: 0.6090 - image_quality_output_acc: 0.5546 - age_output_acc: 0.3965 - weight_output_acc: 0.6353 - bag_output_acc: 0.5625 - footwear_output_acc: 0.5809 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7101 - val_loss: 7.6076 - val_gender_output_loss: 0.6446 - val_image_quality_output_loss: 0.9724 - val_age_output_loss: 1.4086 - val_weight_output_loss: 0.9776 - val_bag_output_loss: 0.9014 - val_footwear_output_loss: 0.9058 - val_pose_output_loss: 0.9079 - val_emotion_output_loss: 0.8892 - val_gender_output_acc: 0.6097 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3966 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.5778 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 7.60463\n",
            "Epoch 50/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6415 - gender_output_loss: 0.6464 - image_quality_output_loss: 0.9772 - age_output_loss: 1.4040 - weight_output_loss: 0.9766 - bag_output_loss: 0.9049 - footwear_output_loss: 0.9111 - pose_output_loss: 0.9092 - emotion_output_loss: 0.9121 - gender_output_acc: 0.6082 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3971 - weight_output_acc: 0.6362 - bag_output_acc: 0.5672 - footwear_output_acc: 0.5803 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7105\n",
            "Epoch 50/100\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6420 - gender_output_loss: 0.6462 - image_quality_output_loss: 0.9770 - age_output_loss: 1.4036 - weight_output_loss: 0.9765 - bag_output_loss: 0.9051 - footwear_output_loss: 0.9116 - pose_output_loss: 0.9089 - emotion_output_loss: 0.9131 - gender_output_acc: 0.6083 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3972 - weight_output_acc: 0.6361 - bag_output_acc: 0.5670 - footwear_output_acc: 0.5799 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7101 - val_loss: 7.6032 - val_gender_output_loss: 0.6441 - val_image_quality_output_loss: 0.9723 - val_age_output_loss: 1.4080 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.9011 - val_footwear_output_loss: 0.9062 - val_pose_output_loss: 0.9066 - val_emotion_output_loss: 0.8879 - val_gender_output_acc: 0.6135 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5778 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00050: val_loss improved from 7.60463 to 7.60316, saving model to gdrive/My Drive/saved_models/model-050-7.6032.hdf5\n",
            "Epoch 51/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6499 - gender_output_loss: 0.6464 - image_quality_output_loss: 0.9795 - age_output_loss: 1.4060 - weight_output_loss: 0.9758 - bag_output_loss: 0.9028 - footwear_output_loss: 0.9143 - pose_output_loss: 0.9114 - emotion_output_loss: 0.9137 - gender_output_acc: 0.6093 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3972 - weight_output_acc: 0.6359 - bag_output_acc: 0.5680 - footwear_output_acc: 0.5780 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7098\n",
            "339/339 [==============================] - 197s 581ms/step - loss: 7.6420 - gender_output_loss: 0.6462 - image_quality_output_loss: 0.9770 - age_output_loss: 1.4036 - weight_output_loss: 0.9765 - bag_output_loss: 0.9051 - footwear_output_loss: 0.9116 - pose_output_loss: 0.9089 - emotion_output_loss: 0.9131 - gender_output_acc: 0.6083 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3972 - weight_output_acc: 0.6361 - bag_output_acc: 0.5670 - footwear_output_acc: 0.5799 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7101 - val_loss: 7.6032 - val_gender_output_loss: 0.6441 - val_image_quality_output_loss: 0.9723 - val_age_output_loss: 1.4080 - val_weight_output_loss: 0.9770 - val_bag_output_loss: 0.9011 - val_footwear_output_loss: 0.9062 - val_pose_output_loss: 0.9066 - val_emotion_output_loss: 0.8879 - val_gender_output_acc: 0.6135 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5580 - val_footwear_output_acc: 0.5778 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================] - 195s 577ms/step - loss: 7.6492 - gender_output_loss: 0.6465 - image_quality_output_loss: 0.9795 - age_output_loss: 1.4059 - weight_output_loss: 0.9761 - bag_output_loss: 0.9025 - footwear_output_loss: 0.9141 - pose_output_loss: 0.9112 - emotion_output_loss: 0.9132 - gender_output_acc: 0.6091 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3972 - weight_output_acc: 0.6358 - bag_output_acc: 0.5685 - footwear_output_acc: 0.5783 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7101 - val_loss: 7.6022 - val_gender_output_loss: 0.6433 - val_image_quality_output_loss: 0.9737 - val_age_output_loss: 1.4086 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.8993 - val_footwear_output_loss: 0.9066 - val_pose_output_loss: 0.9079 - val_emotion_output_loss: 0.8862 - val_gender_output_acc: 0.6131 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5636 - val_footwear_output_acc: 0.5755 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00051: val_loss improved from 7.60316 to 7.60220, saving model to gdrive/My Drive/saved_models/model-051-7.6022.hdf5\n",
            "Epoch 52/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.6485 - gender_output_loss: 0.6453 - image_quality_output_loss: 0.9792 - age_output_loss: 1.4071 - weight_output_loss: 0.9790 - bag_output_loss: 0.9039 - footwear_output_loss: 0.9106 - pose_output_loss: 0.9087 - emotion_output_loss: 0.9147 - gender_output_acc: 0.6160 - image_quality_output_acc: 0.5522 - age_output_acc: 0.3955 - weight_output_acc: 0.6356 - bag_output_acc: 0.5693 - footwear_output_acc: 0.5765 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7101 - val_loss: 7.6146 - val_gender_output_loss: 0.6449 - val_image_quality_output_loss: 0.9757 - val_age_output_loss: 1.4095 - val_weight_output_loss: 0.9776 - val_bag_output_loss: 0.9010 - val_footwear_output_loss: 0.9065 - val_pose_output_loss: 0.9086 - val_emotion_output_loss: 0.8909 - val_gender_output_acc: 0.6112 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.3940 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5588 - val_footwear_output_acc: 0.5737 - val_pose_output_acc: 0.6146 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 7.60220\n",
            "Epoch 53/100\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.6516 - gender_output_loss: 0.6482 - image_quality_output_loss: 0.9763 - age_output_loss: 1.4085 - weight_output_loss: 0.9801 - bag_output_loss: 0.9069 - footwear_output_loss: 0.9090 - pose_output_loss: 0.9104 - emotion_output_loss: 0.9122 - gender_output_acc: 0.5992 - image_quality_output_acc: 0.5520 - age_output_acc: 0.3932 - weight_output_acc: 0.6346 - bag_output_acc: 0.5681 - footwear_output_acc: 0.5807 - pose_output_acc: 0.6180 - emotion_output_acc: 0.7101 - val_loss: 7.6321 - val_gender_output_loss: 0.6420 - val_image_quality_output_loss: 0.9757 - val_age_output_loss: 1.4102 - val_weight_output_loss: 0.9827 - val_bag_output_loss: 0.9044 - val_footwear_output_loss: 0.9107 - val_pose_output_loss: 0.9137 - val_emotion_output_loss: 0.8926 - val_gender_output_acc: 0.6150 - val_image_quality_output_acc: 0.5577 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5625 - val_footwear_output_acc: 0.5733 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7188\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 7.60220\n",
            "Epoch 54/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.6473 - gender_output_loss: 0.6439 - image_quality_output_loss: 0.9771 - age_output_loss: 1.4072 - weight_output_loss: 0.9799 - bag_output_loss: 0.9055 - footwear_output_loss: 0.9106 - pose_output_loss: 0.9101 - emotion_output_loss: 0.9131 - gender_output_acc: 0.6134 - image_quality_output_acc: 0.5515 - age_output_acc: 0.4010 - weight_output_acc: 0.6353 - bag_output_acc: 0.5657 - footwear_output_acc: 0.5771 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7103 - val_loss: 7.6413 - val_gender_output_loss: 0.6459 - val_image_quality_output_loss: 0.9779 - val_age_output_loss: 1.4128 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9030 - val_footwear_output_loss: 0.9151 - val_pose_output_loss: 0.9122 - val_emotion_output_loss: 0.8960 - val_gender_output_acc: 0.6187 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5632 - val_footwear_output_acc: 0.5755 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 7.60220\n",
            "Epoch 55/100\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.6433 - gender_output_loss: 0.6424 - image_quality_output_loss: 0.9785 - age_output_loss: 1.4080 - weight_output_loss: 0.9787 - bag_output_loss: 0.9044 - footwear_output_loss: 0.9077 - pose_output_loss: 0.9100 - emotion_output_loss: 0.9136 - gender_output_acc: 0.6141 - image_quality_output_acc: 0.5537 - age_output_acc: 0.3954 - weight_output_acc: 0.6351 - bag_output_acc: 0.5663 - footwear_output_acc: 0.5810 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7102 - val_loss: 7.6190 - val_gender_output_loss: 0.6492 - val_image_quality_output_loss: 0.9741 - val_age_output_loss: 1.4090 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9015 - val_footwear_output_loss: 0.9159 - val_pose_output_loss: 0.9055 - val_emotion_output_loss: 0.8869 - val_gender_output_acc: 0.5986 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5618 - val_footwear_output_acc: 0.5692 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 7.60220\n",
            "Epoch 56/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.6452 - gender_output_loss: 0.6420 - image_quality_output_loss: 0.9785 - age_output_loss: 1.4070 - weight_output_loss: 0.9802 - bag_output_loss: 0.9036 - footwear_output_loss: 0.9152 - pose_output_loss: 0.9064 - emotion_output_loss: 0.9124 - gender_output_acc: 0.6140 - image_quality_output_acc: 0.5512 - age_output_acc: 0.3972 - weight_output_acc: 0.6345 - bag_output_acc: 0.5632 - footwear_output_acc: 0.5782 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7099 - val_loss: 7.6084 - val_gender_output_loss: 0.6383 - val_image_quality_output_loss: 0.9755 - val_age_output_loss: 1.4081 - val_weight_output_loss: 0.9798 - val_bag_output_loss: 0.9036 - val_footwear_output_loss: 0.9089 - val_pose_output_loss: 0.9010 - val_emotion_output_loss: 0.8931 - val_gender_output_acc: 0.6101 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5610 - val_footwear_output_acc: 0.5711 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 7.60220\n",
            "Epoch 57/100\n",
            "339/339 [==============================] - 194s 574ms/step - loss: 7.6489 - gender_output_loss: 0.6416 - image_quality_output_loss: 0.9786 - age_output_loss: 1.4068 - weight_output_loss: 0.9805 - bag_output_loss: 0.9039 - footwear_output_loss: 0.9121 - pose_output_loss: 0.9091 - emotion_output_loss: 0.9164 - gender_output_acc: 0.6152 - image_quality_output_acc: 0.5520 - age_output_acc: 0.3971 - weight_output_acc: 0.6351 - bag_output_acc: 0.5654 - footwear_output_acc: 0.5832 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7099 - val_loss: 7.5891 - val_gender_output_loss: 0.6387 - val_image_quality_output_loss: 0.9717 - val_age_output_loss: 1.4077 - val_weight_output_loss: 0.9794 - val_bag_output_loss: 0.8972 - val_footwear_output_loss: 0.9020 - val_pose_output_loss: 0.9025 - val_emotion_output_loss: 0.8900 - val_gender_output_acc: 0.6112 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3951 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5666 - val_footwear_output_acc: 0.5837 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00057: val_loss improved from 7.60220 to 7.58914, saving model to gdrive/My Drive/saved_models/model-057-7.5891.hdf5\n",
            "Epoch 58/100\n",
            "339/339 [==============================] - 194s 571ms/step - loss: 7.6317 - gender_output_loss: 0.6414 - image_quality_output_loss: 0.9784 - age_output_loss: 1.4059 - weight_output_loss: 0.9773 - bag_output_loss: 0.9024 - footwear_output_loss: 0.9065 - pose_output_loss: 0.9077 - emotion_output_loss: 0.9120 - gender_output_acc: 0.6192 - image_quality_output_acc: 0.5513 - age_output_acc: 0.4007 - weight_output_acc: 0.6355 - bag_output_acc: 0.5676 - footwear_output_acc: 0.5840 - pose_output_acc: 0.6175 - emotion_output_acc: 0.7101 - val_loss: 7.5948 - val_gender_output_loss: 0.6393 - val_image_quality_output_loss: 0.9757 - val_age_output_loss: 1.4070 - val_weight_output_loss: 0.9784 - val_bag_output_loss: 0.9023 - val_footwear_output_loss: 0.9096 - val_pose_output_loss: 0.8975 - val_emotion_output_loss: 0.8848 - val_gender_output_acc: 0.6231 - val_image_quality_output_acc: 0.5506 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5573 - val_footwear_output_acc: 0.5744 - val_pose_output_acc: 0.6150 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 7.58914\n",
            "Epoch 59/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.6198 - gender_output_loss: 0.6363 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4071 - weight_output_loss: 0.9774 - bag_output_loss: 0.9016 - footwear_output_loss: 0.9039 - pose_output_loss: 0.9026 - emotion_output_loss: 0.9146 - gender_output_acc: 0.6256 - image_quality_output_acc: 0.5535 - age_output_acc: 0.3952 - weight_output_acc: 0.6349 - bag_output_acc: 0.5683 - footwear_output_acc: 0.5843 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7107\n",
            "Epoch 00058: val_loss did not improve from 7.58914\n",
            "Epoch 59/100\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.6216 - gender_output_loss: 0.6366 - image_quality_output_loss: 0.9772 - age_output_loss: 1.4073 - weight_output_loss: 0.9768 - bag_output_loss: 0.9012 - footwear_output_loss: 0.9042 - pose_output_loss: 0.9026 - emotion_output_loss: 0.9157 - gender_output_acc: 0.6256 - image_quality_output_acc: 0.5526 - age_output_acc: 0.3952 - weight_output_acc: 0.6353 - bag_output_acc: 0.5688 - footwear_output_acc: 0.5841 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7101 - val_loss: 7.5638 - val_gender_output_loss: 0.6337 - val_image_quality_output_loss: 0.9693 - val_age_output_loss: 1.4072 - val_weight_output_loss: 0.9738 - val_bag_output_loss: 0.8961 - val_footwear_output_loss: 0.9030 - val_pose_output_loss: 0.8951 - val_emotion_output_loss: 0.8855 - val_gender_output_acc: 0.6254 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6369 - val_bag_output_acc: 0.5751 - val_footwear_output_acc: 0.5841 - val_pose_output_acc: 0.6153 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00059: val_loss improved from 7.58914 to 7.56375, saving model to gdrive/My Drive/saved_models/model-059-7.5638.hdf5\n",
            "Epoch 60/100\n",
            "339/339 [==============================] - 194s 573ms/step - loss: 7.6254 - gender_output_loss: 0.6361 - image_quality_output_loss: 0.9775 - age_output_loss: 1.4052 - weight_output_loss: 0.9755 - bag_output_loss: 0.9015 - footwear_output_loss: 0.9110 - pose_output_loss: 0.9046 - emotion_output_loss: 0.9140 - gender_output_acc: 0.6281 - image_quality_output_acc: 0.5504 - age_output_acc: 0.3970 - weight_output_acc: 0.6350 - bag_output_acc: 0.5709 - footwear_output_acc: 0.5828 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7097 - val_loss: 7.6131 - val_gender_output_loss: 0.6353 - val_image_quality_output_loss: 0.9731 - val_age_output_loss: 1.4063 - val_weight_output_loss: 0.9836 - val_bag_output_loss: 0.9040 - val_footwear_output_loss: 0.9073 - val_pose_output_loss: 0.9008 - val_emotion_output_loss: 0.9025 - val_gender_output_acc: 0.6295 - val_image_quality_output_acc: 0.5577 - val_age_output_acc: 0.3932 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5599 - val_footwear_output_acc: 0.5792 - val_pose_output_acc: 0.6142 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 7.56375\n",
            "Epoch 61/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.6236 - gender_output_loss: 0.6382 - image_quality_output_loss: 0.9771 - age_output_loss: 1.4052 - weight_output_loss: 0.9758 - bag_output_loss: 0.9016 - footwear_output_loss: 0.9092 - pose_output_loss: 0.9032 - emotion_output_loss: 0.9134 - gender_output_acc: 0.6251 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3964 - weight_output_acc: 0.6362 - bag_output_acc: 0.5697 - footwear_output_acc: 0.5798 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7101 - val_loss: 7.5585 - val_gender_output_loss: 0.6311 - val_image_quality_output_loss: 0.9704 - val_age_output_loss: 1.4052 - val_weight_output_loss: 0.9744 - val_bag_output_loss: 0.8992 - val_footwear_output_loss: 0.9025 - val_pose_output_loss: 0.8889 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.6306 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.3999 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5610 - val_footwear_output_acc: 0.5774 - val_pose_output_acc: 0.6138 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00061: val_loss improved from 7.56375 to 7.55852, saving model to gdrive/My Drive/saved_models/model-061-7.5585.hdf5\n",
            "Epoch 62/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.6178 - gender_output_loss: 0.6394 - image_quality_output_loss: 0.9774 - age_output_loss: 1.4047 - weight_output_loss: 0.9802 - bag_output_loss: 0.8992 - footwear_output_loss: 0.8994 - pose_output_loss: 0.9030 - emotion_output_loss: 0.9144 - gender_output_acc: 0.6187 - image_quality_output_acc: 0.5517 - age_output_acc: 0.4035 - weight_output_acc: 0.6349 - bag_output_acc: 0.5713 - footwear_output_acc: 0.5855 - pose_output_acc: 0.6177 - emotion_output_acc: 0.7097 - val_loss: 7.5639 - val_gender_output_loss: 0.6284 - val_image_quality_output_loss: 0.9714 - val_age_output_loss: 1.4014 - val_weight_output_loss: 0.9769 - val_bag_output_loss: 0.9032 - val_footwear_output_loss: 0.9002 - val_pose_output_loss: 0.8916 - val_emotion_output_loss: 0.8908 - val_gender_output_acc: 0.6298 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.3955 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.5818 - val_pose_output_acc: 0.6157 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00062: val_loss did not improve from 7.55852\n",
            "Epoch 63/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.5999 - gender_output_loss: 0.6325 - image_quality_output_loss: 0.9762 - age_output_loss: 1.4003 - weight_output_loss: 0.9754 - bag_output_loss: 0.9027 - footwear_output_loss: 0.9023 - pose_output_loss: 0.8959 - emotion_output_loss: 0.9146 - gender_output_acc: 0.6326 - image_quality_output_acc: 0.5503 - age_output_acc: 0.3973 - weight_output_acc: 0.6353 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5823 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7094\n",
            "Epoch 00062: val_loss did not improve from 7.55852\n",
            "Epoch 63/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.5989 - gender_output_loss: 0.6325 - image_quality_output_loss: 0.9765 - age_output_loss: 1.4001 - weight_output_loss: 0.9750 - bag_output_loss: 0.9021 - footwear_output_loss: 0.9027 - pose_output_loss: 0.8960 - emotion_output_loss: 0.9140 - gender_output_acc: 0.6327 - image_quality_output_acc: 0.5500 - age_output_acc: 0.3974 - weight_output_acc: 0.6352 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5821 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7098 - val_loss: 7.5519 - val_gender_output_loss: 0.6259 - val_image_quality_output_loss: 0.9730 - val_age_output_loss: 1.4059 - val_weight_output_loss: 0.9731 - val_bag_output_loss: 0.8979 - val_footwear_output_loss: 0.9046 - val_pose_output_loss: 0.8848 - val_emotion_output_loss: 0.8867 - val_gender_output_acc: 0.6373 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5703 - val_footwear_output_acc: 0.5818 - val_pose_output_acc: 0.6172 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00063: val_loss improved from 7.55852 to 7.55187, saving model to gdrive/My Drive/saved_models/model-063-7.5519.hdf5\n",
            "Epoch 64/100\n",
            "339/339 [==============================] - 194s 573ms/step - loss: 7.5988 - gender_output_loss: 0.6348 - image_quality_output_loss: 0.9758 - age_output_loss: 1.4026 - weight_output_loss: 0.9776 - bag_output_loss: 0.8976 - footwear_output_loss: 0.8998 - pose_output_loss: 0.9000 - emotion_output_loss: 0.9105 - gender_output_acc: 0.6250 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3960 - weight_output_acc: 0.6354 - bag_output_acc: 0.5699 - footwear_output_acc: 0.5838 - pose_output_acc: 0.6172 - emotion_output_acc: 0.7099 - val_loss: 7.5483 - val_gender_output_loss: 0.6273 - val_image_quality_output_loss: 0.9694 - val_age_output_loss: 1.4054 - val_weight_output_loss: 0.9742 - val_bag_output_loss: 0.8983 - val_footwear_output_loss: 0.9036 - val_pose_output_loss: 0.8853 - val_emotion_output_loss: 0.8849 - val_gender_output_acc: 0.6440 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3984 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5621 - val_footwear_output_acc: 0.5815 - val_pose_output_acc: 0.6157 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00064: val_loss improved from 7.55187 to 7.54830, saving model to gdrive/My Drive/saved_models/model-064-7.5483.hdf5\n",
            "Epoch 65/100\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.5831 - gender_output_loss: 0.6278 - image_quality_output_loss: 0.9752 - age_output_loss: 1.3998 - weight_output_loss: 0.9747 - bag_output_loss: 0.8964 - footwear_output_loss: 0.9042 - pose_output_loss: 0.8960 - emotion_output_loss: 0.9090 - gender_output_acc: 0.6386 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3961 - weight_output_acc: 0.6352 - bag_output_acc: 0.5724 - footwear_output_acc: 0.5872 - pose_output_acc: 0.6175 - emotion_output_acc: 0.7101 - val_loss: 7.5368 - val_gender_output_loss: 0.6222 - val_image_quality_output_loss: 0.9715 - val_age_output_loss: 1.4011 - val_weight_output_loss: 0.9742 - val_bag_output_loss: 0.8951 - val_footwear_output_loss: 0.8982 - val_pose_output_loss: 0.8858 - val_emotion_output_loss: 0.8888 - val_gender_output_acc: 0.6399 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6354 - val_bag_output_acc: 0.5681 - val_footwear_output_acc: 0.5837 - val_pose_output_acc: 0.6190 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00065: val_loss improved from 7.54830 to 7.53680, saving model to gdrive/My Drive/saved_models/model-065-7.5368.hdf5\n",
            "Epoch 66/100\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.5748 - gender_output_loss: 0.6268 - image_quality_output_loss: 0.9750 - age_output_loss: 1.4004 - weight_output_loss: 0.9744 - bag_output_loss: 0.8967 - footwear_output_loss: 0.8981 - pose_output_loss: 0.8936 - emotion_output_loss: 0.9099 - gender_output_acc: 0.6366 - image_quality_output_acc: 0.5506 - age_output_acc: 0.3947 - weight_output_acc: 0.6344 - bag_output_acc: 0.5741 - footwear_output_acc: 0.5875 - pose_output_acc: 0.6194 - emotion_output_acc: 0.7096 - val_loss: 7.5443 - val_gender_output_loss: 0.6218 - val_image_quality_output_loss: 0.9700 - val_age_output_loss: 1.4049 - val_weight_output_loss: 0.9776 - val_bag_output_loss: 0.8972 - val_footwear_output_loss: 0.8988 - val_pose_output_loss: 0.8824 - val_emotion_output_loss: 0.8916 - val_gender_output_acc: 0.6484 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5711 - val_footwear_output_acc: 0.5774 - val_pose_output_acc: 0.6168 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 7.53680\n",
            "Epoch 67/100\n",
            "339/339 [==============================] - 196s 578ms/step - loss: 7.5814 - gender_output_loss: 0.6323 - image_quality_output_loss: 0.9748 - age_output_loss: 1.4029 - weight_output_loss: 0.9764 - bag_output_loss: 0.8978 - footwear_output_loss: 0.8970 - pose_output_loss: 0.8927 - emotion_output_loss: 0.9073 - gender_output_acc: 0.6256 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4001 - weight_output_acc: 0.6345 - bag_output_acc: 0.5705 - footwear_output_acc: 0.5904 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7100 - val_loss: 7.5312 - val_gender_output_loss: 0.6200 - val_image_quality_output_loss: 0.9693 - val_age_output_loss: 1.4007 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.8972 - val_footwear_output_loss: 0.8916 - val_pose_output_loss: 0.8785 - val_emotion_output_loss: 0.8959 - val_gender_output_acc: 0.6466 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5733 - val_footwear_output_acc: 0.5882 - val_pose_output_acc: 0.6187 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00067: val_loss improved from 7.53680 to 7.53124, saving model to gdrive/My Drive/saved_models/model-067-7.5312.hdf5\n",
            "Epoch 68/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.5647 - gender_output_loss: 0.6289 - image_quality_output_loss: 0.9749 - age_output_loss: 1.4005 - weight_output_loss: 0.9740 - bag_output_loss: 0.8946 - footwear_output_loss: 0.8942 - pose_output_loss: 0.8885 - emotion_output_loss: 0.9092 - gender_output_acc: 0.6363 - image_quality_output_acc: 0.5526 - age_output_acc: 0.3959 - weight_output_acc: 0.6351 - bag_output_acc: 0.5741 - footwear_output_acc: 0.5902 - pose_output_acc: 0.6197 - emotion_output_acc: 0.7103\n",
            "Epoch 00067: val_loss improved from 7.53680 to 7.53124, saving model to gdrive/My Drive/saved_models/model-067-7.5312.hdf5\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.5668 - gender_output_loss: 0.6291 - image_quality_output_loss: 0.9751 - age_output_loss: 1.4005 - weight_output_loss: 0.9742 - bag_output_loss: 0.8952 - footwear_output_loss: 0.8940 - pose_output_loss: 0.8886 - emotion_output_loss: 0.9101 - gender_output_acc: 0.6359 - image_quality_output_acc: 0.5524 - age_output_acc: 0.3959 - weight_output_acc: 0.6353 - bag_output_acc: 0.5738 - footwear_output_acc: 0.5904 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7099 - val_loss: 7.5235 - val_gender_output_loss: 0.6183 - val_image_quality_output_loss: 0.9748 - val_age_output_loss: 1.4040 - val_weight_output_loss: 0.9750 - val_bag_output_loss: 0.8939 - val_footwear_output_loss: 0.8934 - val_pose_output_loss: 0.8797 - val_emotion_output_loss: 0.8844 - val_gender_output_acc: 0.6417 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.3929 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5759 - val_footwear_output_acc: 0.5885 - val_pose_output_acc: 0.6239 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00068: val_loss improved from 7.53124 to 7.52347, saving model to gdrive/My Drive/saved_models/model-068-7.5235.hdf5\n",
            "Epoch 69/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.5565 - gender_output_loss: 0.6212 - image_quality_output_loss: 0.9709 - age_output_loss: 1.4004 - weight_output_loss: 0.9757 - bag_output_loss: 0.8973 - footwear_output_loss: 0.8956 - pose_output_loss: 0.8866 - emotion_output_loss: 0.9089 - gender_output_acc: 0.6494 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4022 - weight_output_acc: 0.6353 - bag_output_acc: 0.5711 - footwear_output_acc: 0.5894 - pose_output_acc: 0.6207 - emotion_output_acc: 0.7101\n",
            "Epoch 00068: val_loss improved from 7.53124 to 7.52347, saving model to gdrive/My Drive/saved_models/model-068-7.5235.hdf5\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.5560 - gender_output_loss: 0.6211 - image_quality_output_loss: 0.9709 - age_output_loss: 1.3999 - weight_output_loss: 0.9762 - bag_output_loss: 0.8972 - footwear_output_loss: 0.8953 - pose_output_loss: 0.8863 - emotion_output_loss: 0.9090 - gender_output_acc: 0.6494 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4019 - weight_output_acc: 0.6350 - bag_output_acc: 0.5711 - footwear_output_acc: 0.5897 - pose_output_acc: 0.6209 - emotion_output_acc: 0.7100 - val_loss: 7.5157 - val_gender_output_loss: 0.6214 - val_image_quality_output_loss: 0.9700 - val_age_output_loss: 1.4035 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.8950 - val_footwear_output_loss: 0.8954 - val_pose_output_loss: 0.8662 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.6388 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5677 - val_footwear_output_acc: 0.5859 - val_pose_output_acc: 0.6231 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00069: val_loss improved from 7.52347 to 7.51568, saving model to gdrive/My Drive/saved_models/model-069-7.5157.hdf5\n",
            "Epoch 70/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.5438 - gender_output_loss: 0.6231 - image_quality_output_loss: 0.9739 - age_output_loss: 1.3968 - weight_output_loss: 0.9735 - bag_output_loss: 0.8936 - footwear_output_loss: 0.8929 - pose_output_loss: 0.8809 - emotion_output_loss: 0.9091 - gender_output_acc: 0.6437 - image_quality_output_acc: 0.5535 - age_output_acc: 0.3983 - weight_output_acc: 0.6358 - bag_output_acc: 0.5732 - footwear_output_acc: 0.5881 - pose_output_acc: 0.6199 - emotion_output_acc: 0.7102\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.5560 - gender_output_loss: 0.6211 - image_quality_output_loss: 0.9709 - age_output_loss: 1.3999 - weight_output_loss: 0.9762 - bag_output_loss: 0.8972 - footwear_output_loss: 0.8953 - pose_output_loss: 0.8863 - emotion_output_loss: 0.9090 - gender_output_acc: 0.6494 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4019 - weight_output_acc: 0.6350 - bag_output_acc: 0.5711 - footwear_output_acc: 0.5897 - pose_output_acc: 0.6209 - emotion_output_acc: 0.7100 - val_loss: 7.5157 - val_gender_output_loss: 0.6214 - val_image_quality_output_loss: 0.9700 - val_age_output_loss: 1.4035 - val_weight_output_loss: 0.9774 - val_bag_output_loss: 0.8950 - val_footwear_output_loss: 0.8954 - val_pose_output_loss: 0.8662 - val_emotion_output_loss: 0.8868 - val_gender_output_acc: 0.6388 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5677 - val_footwear_output_acc: 0.5859 - val_pose_output_acc: 0.6231 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.5445 - gender_output_loss: 0.6233 - image_quality_output_loss: 0.9741 - age_output_loss: 1.3971 - weight_output_loss: 0.9738 - bag_output_loss: 0.8935 - footwear_output_loss: 0.8927 - pose_output_loss: 0.8810 - emotion_output_loss: 0.9091 - gender_output_acc: 0.6437 - image_quality_output_acc: 0.5534 - age_output_acc: 0.3980 - weight_output_acc: 0.6356 - bag_output_acc: 0.5733 - footwear_output_acc: 0.5881 - pose_output_acc: 0.6198 - emotion_output_acc: 0.7102 - val_loss: 7.5018 - val_gender_output_loss: 0.6179 - val_image_quality_output_loss: 0.9696 - val_age_output_loss: 1.4003 - val_weight_output_loss: 0.9755 - val_bag_output_loss: 0.8941 - val_footwear_output_loss: 0.8889 - val_pose_output_loss: 0.8661 - val_emotion_output_loss: 0.8894 - val_gender_output_acc: 0.6458 - val_image_quality_output_acc: 0.5577 - val_age_output_acc: 0.3969 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5800 - val_footwear_output_acc: 0.5897 - val_pose_output_acc: 0.6209 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00070: val_loss improved from 7.51568 to 7.50181, saving model to gdrive/My Drive/saved_models/model-070-7.5018.hdf5\n",
            "Epoch 71/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.5307 - gender_output_loss: 0.6211 - image_quality_output_loss: 0.9718 - age_output_loss: 1.3985 - weight_output_loss: 0.9724 - bag_output_loss: 0.8937 - footwear_output_loss: 0.8900 - pose_output_loss: 0.8797 - emotion_output_loss: 0.9035 - gender_output_acc: 0.6476 - image_quality_output_acc: 0.5501 - age_output_acc: 0.3992 - weight_output_acc: 0.6358 - bag_output_acc: 0.5737 - footwear_output_acc: 0.5931 - pose_output_acc: 0.6222 - emotion_output_acc: 0.7103\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.5308 - gender_output_loss: 0.6209 - image_quality_output_loss: 0.9718 - age_output_loss: 1.3989 - weight_output_loss: 0.9724 - bag_output_loss: 0.8933 - footwear_output_loss: 0.8902 - pose_output_loss: 0.8795 - emotion_output_loss: 0.9039 - gender_output_acc: 0.6479 - image_quality_output_acc: 0.5501 - age_output_acc: 0.3991 - weight_output_acc: 0.6360 - bag_output_acc: 0.5737 - footwear_output_acc: 0.5929 - pose_output_acc: 0.6223 - emotion_output_acc: 0.7101 - val_loss: 7.5067 - val_gender_output_loss: 0.6186 - val_image_quality_output_loss: 0.9669 - val_age_output_loss: 1.3990 - val_weight_output_loss: 0.9738 - val_bag_output_loss: 0.8956 - val_footwear_output_loss: 0.8966 - val_pose_output_loss: 0.8657 - val_emotion_output_loss: 0.8906 - val_gender_output_acc: 0.6488 - val_image_quality_output_acc: 0.5577 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5722 - val_footwear_output_acc: 0.5822 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 7.50181\n",
            "Epoch 72/100\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.5246 - gender_output_loss: 0.6204 - image_quality_output_loss: 0.9721 - age_output_loss: 1.3951 - weight_output_loss: 0.9726 - bag_output_loss: 0.8926 - footwear_output_loss: 0.8843 - pose_output_loss: 0.8774 - emotion_output_loss: 0.9100 - gender_output_acc: 0.6509 - image_quality_output_acc: 0.5508 - age_output_acc: 0.3980 - weight_output_acc: 0.6356 - bag_output_acc: 0.5728 - footwear_output_acc: 0.5989 - pose_output_acc: 0.6254 - emotion_output_acc: 0.7101 - val_loss: 7.4732 - val_gender_output_loss: 0.6136 - val_image_quality_output_loss: 0.9670 - val_age_output_loss: 1.3989 - val_weight_output_loss: 0.9711 - val_bag_output_loss: 0.8897 - val_footwear_output_loss: 0.8868 - val_pose_output_loss: 0.8615 - val_emotion_output_loss: 0.8846 - val_gender_output_acc: 0.6566 - val_image_quality_output_acc: 0.5558 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5826 - val_footwear_output_acc: 0.5837 - val_pose_output_acc: 0.6283 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00072: val_loss improved from 7.50181 to 7.47321, saving model to gdrive/My Drive/saved_models/model-072-7.4732.hdf5\n",
            "Epoch 73/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.5214 - gender_output_loss: 0.6167 - image_quality_output_loss: 0.9742 - age_output_loss: 1.3931 - weight_output_loss: 0.9736 - bag_output_loss: 0.8924 - footwear_output_loss: 0.8920 - pose_output_loss: 0.8724 - emotion_output_loss: 0.9069 - gender_output_acc: 0.6492 - image_quality_output_acc: 0.5532 - age_output_acc: 0.4015 - weight_output_acc: 0.6355 - bag_output_acc: 0.5755 - footwear_output_acc: 0.5894 - pose_output_acc: 0.6234 - emotion_output_acc: 0.7100 - val_loss: 7.4721 - val_gender_output_loss: 0.6152 - val_image_quality_output_loss: 0.9685 - val_age_output_loss: 1.3955 - val_weight_output_loss: 0.9733 - val_bag_output_loss: 0.8926 - val_footwear_output_loss: 0.8898 - val_pose_output_loss: 0.8516 - val_emotion_output_loss: 0.8856 - val_gender_output_acc: 0.6525 - val_image_quality_output_acc: 0.5577 - val_age_output_acc: 0.4003 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5733 - val_footwear_output_acc: 0.5818 - val_pose_output_acc: 0.6336 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00073: val_loss improved from 7.47321 to 7.47209, saving model to gdrive/My Drive/saved_models/model-073-7.4721.hdf5\n",
            "\n",
            "Epoch 00072: val_loss improved from 7.50181 to 7.47321, saving model to gdrive/My Drive/saved_models/model-072-7.4732.hdf5\n",
            "Epoch 74/100\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.5017 - gender_output_loss: 0.6172 - image_quality_output_loss: 0.9716 - age_output_loss: 1.3884 - weight_output_loss: 0.9704 - bag_output_loss: 0.8889 - footwear_output_loss: 0.8879 - pose_output_loss: 0.8711 - emotion_output_loss: 0.9062 - gender_output_acc: 0.6527 - image_quality_output_acc: 0.5524 - age_output_acc: 0.4027 - weight_output_acc: 0.6347 - bag_output_acc: 0.5743 - footwear_output_acc: 0.5895 - pose_output_acc: 0.6222 - emotion_output_acc: 0.7098 - val_loss: 7.4863 - val_gender_output_loss: 0.6156 - val_image_quality_output_loss: 0.9684 - val_age_output_loss: 1.3956 - val_weight_output_loss: 0.9740 - val_bag_output_loss: 0.8903 - val_footwear_output_loss: 0.8967 - val_pose_output_loss: 0.8573 - val_emotion_output_loss: 0.8884 - val_gender_output_acc: 0.6455 - val_image_quality_output_acc: 0.5536 - val_age_output_acc: 0.3999 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5770 - val_footwear_output_acc: 0.5818 - val_pose_output_acc: 0.6310 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 7.47209\n",
            "Epoch 75/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.5080 - gender_output_loss: 0.6165 - image_quality_output_loss: 0.9732 - age_output_loss: 1.3980 - weight_output_loss: 0.9715 - bag_output_loss: 0.8917 - footwear_output_loss: 0.8830 - pose_output_loss: 0.8682 - emotion_output_loss: 0.9060 - gender_output_acc: 0.6544 - image_quality_output_acc: 0.5521 - age_output_acc: 0.3976 - weight_output_acc: 0.6357 - bag_output_acc: 0.5762 - footwear_output_acc: 0.5904 - pose_output_acc: 0.6240 - emotion_output_acc: 0.7096\n",
            "339/339 [==============================] - 196s 578ms/step - loss: 7.5074 - gender_output_loss: 0.6164 - image_quality_output_loss: 0.9730 - age_output_loss: 1.3977 - weight_output_loss: 0.9717 - bag_output_loss: 0.8920 - footwear_output_loss: 0.8829 - pose_output_loss: 0.8683 - emotion_output_loss: 0.9054 - gender_output_acc: 0.6545 - image_quality_output_acc: 0.5524 - age_output_acc: 0.3980 - weight_output_acc: 0.6360 - bag_output_acc: 0.5759 - footwear_output_acc: 0.5904 - pose_output_acc: 0.6238 - emotion_output_acc: 0.7099 - val_loss: 7.4724 - val_gender_output_loss: 0.6100 - val_image_quality_output_loss: 0.9683 - val_age_output_loss: 1.3952 - val_weight_output_loss: 0.9727 - val_bag_output_loss: 0.8911 - val_footwear_output_loss: 0.8966 - val_pose_output_loss: 0.8501 - val_emotion_output_loss: 0.8883 - val_gender_output_acc: 0.6670 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5766 - val_footwear_output_acc: 0.5833 - val_pose_output_acc: 0.6313 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 7.47209\n",
            "Epoch 76/100\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.4828 - gender_output_loss: 0.6119 - image_quality_output_loss: 0.9712 - age_output_loss: 1.3925 - weight_output_loss: 0.9672 - bag_output_loss: 0.8867 - footwear_output_loss: 0.8798 - pose_output_loss: 0.8687 - emotion_output_loss: 0.9048 - gender_output_acc: 0.6551 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3994 - weight_output_acc: 0.6355 - bag_output_acc: 0.5792 - footwear_output_acc: 0.5972 - pose_output_acc: 0.6211 - emotion_output_acc: 0.7096 - val_loss: 7.4648 - val_gender_output_loss: 0.6112 - val_image_quality_output_loss: 0.9666 - val_age_output_loss: 1.4001 - val_weight_output_loss: 0.9708 - val_bag_output_loss: 0.8909 - val_footwear_output_loss: 0.8906 - val_pose_output_loss: 0.8491 - val_emotion_output_loss: 0.8856 - val_gender_output_acc: 0.6615 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3955 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5774 - val_footwear_output_acc: 0.5844 - val_pose_output_acc: 0.6324 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================]\n",
            "Epoch 00076: val_loss improved from 7.47209 to 7.46478, saving model to gdrive/My Drive/saved_models/model-076-7.4648.hdf5\n",
            "Epoch 77/100\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.4828 - gender_output_loss: 0.6119 - image_quality_output_loss: 0.9712 - age_output_loss: 1.3925 - weight_output_loss: 0.9672 - bag_output_loss: 0.8867 - footwear_output_loss: 0.8798 - pose_output_loss: 0.8687 - emotion_output_loss: 0.9048 - gender_output_acc: 0.6551 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3994 - weight_output_acc: 0.6355 - bag_output_acc: 0.5792 - footwear_output_acc: 0.5972 - pose_output_acc: 0.6211 - emotion_output_acc: 0.7096 - val_loss: 7.4648 - val_gender_output_loss: 0.6112 - val_image_quality_output_loss: 0.9666 - val_age_output_loss: 1.4001 - val_weight_output_loss: 0.9708 - val_bag_output_loss: 0.8909 - val_footwear_output_loss: 0.8906 - val_pose_output_loss: 0.8491 - val_emotion_output_loss: 0.8856 - val_gender_output_acc: 0.6615 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3955 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5774 - val_footwear_output_acc: 0.5844 - val_pose_output_acc: 0.6324 - val_emotion_output_acc: 0.7180\n",
            "339/339 [==============================] - 194s 574ms/step - loss: 7.4860 - gender_output_loss: 0.6097 - image_quality_output_loss: 0.9719 - age_output_loss: 1.3949 - weight_output_loss: 0.9719 - bag_output_loss: 0.8901 - footwear_output_loss: 0.8796 - pose_output_loss: 0.8645 - emotion_output_loss: 0.9034 - gender_output_acc: 0.6611 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3998 - weight_output_acc: 0.6352 - bag_output_acc: 0.5777 - footwear_output_acc: 0.6013 - pose_output_acc: 0.6241 - emotion_output_acc: 0.7099 - val_loss: 7.4749 - val_gender_output_loss: 0.6117 - val_image_quality_output_loss: 0.9667 - val_age_output_loss: 1.4044 - val_weight_output_loss: 0.9707 - val_bag_output_loss: 0.8935 - val_footwear_output_loss: 0.8929 - val_pose_output_loss: 0.8503 - val_emotion_output_loss: 0.8847 - val_gender_output_acc: 0.6581 - val_image_quality_output_acc: 0.5543 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5766 - val_footwear_output_acc: 0.5908 - val_pose_output_acc: 0.6365 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 7.46478\n",
            "Epoch 78/100\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.4901 - gender_output_loss: 0.6117 - image_quality_output_loss: 0.9716 - age_output_loss: 1.3928 - weight_output_loss: 0.9711 - bag_output_loss: 0.8859 - footwear_output_loss: 0.8859 - pose_output_loss: 0.8682 - emotion_output_loss: 0.9029 - gender_output_acc: 0.6545 - image_quality_output_acc: 0.5529 - age_output_acc: 0.3998 - weight_output_acc: 0.6359 - bag_output_acc: 0.5752 - footwear_output_acc: 0.5936 - pose_output_acc: 0.6232 - emotion_output_acc: 0.7097 - val_loss: 7.4508 - val_gender_output_loss: 0.6089 - val_image_quality_output_loss: 0.9660 - val_age_output_loss: 1.3947 - val_weight_output_loss: 0.9715 - val_bag_output_loss: 0.8920 - val_footwear_output_loss: 0.8893 - val_pose_output_loss: 0.8448 - val_emotion_output_loss: 0.8836 - val_gender_output_acc: 0.6603 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5789 - val_footwear_output_acc: 0.5885 - val_pose_output_acc: 0.6347 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00078: val_loss improved from 7.46478 to 7.45080, saving model to gdrive/My Drive/saved_models/model-078-7.4508.hdf5\n",
            "Epoch 79/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.4749 - gender_output_loss: 0.6081 - image_quality_output_loss: 0.9718 - age_output_loss: 1.3930 - weight_output_loss: 0.9677 - bag_output_loss: 0.8876 - footwear_output_loss: 0.8798 - pose_output_loss: 0.8617 - emotion_output_loss: 0.9051 - gender_output_acc: 0.6609 - image_quality_output_acc: 0.5512 - age_output_acc: 0.3992 - weight_output_acc: 0.6369 - bag_output_acc: 0.5753 - footwear_output_acc: 0.5991 - pose_output_acc: 0.6251 - emotion_output_acc: 0.7098\n",
            "339/339 [==============================] - 195s 574ms/step - loss: 7.4743 - gender_output_loss: 0.6081 - image_quality_output_loss: 0.9713 - age_output_loss: 1.3926 - weight_output_loss: 0.9681 - bag_output_loss: 0.8876 - footwear_output_loss: 0.8799 - pose_output_loss: 0.8616 - emotion_output_loss: 0.9050 - gender_output_acc: 0.6610 - image_quality_output_acc: 0.5517 - age_output_acc: 0.3991 - weight_output_acc: 0.6364 - bag_output_acc: 0.5752 - footwear_output_acc: 0.5993 - pose_output_acc: 0.6252 - emotion_output_acc: 0.7098 - val_loss: 7.4599 - val_gender_output_loss: 0.6087 - val_image_quality_output_loss: 0.9652 - val_age_output_loss: 1.3964 - val_weight_output_loss: 0.9686 - val_bag_output_loss: 0.8942 - val_footwear_output_loss: 0.8980 - val_pose_output_loss: 0.8406 - val_emotion_output_loss: 0.8881 - val_gender_output_acc: 0.6615 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3955 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5737 - val_footwear_output_acc: 0.5837 - val_pose_output_acc: 0.6313 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 7.45080\n",
            "Epoch 80/100\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.4788 - gender_output_loss: 0.6093 - image_quality_output_loss: 0.9698 - age_output_loss: 1.3907 - weight_output_loss: 0.9714 - bag_output_loss: 0.8880 - footwear_output_loss: 0.8794 - pose_output_loss: 0.8665 - emotion_output_loss: 0.9038 - gender_output_acc: 0.6629 - image_quality_output_acc: 0.5518 - age_output_acc: 0.3971 - weight_output_acc: 0.6357 - bag_output_acc: 0.5764 - footwear_output_acc: 0.5996 - pose_output_acc: 0.6288 - emotion_output_acc: 0.7100 - val_loss: 7.4307 - val_gender_output_loss: 0.6044 - val_image_quality_output_loss: 0.9638 - val_age_output_loss: 1.3951 - val_weight_output_loss: 0.9697 - val_bag_output_loss: 0.8839 - val_footwear_output_loss: 0.8843 - val_pose_output_loss: 0.8407 - val_emotion_output_loss: 0.8888 - val_gender_output_acc: 0.6741 - val_image_quality_output_acc: 0.5577 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5826 - val_footwear_output_acc: 0.5885 - val_pose_output_acc: 0.6440 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00080: val_loss improved from 7.45080 to 7.43067, saving model to gdrive/My Drive/saved_models/model-080-7.4307.hdf5\n",
            "Epoch 81/100\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.4745 - gender_output_loss: 0.6068 - image_quality_output_loss: 0.9690 - age_output_loss: 1.3948 - weight_output_loss: 0.9678 - bag_output_loss: 0.8869 - footwear_output_loss: 0.8828 - pose_output_loss: 0.8602 - emotion_output_loss: 0.9061 - gender_output_acc: 0.6700 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3999 - weight_output_acc: 0.6351 - bag_output_acc: 0.5799 - footwear_output_acc: 0.5939 - pose_output_acc: 0.6256 - emotion_output_acc: 0.7100 - val_loss: 7.4711 - val_gender_output_loss: 0.6154 - val_image_quality_output_loss: 0.9727 - val_age_output_loss: 1.4028 - val_weight_output_loss: 0.9693 - val_bag_output_loss: 0.8936 - val_footwear_output_loss: 0.8920 - val_pose_output_loss: 0.8400 - val_emotion_output_loss: 0.8852 - val_gender_output_acc: 0.6503 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3969 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5729 - val_footwear_output_acc: 0.5964 - val_pose_output_acc: 0.6425 - val_emotion_output_acc: 0.7188\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.4788 - gender_output_loss: 0.6093 - image_quality_output_loss: 0.9698 - age_output_loss: 1.3907 - weight_output_loss: 0.9714 - bag_output_loss: 0.8880 - footwear_output_loss: 0.8794 - pose_output_loss: 0.8665 - emotion_output_loss: 0.9038 - gender_output_acc: 0.6629 - image_quality_output_acc: 0.5518 - age_output_acc: 0.3971 - weight_output_acc: 0.6357 - bag_output_acc: 0.5764 - footwear_output_acc: 0.5996 - pose_output_acc: 0.6288 - emotion_output_acc: 0.7100 - val_loss: 7.4307 - val_gender_output_loss: 0.6044 - val_image_quality_output_loss: 0.9638 - val_age_output_loss: 1.3951 - val_weight_output_loss: 0.9697 - val_bag_output_loss: 0.8839 - val_footwear_output_loss: 0.8843 - val_pose_output_loss: 0.8407 - val_emotion_output_loss: 0.8888 - val_gender_output_acc: 0.6741 - val_image_quality_output_acc: 0.5577 - val_age_output_acc: 0.3958 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5826 - val_footwear_output_acc: 0.5885 - val_pose_output_acc: 0.6440 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 7.43067\n",
            "Epoch 82/100\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.4747 - gender_output_loss: 0.6060 - image_quality_output_loss: 0.9723 - age_output_loss: 1.3940 - weight_output_loss: 0.9668 - bag_output_loss: 0.8866 - footwear_output_loss: 0.8804 - pose_output_loss: 0.8604 - emotion_output_loss: 0.9082 - gender_output_acc: 0.6626 - image_quality_output_acc: 0.5516 - age_output_acc: 0.3993 - weight_output_acc: 0.6356 - bag_output_acc: 0.5766 - footwear_output_acc: 0.5945 - pose_output_acc: 0.6286 - emotion_output_acc: 0.7099 - val_loss: 7.4471 - val_gender_output_loss: 0.6083 - val_image_quality_output_loss: 0.9704 - val_age_output_loss: 1.3955 - val_weight_output_loss: 0.9729 - val_bag_output_loss: 0.8832 - val_footwear_output_loss: 0.8910 - val_pose_output_loss: 0.8382 - val_emotion_output_loss: 0.8874 - val_gender_output_acc: 0.6525 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5830 - val_footwear_output_acc: 0.5848 - val_pose_output_acc: 0.6417 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 7.43067\n",
            "Epoch 83/100\n",
            "339/339 [==============================] - 196s 578ms/step - loss: 7.4563 - gender_output_loss: 0.6048 - image_quality_output_loss: 0.9709 - age_output_loss: 1.3928 - weight_output_loss: 0.9682 - bag_output_loss: 0.8871 - footwear_output_loss: 0.8737 - pose_output_loss: 0.8549 - emotion_output_loss: 0.9039 - gender_output_acc: 0.6596 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3988 - weight_output_acc: 0.6353 - bag_output_acc: 0.5762 - footwear_output_acc: 0.5980 - pose_output_acc: 0.6303 - emotion_output_acc: 0.7098 - val_loss: 7.4383 - val_gender_output_loss: 0.6029 - val_image_quality_output_loss: 0.9717 - val_age_output_loss: 1.3971 - val_weight_output_loss: 0.9758 - val_bag_output_loss: 0.8862 - val_footwear_output_loss: 0.8880 - val_pose_output_loss: 0.8288 - val_emotion_output_loss: 0.8879 - val_gender_output_acc: 0.6618 - val_image_quality_output_acc: 0.5595 - val_age_output_acc: 0.3969 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5718 - val_footwear_output_acc: 0.5878 - val_pose_output_acc: 0.6510 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 7.43067\n",
            "Epoch 84/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.4537 - gender_output_loss: 0.6007 - image_quality_output_loss: 0.9684 - age_output_loss: 1.3920 - weight_output_loss: 0.9695 - bag_output_loss: 0.8878 - footwear_output_loss: 0.8796 - pose_output_loss: 0.8512 - emotion_output_loss: 0.9045 - gender_output_acc: 0.6683 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3980 - weight_output_acc: 0.6341 - bag_output_acc: 0.5822 - footwear_output_acc: 0.5958 - pose_output_acc: 0.6345 - emotion_output_acc: 0.7095\n",
            "Epoch 00083: val_loss did not improve from 7.43067\n",
            "Epoch 84/100\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.4530 - gender_output_loss: 0.6005 - image_quality_output_loss: 0.9681 - age_output_loss: 1.3920 - weight_output_loss: 0.9695 - bag_output_loss: 0.8876 - footwear_output_loss: 0.8799 - pose_output_loss: 0.8511 - emotion_output_loss: 0.9043 - gender_output_acc: 0.6684 - image_quality_output_acc: 0.5530 - age_output_acc: 0.3981 - weight_output_acc: 0.6342 - bag_output_acc: 0.5823 - footwear_output_acc: 0.5954 - pose_output_acc: 0.6344 - emotion_output_acc: 0.7097 - val_loss: 7.4481 - val_gender_output_loss: 0.6147 - val_image_quality_output_loss: 0.9662 - val_age_output_loss: 1.3976 - val_weight_output_loss: 0.9700 - val_bag_output_loss: 0.8880 - val_footwear_output_loss: 0.8807 - val_pose_output_loss: 0.8484 - val_emotion_output_loss: 0.8826 - val_gender_output_acc: 0.6581 - val_image_quality_output_acc: 0.5547 - val_age_output_acc: 0.3943 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5878 - val_footwear_output_acc: 0.5938 - val_pose_output_acc: 0.6417 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 7.43067\n",
            "Epoch 85/100\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.4470 - gender_output_loss: 0.6036 - image_quality_output_loss: 0.9708 - age_output_loss: 1.3903 - weight_output_loss: 0.9665 - bag_output_loss: 0.8864 - footwear_output_loss: 0.8818 - pose_output_loss: 0.8456 - emotion_output_loss: 0.9021 - gender_output_acc: 0.6704 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4010 - weight_output_acc: 0.6355 - bag_output_acc: 0.5828 - footwear_output_acc: 0.5982 - pose_output_acc: 0.6321 - emotion_output_acc: 0.7098 - val_loss: 7.3848 - val_gender_output_loss: 0.5994 - val_image_quality_output_loss: 0.9623 - val_age_output_loss: 1.3979 - val_weight_output_loss: 0.9681 - val_bag_output_loss: 0.8833 - val_footwear_output_loss: 0.8826 - val_pose_output_loss: 0.8104 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.6629 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5897 - val_footwear_output_acc: 0.5874 - val_pose_output_acc: 0.6529 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00085: val_loss improved from 7.43067 to 7.38480, saving model to gdrive/My Drive/saved_models/model-085-7.3848.hdf5\n",
            "Epoch 86/100\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.4352 - gender_output_loss: 0.5976 - image_quality_output_loss: 0.9719 - age_output_loss: 1.3926 - weight_output_loss: 0.9700 - bag_output_loss: 0.8812 - footwear_output_loss: 0.8750 - pose_output_loss: 0.8429 - emotion_output_loss: 0.9041 - gender_output_acc: 0.6735 - image_quality_output_acc: 0.5509 - age_output_acc: 0.3961 - weight_output_acc: 0.6347 - bag_output_acc: 0.5828 - footwear_output_acc: 0.6011 - pose_output_acc: 0.6319 - emotion_output_acc: 0.7099\n",
            "Epoch 00085: val_loss improved from 7.43067 to 7.38480, saving model to gdrive/My Drive/saved_models/model-085-7.3848.hdf5\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.4348 - gender_output_loss: 0.5973 - image_quality_output_loss: 0.9716 - age_output_loss: 1.3927 - weight_output_loss: 0.9699 - bag_output_loss: 0.8813 - footwear_output_loss: 0.8752 - pose_output_loss: 0.8430 - emotion_output_loss: 0.9037 - gender_output_acc: 0.6736 - image_quality_output_acc: 0.5513 - age_output_acc: 0.3959 - weight_output_acc: 0.6350 - bag_output_acc: 0.5827 - footwear_output_acc: 0.6010 - pose_output_acc: 0.6318 - emotion_output_acc: 0.7101 - val_loss: 7.4165 - val_gender_output_loss: 0.6072 - val_image_quality_output_loss: 0.9655 - val_age_output_loss: 1.3959 - val_weight_output_loss: 0.9726 - val_bag_output_loss: 0.8840 - val_footwear_output_loss: 0.8811 - val_pose_output_loss: 0.8297 - val_emotion_output_loss: 0.8805 - val_gender_output_acc: 0.6629 - val_image_quality_output_acc: 0.5584 - val_age_output_acc: 0.3966 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.5908 - val_pose_output_acc: 0.6429 - val_emotion_output_acc: 0.7188\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 7.38480\n",
            "Epoch 87/100\n",
            "  1/339 [..............................] - ETA: 4:48 - loss: 7.1548 - gender_output_loss: 0.4738 - image_quality_output_loss: 0.9956 - age_output_loss: 1.3361 - weight_output_loss: 0.9969 - bag_output_loss: 0.9379 - footwear_output_loss: 0.8438 - pose_output_loss: 0.7944 - emotion_output_loss: 0.7763 - gender_output_acc: 0.7812 - image_quality_output_acc: 0.4375 - age_output_acc: 0.5000 - weight_output_acc: 0.6562 - bag_output_acc: 0.5000 - footwear_output_acc: 0.6250 - pose_output_acc: 0.6875 - emotion_output_acc: 0.7500"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:610: UserWarning: The input 106 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "338/339 [============================>.] - ETA: 0s - loss: 7.4263 - gender_output_loss: 0.5990 - image_quality_output_loss: 0.9706 - age_output_loss: 1.3898 - weight_output_loss: 0.9684 - bag_output_loss: 0.8845 - footwear_output_loss: 0.8746 - pose_output_loss: 0.8376 - emotion_output_loss: 0.9016 - gender_output_acc: 0.6665 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3995 - weight_output_acc: 0.6353 - bag_output_acc: 0.5803 - footwear_output_acc: 0.6008 - pose_output_acc: 0.6382 - emotion_output_acc: 0.7098\n",
            "Epoch 00086: val_loss did not improve from 7.38480\n",
            "Epoch 87/100\n",
            "339/339 [==============================] - 224s 659ms/step - loss: 7.4245 - gender_output_loss: 0.5988 - image_quality_output_loss: 0.9704 - age_output_loss: 1.3896 - weight_output_loss: 0.9683 - bag_output_loss: 0.8839 - footwear_output_loss: 0.8747 - pose_output_loss: 0.8375 - emotion_output_loss: 0.9012 - gender_output_acc: 0.6668 - image_quality_output_acc: 0.5527 - age_output_acc: 0.3997 - weight_output_acc: 0.6357 - bag_output_acc: 0.5806 - footwear_output_acc: 0.6008 - pose_output_acc: 0.6383 - emotion_output_acc: 0.7099 - val_loss: 7.3857 - val_gender_output_loss: 0.5998 - val_image_quality_output_loss: 0.9659 - val_age_output_loss: 1.3982 - val_weight_output_loss: 0.9662 - val_bag_output_loss: 0.8831 - val_footwear_output_loss: 0.8788 - val_pose_output_loss: 0.8113 - val_emotion_output_loss: 0.8824 - val_gender_output_acc: 0.6618 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5796 - val_footwear_output_acc: 0.6057 - val_pose_output_acc: 0.6540 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 7.38480\n",
            "Epoch 88/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-1065:\n",
            "Process ForkPoolWorker-1063:\n",
            "Process ForkPoolWorker-1054:\n",
            "Process ForkPoolWorker-1055:\n",
            "Process ForkPoolWorker-1051:\n",
            "Process ForkPoolWorker-1066:\n",
            "Process ForkPoolWorker-1053:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-1056:\n",
            "Process ForkPoolWorker-1064:\n",
            "Process ForkPoolWorker-1067:\n",
            "Process ForkPoolWorker-1068:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b1852db27ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#class_weight= class_weights,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#callbacks=[lr_reducer, early_stopper, csv_logger, checkpoint, LearningRateScheduler(scheduler, verbose=1)],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05fuCQN0X4pp",
        "colab_type": "code",
        "outputId": "3b138c07-4d4f-4a9f-a938-090909608c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "accuracy = {}\n",
        "losses = {}\n",
        "for k, v in zip(model.metrics_names, results):\n",
        "    if k.endswith('acc'):\n",
        "        accuracy[k] = round(v * 100, 4)\n",
        "    else:\n",
        "        losses[k] = v\n",
        "\n",
        "print(accuracy)\n",
        "print(losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "84/84 [==============================] - 13s 158ms/step\n",
            "{'gender_output_acc': 66.183, 'image_quality_output_acc': 55.3943, 'age_output_acc': 39.9182, 'weight_output_acc': 63.6533, 'bag_output_acc': 57.9613, 'footwear_output_acc': 60.5655, 'pose_output_acc': 65.4018, 'emotion_output_acc': 71.8378}\n",
            "{'loss': 7.3856657573155, 'gender_output_loss': 0.5998035910583678, 'image_quality_output_loss': 0.9659461286805925, 'age_output_loss': 1.3981729674906958, 'weight_output_loss': 0.9661985082285744, 'bag_output_loss': 0.8831210249946231, 'footwear_output_loss': 0.8787677515120733, 'pose_output_loss': 0.8112921594154268, 'emotion_output_loss': 0.8823635989711398}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB3ztzZPYgml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('gdrive/My Drive/saved_models/model-085-7.3848.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko-BAq3CaCkQ",
        "colab_type": "code",
        "outputId": "973b6aa7-c9e6-491f-a7f7-7766328758c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=125,\n",
        "    verbose=1,\n",
        "    initial_epoch=87,\n",
        "    #class_weight= class_weights,\n",
        "    #callbacks=[lr_reducer, early_stopper, csv_logger, checkpoint, LearningRateScheduler(scheduler, verbose=1)],\n",
        "    callbacks=[checkpoint,clr]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 88/125\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.4340 - gender_output_loss: 0.5989 - image_quality_output_loss: 0.9690 - age_output_loss: 1.3924 - weight_output_loss: 0.9675 - bag_output_loss: 0.8876 - footwear_output_loss: 0.8760 - pose_output_loss: 0.8413 - emotion_output_loss: 0.9013 - gender_output_acc: 0.6685 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3967 - weight_output_acc: 0.6357 - bag_output_acc: 0.5768 - footwear_output_acc: 0.6013 - pose_output_acc: 0.6335 - emotion_output_acc: 0.7097\n",
            "339/339 [==============================] - 211s 623ms/step - loss: 7.4336 - gender_output_loss: 0.5988 - image_quality_output_loss: 0.9689 - age_output_loss: 1.3921 - weight_output_loss: 0.9679 - bag_output_loss: 0.8869 - footwear_output_loss: 0.8760 - pose_output_loss: 0.8419 - emotion_output_loss: 0.9010 - gender_output_acc: 0.6685 - image_quality_output_acc: 0.5524 - age_output_acc: 0.3968 - weight_output_acc: 0.6357 - bag_output_acc: 0.5775 - footwear_output_acc: 0.6012 - pose_output_acc: 0.6332 - emotion_output_acc: 0.7097 - val_loss: 7.3983 - val_gender_output_loss: 0.5983 - val_image_quality_output_loss: 0.9683 - val_age_output_loss: 1.3974 - val_weight_output_loss: 0.9703 - val_bag_output_loss: 0.8874 - val_footwear_output_loss: 0.8770 - val_pose_output_loss: 0.8132 - val_emotion_output_loss: 0.8865 - val_gender_output_acc: 0.6592 - val_image_quality_output_acc: 0.5573 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6354 - val_bag_output_acc: 0.5859 - val_footwear_output_acc: 0.6019 - val_pose_output_acc: 0.6551 - val_emotion_output_acc: 0.7188\n",
            "Epoch 88/125\n",
            "Epoch 00088: val_loss did not improve from 7.38480\n",
            "Epoch 89/125\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.4420 - gender_output_loss: 0.5997 - image_quality_output_loss: 0.9716 - age_output_loss: 1.3906 - weight_output_loss: 0.9672 - bag_output_loss: 0.8856 - footwear_output_loss: 0.8777 - pose_output_loss: 0.8451 - emotion_output_loss: 0.9045 - gender_output_acc: 0.6687 - image_quality_output_acc: 0.5514 - age_output_acc: 0.3957 - weight_output_acc: 0.6347 - bag_output_acc: 0.5822 - footwear_output_acc: 0.5982 - pose_output_acc: 0.6341 - emotion_output_acc: 0.7100 - val_loss: 7.4072 - val_gender_output_loss: 0.5979 - val_image_quality_output_loss: 0.9657 - val_age_output_loss: 1.3980 - val_weight_output_loss: 0.9714 - val_bag_output_loss: 0.8887 - val_footwear_output_loss: 0.8798 - val_pose_output_loss: 0.8261 - val_emotion_output_loss: 0.8796 - val_gender_output_acc: 0.6704 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3936 - val_weight_output_acc: 0.6369 - val_bag_output_acc: 0.5815 - val_footwear_output_acc: 0.5978 - val_pose_output_acc: 0.6492 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 7.38480\n",
            "Epoch 89/125\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 7.38480\n",
            "Epoch 90/125\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.4149 - gender_output_loss: 0.5975 - image_quality_output_loss: 0.9688 - age_output_loss: 1.3934 - weight_output_loss: 0.9662 - bag_output_loss: 0.8794 - footwear_output_loss: 0.8728 - pose_output_loss: 0.8349 - emotion_output_loss: 0.9020 - gender_output_acc: 0.6715 - image_quality_output_acc: 0.5535 - age_output_acc: 0.3963 - weight_output_acc: 0.6361 - bag_output_acc: 0.5852 - footwear_output_acc: 0.6010 - pose_output_acc: 0.6349 - emotion_output_acc: 0.7098\n",
            "339/339 [==============================] - 196s 578ms/step - loss: 7.4158 - gender_output_loss: 0.5979 - image_quality_output_loss: 0.9688 - age_output_loss: 1.3929 - weight_output_loss: 0.9660 - bag_output_loss: 0.8801 - footwear_output_loss: 0.8727 - pose_output_loss: 0.8348 - emotion_output_loss: 0.9025 - gender_output_acc: 0.6714 - image_quality_output_acc: 0.5537 - age_output_acc: 0.3966 - weight_output_acc: 0.6362 - bag_output_acc: 0.5848 - footwear_output_acc: 0.6011 - pose_output_acc: 0.6351 - emotion_output_acc: 0.7096 - val_loss: 7.3858 - val_gender_output_loss: 0.5973 - val_image_quality_output_loss: 0.9665 - val_age_output_loss: 1.3964 - val_weight_output_loss: 0.9694 - val_bag_output_loss: 0.8823 - val_footwear_output_loss: 0.8886 - val_pose_output_loss: 0.8031 - val_emotion_output_loss: 0.8823 - val_gender_output_acc: 0.6626 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3977 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5859 - val_footwear_output_acc: 0.5867 - val_pose_output_acc: 0.6469 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 7.38480\n",
            "Epoch 91/125\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.4140 - gender_output_loss: 0.5984 - image_quality_output_loss: 0.9710 - age_output_loss: 1.3865 - weight_output_loss: 0.9669 - bag_output_loss: 0.8817 - footwear_output_loss: 0.8754 - pose_output_loss: 0.8305 - emotion_output_loss: 0.9036 - gender_output_acc: 0.6654 - image_quality_output_acc: 0.5509 - age_output_acc: 0.4025 - weight_output_acc: 0.6337 - bag_output_acc: 0.5819 - footwear_output_acc: 0.5967 - pose_output_acc: 0.6395 - emotion_output_acc: 0.7101\n",
            "Epoch 00090: val_loss did not improve from 7.38480\n",
            "Epoch 91/125\n",
            "339/339 [==============================] - 195s 576ms/step - loss: 7.4137 - gender_output_loss: 0.5985 - image_quality_output_loss: 0.9708 - age_output_loss: 1.3861 - weight_output_loss: 0.9665 - bag_output_loss: 0.8814 - footwear_output_loss: 0.8761 - pose_output_loss: 0.8307 - emotion_output_loss: 0.9037 - gender_output_acc: 0.6655 - image_quality_output_acc: 0.5512 - age_output_acc: 0.4026 - weight_output_acc: 0.6338 - bag_output_acc: 0.5822 - footwear_output_acc: 0.5962 - pose_output_acc: 0.6394 - emotion_output_acc: 0.7101 - val_loss: 7.3761 - val_gender_output_loss: 0.5982 - val_image_quality_output_loss: 0.9637 - val_age_output_loss: 1.3961 - val_weight_output_loss: 0.9685 - val_bag_output_loss: 0.8858 - val_footwear_output_loss: 0.8805 - val_pose_output_loss: 0.7993 - val_emotion_output_loss: 0.8841 - val_gender_output_acc: 0.6611 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6373 - val_bag_output_acc: 0.5844 - val_footwear_output_acc: 0.5926 - val_pose_output_acc: 0.6648 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00091: val_loss improved from 7.38480 to 7.37613, saving model to gdrive/My Drive/saved_models/model-091-7.3761.hdf5\n",
            "Epoch 92/125\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.4087 - gender_output_loss: 0.5918 - image_quality_output_loss: 0.9677 - age_output_loss: 1.3923 - weight_output_loss: 0.9698 - bag_output_loss: 0.8831 - footwear_output_loss: 0.8718 - pose_output_loss: 0.8292 - emotion_output_loss: 0.9029 - gender_output_acc: 0.6790 - image_quality_output_acc: 0.5511 - age_output_acc: 0.4008 - weight_output_acc: 0.6350 - bag_output_acc: 0.5808 - footwear_output_acc: 0.6020 - pose_output_acc: 0.6390 - emotion_output_acc: 0.7097 - val_loss: 7.4052 - val_gender_output_loss: 0.5997 - val_image_quality_output_loss: 0.9668 - val_age_output_loss: 1.4041 - val_weight_output_loss: 0.9713 - val_bag_output_loss: 0.8876 - val_footwear_output_loss: 0.8856 - val_pose_output_loss: 0.7998 - val_emotion_output_loss: 0.8902 - val_gender_output_acc: 0.6711 - val_image_quality_output_acc: 0.5551 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5848 - val_footwear_output_acc: 0.5930 - val_pose_output_acc: 0.6611 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 7.37613\n",
            "Epoch 93/125\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.3819 - gender_output_loss: 0.5901 - image_quality_output_loss: 0.9667 - age_output_loss: 1.3878 - weight_output_loss: 0.9669 - bag_output_loss: 0.8825 - footwear_output_loss: 0.8676 - pose_output_loss: 0.8215 - emotion_output_loss: 0.8987 - gender_output_acc: 0.6832 - image_quality_output_acc: 0.5559 - age_output_acc: 0.4040 - weight_output_acc: 0.6357 - bag_output_acc: 0.5813 - footwear_output_acc: 0.6066 - pose_output_acc: 0.6432 - emotion_output_acc: 0.7100\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00092: val_loss did not improve from 7.37613\n",
            "Epoch 93/125\n",
            "339/339 [==============================] - 196s 578ms/step - loss: 7.3812 - gender_output_loss: 0.5899 - image_quality_output_loss: 0.9666 - age_output_loss: 1.3876 - weight_output_loss: 0.9667 - bag_output_loss: 0.8820 - footwear_output_loss: 0.8679 - pose_output_loss: 0.8218 - emotion_output_loss: 0.8987 - gender_output_acc: 0.6833 - image_quality_output_acc: 0.5559 - age_output_acc: 0.4041 - weight_output_acc: 0.6360 - bag_output_acc: 0.5818 - footwear_output_acc: 0.6061 - pose_output_acc: 0.6429 - emotion_output_acc: 0.7101 - val_loss: 7.3710 - val_gender_output_loss: 0.5930 - val_image_quality_output_loss: 0.9619 - val_age_output_loss: 1.4027 - val_weight_output_loss: 0.9716 - val_bag_output_loss: 0.8797 - val_footwear_output_loss: 0.8804 - val_pose_output_loss: 0.7948 - val_emotion_output_loss: 0.8869 - val_gender_output_acc: 0.6771 - val_image_quality_output_acc: 0.5592 - val_age_output_acc: 0.3891 - val_weight_output_acc: 0.6362 - val_bag_output_acc: 0.5915 - val_footwear_output_acc: 0.5900 - val_pose_output_acc: 0.6648 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 00093: val_loss improved from 7.37613 to 7.37103, saving model to gdrive/My Drive/saved_models/model-093-7.3710.hdf5\n",
            "Epoch 94/125\n",
            "339/339 [==============================] - 196s 578ms/step - loss: 7.3732 - gender_output_loss: 0.5886 - image_quality_output_loss: 0.9706 - age_output_loss: 1.3850 - weight_output_loss: 0.9674 - bag_output_loss: 0.8828 - footwear_output_loss: 0.8618 - pose_output_loss: 0.8177 - emotion_output_loss: 0.8993 - gender_output_acc: 0.6846 - image_quality_output_acc: 0.5514 - age_output_acc: 0.4023 - weight_output_acc: 0.6355 - bag_output_acc: 0.5843 - footwear_output_acc: 0.6104 - pose_output_acc: 0.6487 - emotion_output_acc: 0.7102 - val_loss: 7.3598 - val_gender_output_loss: 0.5968 - val_image_quality_output_loss: 0.9650 - val_age_output_loss: 1.3985 - val_weight_output_loss: 0.9722 - val_bag_output_loss: 0.8827 - val_footwear_output_loss: 0.8796 - val_pose_output_loss: 0.7810 - val_emotion_output_loss: 0.8841 - val_gender_output_acc: 0.6648 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3988 - val_weight_output_acc: 0.6354 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.5878 - val_pose_output_acc: 0.6633 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00094: val_loss improved from 7.37103 to 7.35977, saving model to gdrive/My Drive/saved_models/model-094-7.3598.hdf5\n",
            "Epoch 95/125\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.3701 - gender_output_loss: 0.5880 - image_quality_output_loss: 0.9662 - age_output_loss: 1.3886 - weight_output_loss: 0.9670 - bag_output_loss: 0.8819 - footwear_output_loss: 0.8692 - pose_output_loss: 0.8109 - emotion_output_loss: 0.8983 - gender_output_acc: 0.6798 - image_quality_output_acc: 0.5539 - age_output_acc: 0.3996 - weight_output_acc: 0.6354 - bag_output_acc: 0.5846 - footwear_output_acc: 0.6061 - pose_output_acc: 0.6464 - emotion_output_acc: 0.7103\n",
            " - 195s 576ms/step - loss: 7.3698 - gender_output_loss: 0.5878 - image_quality_output_loss: 0.9665 - age_output_loss: 1.3884 - weight_output_loss: 0.9669 - bag_output_loss: 0.8817 - footwear_output_loss: 0.8692 - pose_output_loss: 0.8107 - emotion_output_loss: 0.8987 - gender_output_acc: 0.6799 - image_quality_output_acc: 0.5538 - age_output_acc: 0.3999 - weight_output_acc: 0.6355 - bag_output_acc: 0.5846 - footwear_output_acc: 0.6062 - pose_output_acc: 0.6467 - emotion_output_acc: 0.7101 - val_loss: 7.3452 - val_gender_output_loss: 0.5925 - val_image_quality_output_loss: 0.9613 - val_age_output_loss: 1.3979 - val_weight_output_loss: 0.9702 - val_bag_output_loss: 0.8773 - val_footwear_output_loss: 0.8780 - val_pose_output_loss: 0.7843 - val_emotion_output_loss: 0.8838 - val_gender_output_acc: 0.6756 - val_image_quality_output_acc: 0.5606 - val_age_output_acc: 0.3984 - val_weight_output_acc: 0.6369 - val_bag_output_acc: 0.5885 - val_footwear_output_acc: 0.5971 - val_pose_output_acc: 0.6719 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00095: val_loss improved from 7.35977 to 7.34520, saving model to gdrive/My Drive/saved_models/model-095-7.3452.hdf5\n",
            "Epoch 96/125\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.3587 - gender_output_loss: 0.5853 - image_quality_output_loss: 0.9664 - age_output_loss: 1.3841 - weight_output_loss: 0.9673 - bag_output_loss: 0.8819 - footwear_output_loss: 0.8665 - pose_output_loss: 0.8117 - emotion_output_loss: 0.8955 - gender_output_acc: 0.6821 - image_quality_output_acc: 0.5526 - age_output_acc: 0.3998 - weight_output_acc: 0.6357 - bag_output_acc: 0.5838 - footwear_output_acc: 0.6072 - pose_output_acc: 0.6495 - emotion_output_acc: 0.7099 - val_loss: 7.3570 - val_gender_output_loss: 0.5892 - val_image_quality_output_loss: 0.9643 - val_age_output_loss: 1.3968 - val_weight_output_loss: 0.9782 - val_bag_output_loss: 0.8808 - val_footwear_output_loss: 0.8690 - val_pose_output_loss: 0.7868 - val_emotion_output_loss: 0.8918 - val_gender_output_acc: 0.6741 - val_image_quality_output_acc: 0.5614 - val_age_output_acc: 0.3906 - val_weight_output_acc: 0.6380 - val_bag_output_acc: 0.5908 - val_footwear_output_acc: 0.6001 - val_pose_output_acc: 0.6752 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 7.34520\n",
            "Epoch 97/125\n",
            "339/339 [==============================] - 195s 575ms/step - loss: 7.3449 - gender_output_loss: 0.5861 - image_quality_output_loss: 0.9664 - age_output_loss: 1.3872 - weight_output_loss: 0.9638 - bag_output_loss: 0.8784 - footwear_output_loss: 0.8646 - pose_output_loss: 0.8034 - emotion_output_loss: 0.8951 - gender_output_acc: 0.6834 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4038 - weight_output_acc: 0.6359 - bag_output_acc: 0.5887 - footwear_output_acc: 0.6109 - pose_output_acc: 0.6539 - emotion_output_acc: 0.7102 - val_loss: 7.3164 - val_gender_output_loss: 0.5904 - val_image_quality_output_loss: 0.9603 - val_age_output_loss: 1.3955 - val_weight_output_loss: 0.9680 - val_bag_output_loss: 0.8804 - val_footwear_output_loss: 0.8685 - val_pose_output_loss: 0.7704 - val_emotion_output_loss: 0.8830 - val_gender_output_acc: 0.6749 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3973 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.5915 - val_footwear_output_acc: 0.6031 - val_pose_output_acc: 0.6752 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 7.34520\n",
            "Epoch 97/125\n",
            "\n",
            "Epoch 00097: val_loss improved from 7.34520 to 7.31644, saving model to gdrive/My Drive/saved_models/model-097-7.3164.hdf5\n",
            "Epoch 98/125\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.3430 - gender_output_loss: 0.5821 - image_quality_output_loss: 0.9678 - age_output_loss: 1.3808 - weight_output_loss: 0.9638 - bag_output_loss: 0.8788 - footwear_output_loss: 0.8653 - pose_output_loss: 0.8079 - emotion_output_loss: 0.8964 - gender_output_acc: 0.6826 - image_quality_output_acc: 0.5535 - age_output_acc: 0.4010 - weight_output_acc: 0.6359 - bag_output_acc: 0.5852 - footwear_output_acc: 0.6090 - pose_output_acc: 0.6489 - emotion_output_acc: 0.7099 - val_loss: 7.3357 - val_gender_output_loss: 0.5851 - val_image_quality_output_loss: 0.9625 - val_age_output_loss: 1.3972 - val_weight_output_loss: 0.9715 - val_bag_output_loss: 0.8768 - val_footwear_output_loss: 0.8760 - val_pose_output_loss: 0.7751 - val_emotion_output_loss: 0.8915 - val_gender_output_acc: 0.6767 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3914 - val_weight_output_acc: 0.6343 - val_bag_output_acc: 0.5960 - val_footwear_output_acc: 0.5908 - val_pose_output_acc: 0.6700 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 00097: val_loss improved from 7.34520 to 7.31644, saving model to gdrive/My Drive/saved_models/model-097-7.3164.hdf5\n",
            "Epoch 00098: val_loss did not improve from 7.31644\n",
            "Epoch 99/125\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.3342 - gender_output_loss: 0.5849 - image_quality_output_loss: 0.9643 - age_output_loss: 1.3864 - weight_output_loss: 0.9656 - bag_output_loss: 0.8766 - footwear_output_loss: 0.8634 - pose_output_loss: 0.7998 - emotion_output_loss: 0.8932 - gender_output_acc: 0.6825 - image_quality_output_acc: 0.5560 - age_output_acc: 0.4034 - weight_output_acc: 0.6362 - bag_output_acc: 0.5860 - footwear_output_acc: 0.6050 - pose_output_acc: 0.6547 - emotion_output_acc: 0.7102 - val_loss: 7.3208 - val_gender_output_loss: 0.5852 - val_image_quality_output_loss: 0.9623 - val_age_output_loss: 1.3963 - val_weight_output_loss: 0.9673 - val_bag_output_loss: 0.8809 - val_footwear_output_loss: 0.8688 - val_pose_output_loss: 0.7766 - val_emotion_output_loss: 0.8834 - val_gender_output_acc: 0.6834 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3955 - val_weight_output_acc: 0.6354 - val_bag_output_acc: 0.5878 - val_footwear_output_acc: 0.6038 - val_pose_output_acc: 0.6804 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 7.31644\n",
            "Epoch 100/125\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.3154 - gender_output_loss: 0.5764 - image_quality_output_loss: 0.9655 - age_output_loss: 1.3829 - weight_output_loss: 0.9642 - bag_output_loss: 0.8747 - footwear_output_loss: 0.8606 - pose_output_loss: 0.7962 - emotion_output_loss: 0.8949 - gender_output_acc: 0.6928 - image_quality_output_acc: 0.5540 - age_output_acc: 0.4018 - weight_output_acc: 0.6351 - bag_output_acc: 0.5909 - footwear_output_acc: 0.6097 - pose_output_acc: 0.6554 - emotion_output_acc: 0.7108\n",
            "Epoch 00099: val_loss did not improve from 7.31644\n",
            "Epoch 100/125\n",
            "339/339 [==============================] - 196s 577ms/step - loss: 7.3154 - gender_output_loss: 0.5761 - image_quality_output_loss: 0.9657 - age_output_loss: 1.3827 - weight_output_loss: 0.9647 - bag_output_loss: 0.8743 - footwear_output_loss: 0.8599 - pose_output_loss: 0.7961 - emotion_output_loss: 0.8959 - gender_output_acc: 0.6930 - image_quality_output_acc: 0.5537 - age_output_acc: 0.4018 - weight_output_acc: 0.6347 - bag_output_acc: 0.5912 - footwear_output_acc: 0.6101 - pose_output_acc: 0.6558 - emotion_output_acc: 0.7103 - val_loss: 7.3051 - val_gender_output_loss: 0.5814 - val_image_quality_output_loss: 0.9633 - val_age_output_loss: 1.3928 - val_weight_output_loss: 0.9705 - val_bag_output_loss: 0.8767 - val_footwear_output_loss: 0.8678 - val_pose_output_loss: 0.7671 - val_emotion_output_loss: 0.8855 - val_gender_output_acc: 0.6845 - val_image_quality_output_acc: 0.5588 - val_age_output_acc: 0.3966 - val_weight_output_acc: 0.6350 - val_bag_output_acc: 0.5908 - val_footwear_output_acc: 0.6012 - val_pose_output_acc: 0.6778 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00100: val_loss improved from 7.31644 to 7.30506, saving model to gdrive/My Drive/saved_models/model-100-7.3051.hdf5\n",
            "Epoch 101/125\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.3025 - gender_output_loss: 0.5749 - image_quality_output_loss: 0.9657 - age_output_loss: 1.3782 - weight_output_loss: 0.9608 - bag_output_loss: 0.8778 - footwear_output_loss: 0.8625 - pose_output_loss: 0.7902 - emotion_output_loss: 0.8924 - gender_output_acc: 0.6910 - image_quality_output_acc: 0.5540 - age_output_acc: 0.4031 - weight_output_acc: 0.6340 - bag_output_acc: 0.5876 - footwear_output_acc: 0.6082 - pose_output_acc: 0.6568 - emotion_output_acc: 0.7104\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00100: val_loss improved from 7.31644 to 7.30506, saving model to gdrive/My Drive/saved_models/model-100-7.3051.hdf5\n",
            "339/339 [==============================] - 196s 578ms/step - loss: 7.3031 - gender_output_loss: 0.5748 - image_quality_output_loss: 0.9656 - age_output_loss: 1.3787 - weight_output_loss: 0.9604 - bag_output_loss: 0.8775 - footwear_output_loss: 0.8624 - pose_output_loss: 0.7906 - emotion_output_loss: 0.8932 - gender_output_acc: 0.6910 - image_quality_output_acc: 0.5540 - age_output_acc: 0.4028 - weight_output_acc: 0.6343 - bag_output_acc: 0.5879 - footwear_output_acc: 0.6081 - pose_output_acc: 0.6564 - emotion_output_acc: 0.7101 - val_loss: 7.3120 - val_gender_output_loss: 0.5827 - val_image_quality_output_loss: 0.9640 - val_age_output_loss: 1.3940 - val_weight_output_loss: 0.9706 - val_bag_output_loss: 0.8766 - val_footwear_output_loss: 0.8700 - val_pose_output_loss: 0.7662 - val_emotion_output_loss: 0.8879 - val_gender_output_acc: 0.6838 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6354 - val_bag_output_acc: 0.5938 - val_footwear_output_acc: 0.5967 - val_pose_output_acc: 0.6797 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 7.30506\n",
            "Epoch 102/125\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.3170 - gender_output_loss: 0.5756 - image_quality_output_loss: 0.9675 - age_output_loss: 1.3823 - weight_output_loss: 0.9631 - bag_output_loss: 0.8769 - footwear_output_loss: 0.8601 - pose_output_loss: 0.7971 - emotion_output_loss: 0.8944 - gender_output_acc: 0.6904 - image_quality_output_acc: 0.5535 - age_output_acc: 0.4016 - weight_output_acc: 0.6362 - bag_output_acc: 0.5886 - footwear_output_acc: 0.6065 - pose_output_acc: 0.6515 - emotion_output_acc: 0.7101 - val_loss: 7.3124 - val_gender_output_loss: 0.5810 - val_image_quality_output_loss: 0.9611 - val_age_output_loss: 1.3977 - val_weight_output_loss: 0.9716 - val_bag_output_loss: 0.8779 - val_footwear_output_loss: 0.8684 - val_pose_output_loss: 0.7678 - val_emotion_output_loss: 0.8870 - val_gender_output_acc: 0.6890 - val_image_quality_output_acc: 0.5562 - val_age_output_acc: 0.3917 - val_weight_output_acc: 0.6365 - val_bag_output_acc: 0.5952 - val_footwear_output_acc: 0.5982 - val_pose_output_acc: 0.6775 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 102/125\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 7.30506\n",
            "Epoch 103/125\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.3039 - gender_output_loss: 0.5753 - image_quality_output_loss: 0.9667 - age_output_loss: 1.3837 - weight_output_loss: 0.9608 - bag_output_loss: 0.8768 - footwear_output_loss: 0.8546 - pose_output_loss: 0.7882 - emotion_output_loss: 0.8977 - gender_output_acc: 0.6919 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4012 - weight_output_acc: 0.6359 - bag_output_acc: 0.5881 - footwear_output_acc: 0.6098 - pose_output_acc: 0.6541 - emotion_output_acc: 0.7100\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.3050 - gender_output_loss: 0.5754 - image_quality_output_loss: 0.9667 - age_output_loss: 1.3839 - weight_output_loss: 0.9608 - bag_output_loss: 0.8770 - footwear_output_loss: 0.8553 - pose_output_loss: 0.7880 - emotion_output_loss: 0.8979 - gender_output_acc: 0.6920 - image_quality_output_acc: 0.5541 - age_output_acc: 0.4008 - weight_output_acc: 0.6358 - bag_output_acc: 0.5878 - footwear_output_acc: 0.6094 - pose_output_acc: 0.6543 - emotion_output_acc: 0.7099 - val_loss: 7.3269 - val_gender_output_loss: 0.5926 - val_image_quality_output_loss: 0.9631 - val_age_output_loss: 1.4010 - val_weight_output_loss: 0.9683 - val_bag_output_loss: 0.8846 - val_footwear_output_loss: 0.8698 - val_pose_output_loss: 0.7653 - val_emotion_output_loss: 0.8821 - val_gender_output_acc: 0.6786 - val_image_quality_output_acc: 0.5595 - val_age_output_acc: 0.3899 - val_weight_output_acc: 0.6339 - val_bag_output_acc: 0.5900 - val_footwear_output_acc: 0.5949 - val_pose_output_acc: 0.6767 - val_emotion_output_acc: 0.7191\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 7.30506\n",
            "Epoch 104/125\n",
            "338/339 [============================>.] - ETA: 0s - loss: 7.3150 - gender_output_loss: 0.5802 - image_quality_output_loss: 0.9662 - age_output_loss: 1.3824 - weight_output_loss: 0.9621 - bag_output_loss: 0.8749 - footwear_output_loss: 0.8589 - pose_output_loss: 0.7936 - emotion_output_loss: 0.8966 - gender_output_acc: 0.6874 - image_quality_output_acc: 0.5542 - age_output_acc: 0.4002 - weight_output_acc: 0.6359 - bag_output_acc: 0.5940 - footwear_output_acc: 0.6090 - pose_output_acc: 0.6578 - emotion_output_acc: 0.7096\n",
            "Epoch 00103: val_loss did not improve from 7.30506\n",
            "Epoch 104/125\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.3138 - gender_output_loss: 0.5800 - image_quality_output_loss: 0.9660 - age_output_loss: 1.3824 - weight_output_loss: 0.9625 - bag_output_loss: 0.8753 - footwear_output_loss: 0.8590 - pose_output_loss: 0.7927 - emotion_output_loss: 0.8960 - gender_output_acc: 0.6873 - image_quality_output_acc: 0.5545 - age_output_acc: 0.4001 - weight_output_acc: 0.6356 - bag_output_acc: 0.5938 - footwear_output_acc: 0.6091 - pose_output_acc: 0.6584 - emotion_output_acc: 0.7099 - val_loss: 7.2952 - val_gender_output_loss: 0.5820 - val_image_quality_output_loss: 0.9630 - val_age_output_loss: 1.3956 - val_weight_output_loss: 0.9685 - val_bag_output_loss: 0.8751 - val_footwear_output_loss: 0.8722 - val_pose_output_loss: 0.7566 - val_emotion_output_loss: 0.8823 - val_gender_output_acc: 0.6905 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3981 - val_weight_output_acc: 0.6358 - val_bag_output_acc: 0.6001 - val_footwear_output_acc: 0.5949 - val_pose_output_acc: 0.6823 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00104: val_loss improved from 7.30506 to 7.29523, saving model to gdrive/My Drive/saved_models/model-104-7.2952.hdf5\n",
            "Epoch 105/125\n",
            "339/339 [==============================] - 195s 577ms/step - loss: 7.3042 - gender_output_loss: 0.5793 - image_quality_output_loss: 0.9646 - age_output_loss: 1.3801 - weight_output_loss: 0.9625 - bag_output_loss: 0.8734 - footwear_output_loss: 0.8615 - pose_output_loss: 0.7872 - emotion_output_loss: 0.8955 - gender_output_acc: 0.6831 - image_quality_output_acc: 0.5524 - age_output_acc: 0.4042 - weight_output_acc: 0.6358 - bag_output_acc: 0.5902 - footwear_output_acc: 0.6090 - pose_output_acc: 0.6609 - emotion_output_acc: 0.7098 - val_loss: 7.3008 - val_gender_output_loss: 0.5836 - val_image_quality_output_loss: 0.9590 - val_age_output_loss: 1.3988 - val_weight_output_loss: 0.9667 - val_bag_output_loss: 0.8748 - val_footwear_output_loss: 0.8798 - val_pose_output_loss: 0.7558 - val_emotion_output_loss: 0.8823 - val_gender_output_acc: 0.6886 - val_image_quality_output_acc: 0.5569 - val_age_output_acc: 0.3962 - val_weight_output_acc: 0.6336 - val_bag_output_acc: 0.5911 - val_footwear_output_acc: 0.5878 - val_pose_output_acc: 0.6767 - val_emotion_output_acc: 0.7184\n",
            "\n",
            "Epoch 00104: val_loss improved from 7.30506 to 7.29523, saving model to gdrive/My Drive/saved_models/model-104-7.2952.hdf5\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 7.29523\n",
            "Epoch 106/125\n",
            "339/339 [==============================] - 196s 579ms/step - loss: 7.3003 - gender_output_loss: 0.5736 - image_quality_output_loss: 0.9666 - age_output_loss: 1.3813 - weight_output_loss: 0.9607 - bag_output_loss: 0.8735 - footwear_output_loss: 0.8611 - pose_output_loss: 0.7884 - emotion_output_loss: 0.8951 - gender_output_acc: 0.6927 - image_quality_output_acc: 0.5536 - age_output_acc: 0.4035 - weight_output_acc: 0.6356 - bag_output_acc: 0.5912 - footwear_output_acc: 0.6070 - pose_output_acc: 0.6580 - emotion_output_acc: 0.7097 - val_loss: 7.3030 - val_gender_output_loss: 0.5837 - val_image_quality_output_loss: 0.9615 - val_age_output_loss: 1.3970 - val_weight_output_loss: 0.9643 - val_bag_output_loss: 0.8824 - val_footwear_output_loss: 0.8725 - val_pose_output_loss: 0.7543 - val_emotion_output_loss: 0.8873 - val_gender_output_acc: 0.6789 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3999 - val_weight_output_acc: 0.6343 - val_bag_output_acc: 0.5874 - val_footwear_output_acc: 0.5900 - val_pose_output_acc: 0.6737 - val_emotion_output_acc: 0.7180\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 7.29523\n",
            "Epoch 107/125\n",
            " 78/339 [=====>........................] - ETA: 2:20 - loss: 7.2507 - gender_output_loss: 0.5610 - image_quality_output_loss: 0.9724 - age_output_loss: 1.3678 - weight_output_loss: 0.9637 - bag_output_loss: 0.8595 - footwear_output_loss: 0.8534 - pose_output_loss: 0.7902 - emotion_output_loss: 0.8828 - gender_output_acc: 0.7059 - image_quality_output_acc: 0.5445 - age_output_acc: 0.4195 - weight_output_acc: 0.6362 - bag_output_acc: 0.5994 - footwear_output_acc: 0.6170 - pose_output_acc: 0.6631 - emotion_output_acc: 0.7131"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jNt4gGXaVFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "169bd07d-7ec6-41ea-fb94-a435e583eb37"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('gdrive/My Drive/saved_models/model-104-7.2952.hdf5')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG1NRYAJG6UE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e4aed404-9207-41e5-aff2-85116231ee98"
      },
      "source": [
        "results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "accuracy = {}\n",
        "losses = {}\n",
        "for k, v in zip(model.metrics_names, results):\n",
        "    if k.endswith('acc'):\n",
        "        accuracy[k] = round(v * 100, 4)\n",
        "    else:\n",
        "        losses[k] = v\n",
        "\n",
        "print(accuracy)\n",
        "print(losses)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "84/84 [==============================] - 14s 161ms/step\n",
            "{'gender_output_acc': 69.0476, 'image_quality_output_acc': 55.6548, 'age_output_acc': 39.8065, 'weight_output_acc': 63.5789, 'bag_output_acc': 60.0074, 'footwear_output_acc': 59.4866, 'pose_output_acc': 68.2292, 'emotion_output_acc': 71.8006}\n",
            "{'loss': 7.29522743111565, 'gender_output_loss': 0.5819603797225725, 'image_quality_output_loss': 0.962988283662569, 'age_output_loss': 1.3956435322761536, 'weight_output_loss': 0.9685196124372029, 'bag_output_loss': 0.8750901449294317, 'footwear_output_loss': 0.872197225689888, 'pose_output_loss': 0.7565630241518929, 'emotion_output_loss': 0.8822652342773619}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNRzMYdYHPIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}